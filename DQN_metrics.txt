    31/50000: episode: 1, duration: 1.430s, episode steps: 31, steps per second: 22, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.419 [0.000, 1.000], mean observation: 0.013 [-1.185, 1.776], loss: 0.462173, mean_absolute_error: 0.520752, mean_q: 0.096439
    44/50000: episode: 2, duration: 0.078s, episode steps: 13, steps per second: 166, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.069 [-1.867, 1.224], loss: 0.350131, mean_absolute_error: 0.542971, mean_q: 0.292715
    64/50000: episode: 3, duration: 0.116s, episode steps: 20, steps per second: 173, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.070 [-1.375, 0.833], loss: 0.230972, mean_absolute_error: 0.559493, mean_q: 0.501413
    83/50000: episode: 4, duration: 0.119s, episode steps: 19, steps per second: 160, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.074 [-0.817, 1.505], loss: 0.116485, mean_absolute_error: 0.604493, mean_q: 0.832621
    98/50000: episode: 5, duration: 0.089s, episode steps: 15, steps per second: 169, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.090 [-1.368, 0.825], loss: 0.053599, mean_absolute_error: 0.671270, mean_q: 1.152627
   114/50000: episode: 6, duration: 0.090s, episode steps: 16, steps per second: 177, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.812 [0.000, 1.000], mean observation: -0.072 [-3.056, 1.962], loss: 0.036350, mean_absolute_error: 0.727577, mean_q: 1.335307
   128/50000: episode: 7, duration: 0.087s, episode steps: 14, steps per second: 160, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.111 [-0.934, 1.483], loss: 0.036427, mean_absolute_error: 0.768431, mean_q: 1.452697
   151/50000: episode: 8, duration: 0.132s, episode steps: 23, steps per second: 175, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.304 [0.000, 1.000], mean observation: 0.050 [-1.760, 2.679], loss: 0.029889, mean_absolute_error: 0.820142, mean_q: 1.600496
   162/50000: episode: 9, duration: 0.066s, episode steps: 11, steps per second: 166, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.133 [-1.133, 1.962], loss: 0.032700, mean_absolute_error: 0.893022, mean_q: 1.764888
   175/50000: episode: 10, duration: 0.080s, episode steps: 13, steps per second: 162, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.118 [-2.439, 1.549], loss: 0.052971, mean_absolute_error: 0.947109, mean_q: 1.870785
   201/50000: episode: 11, duration: 0.156s, episode steps: 26, steps per second: 167, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.346 [0.000, 1.000], mean observation: 0.032 [-1.588, 2.376], loss: 0.040956, mean_absolute_error: 1.002012, mean_q: 2.005967
   259/50000: episode: 12, duration: 0.330s, episode steps: 58, steps per second: 176, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.032 [-1.274, 1.190], loss: 0.058933, mean_absolute_error: 1.170749, mean_q: 2.312404
   292/50000: episode: 13, duration: 0.185s, episode steps: 33, steps per second: 178, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.394 [0.000, 1.000], mean observation: 0.067 [-1.337, 2.339], loss: 0.056220, mean_absolute_error: 1.316334, mean_q: 2.615702
   317/50000: episode: 14, duration: 0.150s, episode steps: 25, steps per second: 167, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.070 [-1.149, 0.614], loss: 0.066928, mean_absolute_error: 1.464536, mean_q: 2.902142
   333/50000: episode: 15, duration: 0.096s, episode steps: 16, steps per second: 166, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.070 [-0.988, 1.620], loss: 0.067886, mean_absolute_error: 1.546759, mean_q: 3.074700
   366/50000: episode: 16, duration: 0.204s, episode steps: 33, steps per second: 162, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.576 [0.000, 1.000], mean observation: -0.014 [-1.863, 1.183], loss: 0.111812, mean_absolute_error: 1.668563, mean_q: 3.202938
   389/50000: episode: 17, duration: 0.131s, episode steps: 23, steps per second: 176, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.391 [0.000, 1.000], mean observation: 0.055 [-1.163, 1.975], loss: 0.100669, mean_absolute_error: 1.771511, mean_q: 3.488927
   403/50000: episode: 18, duration: 0.090s, episode steps: 14, steps per second: 155, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.104 [-2.281, 1.364], loss: 0.082871, mean_absolute_error: 1.853316, mean_q: 3.670793
   428/50000: episode: 19, duration: 0.146s, episode steps: 25, steps per second: 172, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.360 [0.000, 1.000], mean observation: 0.059 [-1.345, 2.180], loss: 0.141222, mean_absolute_error: 1.945843, mean_q: 3.771812
   474/50000: episode: 20, duration: 0.259s, episode steps: 46, steps per second: 177, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.018 [-1.109, 0.965], loss: 0.128011, mean_absolute_error: 2.107427, mean_q: 4.119469
   489/50000: episode: 21, duration: 0.090s, episode steps: 15, steps per second: 166, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.123 [-0.565, 0.950], loss: 0.152254, mean_absolute_error: 2.247525, mean_q: 4.381685
   511/50000: episode: 22, duration: 0.130s, episode steps: 22, steps per second: 169, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.087 [-0.806, 1.504], loss: 0.133530, mean_absolute_error: 2.313178, mean_q: 4.535358
   521/50000: episode: 23, duration: 0.060s, episode steps: 10, steps per second: 167, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.132 [-0.942, 1.556], loss: 0.192256, mean_absolute_error: 2.424197, mean_q: 4.714869
   559/50000: episode: 24, duration: 0.229s, episode steps: 38, steps per second: 166, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.368 [0.000, 1.000], mean observation: 0.003 [-2.112, 2.831], loss: 0.221496, mean_absolute_error: 2.502866, mean_q: 4.851118
   572/50000: episode: 25, duration: 0.081s, episode steps: 13, steps per second: 161, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.085 [-1.000, 1.699], loss: 0.183715, mean_absolute_error: 2.625215, mean_q: 5.129649
   598/50000: episode: 26, duration: 0.150s, episode steps: 26, steps per second: 174, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.577 [0.000, 1.000], mean observation: -0.048 [-1.697, 1.002], loss: 0.233408, mean_absolute_error: 2.712213, mean_q: 5.216707
   610/50000: episode: 27, duration: 0.072s, episode steps: 12, steps per second: 167, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.140 [-2.139, 1.174], loss: 0.325784, mean_absolute_error: 2.766895, mean_q: 5.250640
   628/50000: episode: 28, duration: 0.103s, episode steps: 18, steps per second: 174, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.083 [-1.236, 2.232], loss: 0.282634, mean_absolute_error: 2.831615, mean_q: 5.463905
   656/50000: episode: 29, duration: 0.167s, episode steps: 28, steps per second: 168, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.089 [-0.946, 1.858], loss: 0.340393, mean_absolute_error: 2.928390, mean_q: 5.530630
   670/50000: episode: 30, duration: 0.084s, episode steps: 14, steps per second: 166, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.214 [0.000, 1.000], mean observation: 0.091 [-1.521, 2.477], loss: 0.309740, mean_absolute_error: 3.011515, mean_q: 5.770519
   693/50000: episode: 31, duration: 0.135s, episode steps: 23, steps per second: 170, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.391 [0.000, 1.000], mean observation: 0.030 [-1.205, 1.840], loss: 0.285380, mean_absolute_error: 3.064477, mean_q: 5.888163
   714/50000: episode: 32, duration: 0.130s, episode steps: 21, steps per second: 162, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.052 [-0.828, 1.261], loss: 0.208964, mean_absolute_error: 3.175031, mean_q: 6.149430
   740/50000: episode: 33, duration: 0.153s, episode steps: 26, steps per second: 169, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.073 [-1.025, 0.432], loss: 0.297080, mean_absolute_error: 3.252467, mean_q: 6.213748
   755/50000: episode: 34, duration: 0.090s, episode steps: 15, steps per second: 166, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.096 [-1.324, 2.289], loss: 0.272522, mean_absolute_error: 3.355489, mean_q: 6.456414
   772/50000: episode: 35, duration: 0.103s, episode steps: 17, steps per second: 166, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.353 [0.000, 1.000], mean observation: 0.093 [-1.015, 1.912], loss: 0.295594, mean_absolute_error: 3.421803, mean_q: 6.603698
   789/50000: episode: 36, duration: 0.106s, episode steps: 17, steps per second: 161, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.101 [-0.950, 0.566], loss: 0.302521, mean_absolute_error: 3.460435, mean_q: 6.664082
   811/50000: episode: 37, duration: 0.128s, episode steps: 22, steps per second: 172, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.070 [-1.709, 1.002], loss: 0.388621, mean_absolute_error: 3.523707, mean_q: 6.740643
   859/50000: episode: 38, duration: 0.276s, episode steps: 48, steps per second: 174, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: 0.030 [-1.137, 1.477], loss: 0.377797, mean_absolute_error: 3.665827, mean_q: 7.061131
   920/50000: episode: 39, duration: 0.355s, episode steps: 61, steps per second: 172, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: -0.065 [-1.211, 0.779], loss: 0.308047, mean_absolute_error: 3.870754, mean_q: 7.578685
   932/50000: episode: 40, duration: 0.070s, episode steps: 12, steps per second: 171, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.097 [-1.188, 1.888], loss: 0.321739, mean_absolute_error: 3.981777, mean_q: 7.840284
   958/50000: episode: 41, duration: 0.154s, episode steps: 26, steps per second: 168, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.423 [0.000, 1.000], mean observation: 0.094 [-0.798, 1.693], loss: 0.266805, mean_absolute_error: 4.115370, mean_q: 8.147223
   980/50000: episode: 42, duration: 0.131s, episode steps: 22, steps per second: 167, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.055 [-1.232, 2.086], loss: 0.444621, mean_absolute_error: 4.170650, mean_q: 8.172418
   994/50000: episode: 43, duration: 0.087s, episode steps: 14, steps per second: 161, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.065 [-1.579, 1.020], loss: 0.362701, mean_absolute_error: 4.273991, mean_q: 8.383802
  1014/50000: episode: 44, duration: 0.122s, episode steps: 20, steps per second: 165, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.075 [-0.953, 0.628], loss: 0.276549, mean_absolute_error: 4.315343, mean_q: 8.534784
  1112/50000: episode: 45, duration: 0.554s, episode steps: 98, steps per second: 177, episode reward: 98.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.184 [-1.038, 1.277], loss: 0.395301, mean_absolute_error: 4.567656, mean_q: 9.028313
  1190/50000: episode: 46, duration: 0.441s, episode steps: 78, steps per second: 177, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.141 [-0.729, 1.100], loss: 0.470776, mean_absolute_error: 4.920171, mean_q: 9.710147
  1213/50000: episode: 47, duration: 0.132s, episode steps: 23, steps per second: 175, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.565 [0.000, 1.000], mean observation: -0.040 [-1.550, 1.019], loss: 0.325626, mean_absolute_error: 5.221853, mean_q: 10.362164
  1290/50000: episode: 48, duration: 0.438s, episode steps: 77, steps per second: 176, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: 0.134 [-2.303, 2.252], loss: 0.415362, mean_absolute_error: 5.387382, mean_q: 10.708932
  1312/50000: episode: 49, duration: 0.129s, episode steps: 22, steps per second: 170, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.096 [-0.789, 1.195], loss: 0.471949, mean_absolute_error: 5.515840, mean_q: 11.026488
  1359/50000: episode: 50, duration: 0.274s, episode steps: 47, steps per second: 172, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: -0.138 [-0.934, 0.370], loss: 0.557169, mean_absolute_error: 5.749341, mean_q: 11.451800
  1498/50000: episode: 51, duration: 0.778s, episode steps: 139, steps per second: 179, episode reward: 139.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.504 [0.000, 1.000], mean observation: 0.219 [-0.839, 1.318], loss: 0.478728, mean_absolute_error: 6.116997, mean_q: 12.276966
  1675/50000: episode: 52, duration: 0.987s, episode steps: 177, steps per second: 179, episode reward: 177.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.503 [0.000, 1.000], mean observation: 0.123 [-1.078, 1.129], loss: 0.554651, mean_absolute_error: 6.825164, mean_q: 13.714019
  1849/50000: episode: 53, duration: 0.999s, episode steps: 174, steps per second: 174, episode reward: 174.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.185 [-1.506, 0.739], loss: 0.597474, mean_absolute_error: 7.560439, mean_q: 15.241479
  2095/50000: episode: 54, duration: 1.379s, episode steps: 246, steps per second: 178, episode reward: 246.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.173 [-2.020, 1.053], loss: 0.652458, mean_absolute_error: 8.553292, mean_q: 17.315136
  2348/50000: episode: 55, duration: 1.423s, episode steps: 253, steps per second: 178, episode reward: 253.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.269 [-1.025, 2.406], loss: 0.817234, mean_absolute_error: 9.773493, mean_q: 19.840322
  2562/50000: episode: 56, duration: 1.194s, episode steps: 214, steps per second: 179, episode reward: 214.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.241 [-2.053, 0.732], loss: 1.081461, mean_absolute_error: 10.824377, mean_q: 21.984941
  2834/50000: episode: 57, duration: 1.504s, episode steps: 272, steps per second: 181, episode reward: 272.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.160 [-1.213, 1.521], loss: 0.817214, mean_absolute_error: 12.050313, mean_q: 24.514225
  3052/50000: episode: 58, duration: 1.228s, episode steps: 218, steps per second: 178, episode reward: 218.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.246 [-1.880, 0.737], loss: 1.177888, mean_absolute_error: 13.279670, mean_q: 26.972460
  3262/50000: episode: 59, duration: 1.180s, episode steps: 210, steps per second: 178, episode reward: 210.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.291 [-2.074, 1.030], loss: 1.626281, mean_absolute_error: 14.275281, mean_q: 28.941036
  3521/50000: episode: 60, duration: 1.441s, episode steps: 259, steps per second: 180, episode reward: 259.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.270 [-2.416, 0.947], loss: 1.454957, mean_absolute_error: 15.498728, mean_q: 31.499500
  3865/50000: episode: 61, duration: 1.911s, episode steps: 344, steps per second: 180, episode reward: 344.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.214 [-2.409, 0.953], loss: 1.579097, mean_absolute_error: 17.051638, mean_q: 34.678478
  4087/50000: episode: 62, duration: 1.230s, episode steps: 222, steps per second: 181, episode reward: 222.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.324 [-2.409, 0.833], loss: 1.844857, mean_absolute_error: 18.484995, mean_q: 37.517063
  4316/50000: episode: 63, duration: 1.270s, episode steps: 229, steps per second: 180, episode reward: 229.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.303 [-2.410, 0.753], loss: 2.171824, mean_absolute_error: 19.672974, mean_q: 39.870701
  4583/50000: episode: 64, duration: 1.501s, episode steps: 267, steps per second: 178, episode reward: 267.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.280 [-2.434, 0.684], loss: 2.968916, mean_absolute_error: 20.811958, mean_q: 42.111191
  4803/50000: episode: 65, duration: 1.230s, episode steps: 220, steps per second: 179, episode reward: 220.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.318 [-2.402, 0.723], loss: 2.831054, mean_absolute_error: 21.949261, mean_q: 44.379471
  5233/50000: episode: 66, duration: 2.394s, episode steps: 430, steps per second: 180, episode reward: 430.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.171 [-1.187, 2.427], loss: 2.740174, mean_absolute_error: 23.215549, mean_q: 47.071175
  5477/50000: episode: 67, duration: 1.378s, episode steps: 244, steps per second: 177, episode reward: 244.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.286 [-2.412, 0.850], loss: 3.389724, mean_absolute_error: 24.567900, mean_q: 49.713287
  5775/50000: episode: 68, duration: 1.650s, episode steps: 298, steps per second: 181, episode reward: 298.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.250 [-1.047, 2.414], loss: 3.148123, mean_absolute_error: 25.677740, mean_q: 51.992470
  6060/50000: episode: 69, duration: 1.587s, episode steps: 285, steps per second: 180, episode reward: 285.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.254 [-2.416, 0.635], loss: 3.001022, mean_absolute_error: 26.740484, mean_q: 54.086567
  6296/50000: episode: 70, duration: 1.328s, episode steps: 236, steps per second: 178, episode reward: 236.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.303 [-0.898, 2.432], loss: 3.708522, mean_absolute_error: 27.619186, mean_q: 55.827106
  6565/50000: episode: 71, duration: 1.510s, episode steps: 269, steps per second: 178, episode reward: 269.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.263 [-0.908, 2.410], loss: 3.557364, mean_absolute_error: 28.477688, mean_q: 57.500683
  6811/50000: episode: 72, duration: 1.371s, episode steps: 246, steps per second: 179, episode reward: 246.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.290 [-0.795, 2.406], loss: 3.480074, mean_absolute_error: 29.212271, mean_q: 59.046383
  7052/50000: episode: 73, duration: 1.351s, episode steps: 241, steps per second: 178, episode reward: 241.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.297 [-2.406, 0.741], loss: 3.266057, mean_absolute_error: 30.000526, mean_q: 60.690567
  7285/50000: episode: 74, duration: 1.313s, episode steps: 233, steps per second: 178, episode reward: 233.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.302 [-1.156, 2.425], loss: 4.195776, mean_absolute_error: 30.777973, mean_q: 62.209000
  7570/50000: episode: 75, duration: 1.636s, episode steps: 285, steps per second: 174, episode reward: 285.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.250 [-1.314, 2.626], loss: 3.429517, mean_absolute_error: 31.554945, mean_q: 63.810566
  7782/50000: episode: 76, duration: 1.217s, episode steps: 212, steps per second: 174, episode reward: 212.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.324 [-0.957, 2.413], loss: 3.058374, mean_absolute_error: 32.236549, mean_q: 65.190834
  8089/50000: episode: 77, duration: 1.716s, episode steps: 307, steps per second: 179, episode reward: 307.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.230 [-2.411, 1.044], loss: 3.581729, mean_absolute_error: 33.206036, mean_q: 67.115585
  8368/50000: episode: 78, duration: 1.577s, episode steps: 279, steps per second: 177, episode reward: 279.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.253 [-0.956, 2.407], loss: 3.046707, mean_absolute_error: 34.062225, mean_q: 68.903381
  8691/50000: episode: 79, duration: 1.819s, episode steps: 323, steps per second: 178, episode reward: 323.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.221 [-0.878, 2.415], loss: 3.207757, mean_absolute_error: 34.824921, mean_q: 70.375656
  9108/50000: episode: 80, duration: 2.353s, episode steps: 417, steps per second: 177, episode reward: 417.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.172 [-0.795, 2.421], loss: 3.390424, mean_absolute_error: 35.718620, mean_q: 72.161980
  9396/50000: episode: 81, duration: 1.638s, episode steps: 288, steps per second: 176, episode reward: 288.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.247 [-0.777, 2.408], loss: 3.245634, mean_absolute_error: 36.439644, mean_q: 73.697540
  9630/50000: episode: 82, duration: 1.324s, episode steps: 234, steps per second: 177, episode reward: 234.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.300 [-2.429, 0.703], loss: 3.016428, mean_absolute_error: 37.160458, mean_q: 75.085953
  9895/50000: episode: 83, duration: 1.500s, episode steps: 265, steps per second: 177, episode reward: 265.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.267 [-0.802, 2.422], loss: 2.770354, mean_absolute_error: 37.624905, mean_q: 76.101135
 10158/50000: episode: 84, duration: 1.490s, episode steps: 263, steps per second: 177, episode reward: 263.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.269 [-0.851, 2.407], loss: 3.094890, mean_absolute_error: 38.217167, mean_q: 77.260498
 10378/50000: episode: 85, duration: 1.237s, episode steps: 220, steps per second: 178, episode reward: 220.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.317 [-0.841, 2.416], loss: 2.319074, mean_absolute_error: 38.708122, mean_q: 78.273155
 10679/50000: episode: 86, duration: 1.689s, episode steps: 301, steps per second: 178, episode reward: 301.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.239 [-0.823, 2.414], loss: 2.899851, mean_absolute_error: 39.356682, mean_q: 79.535568
 10985/50000: episode: 87, duration: 1.733s, episode steps: 306, steps per second: 177, episode reward: 306.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.236 [-0.843, 2.429], loss: 3.354449, mean_absolute_error: 39.794010, mean_q: 80.464043
 11255/50000: episode: 88, duration: 1.521s, episode steps: 270, steps per second: 178, episode reward: 270.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.259 [-2.407, 0.724], loss: 2.977160, mean_absolute_error: 40.432899, mean_q: 81.670151
 11485/50000: episode: 89, duration: 1.292s, episode steps: 230, steps per second: 178, episode reward: 230.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.302 [-1.012, 2.412], loss: 2.503995, mean_absolute_error: 40.682106, mean_q: 82.188980
 11819/50000: episode: 90, duration: 1.965s, episode steps: 334, steps per second: 170, episode reward: 334.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.215 [-2.427, 0.644], loss: 2.444523, mean_absolute_error: 41.331371, mean_q: 83.575890
 12318/50000: episode: 91, duration: 2.874s, episode steps: 499, steps per second: 174, episode reward: 499.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.142 [-2.434, 0.642], loss: 3.113352, mean_absolute_error: 42.054672, mean_q: 85.008003
 12637/50000: episode: 92, duration: 1.829s, episode steps: 319, steps per second: 174, episode reward: 319.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.225 [-0.997, 2.400], loss: 2.554063, mean_absolute_error: 42.568665, mean_q: 86.094284
 12892/50000: episode: 93, duration: 1.526s, episode steps: 255, steps per second: 167, episode reward: 255.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.276 [-2.433, 0.668], loss: 2.468921, mean_absolute_error: 43.267311, mean_q: 87.494179
 13177/50000: episode: 94, duration: 1.673s, episode steps: 285, steps per second: 170, episode reward: 285.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.250 [-0.908, 2.415], loss: 2.918859, mean_absolute_error: 43.599834, mean_q: 88.032486
 13388/50000: episode: 95, duration: 1.222s, episode steps: 211, steps per second: 173, episode reward: 211.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.325 [-1.043, 2.403], loss: 2.889079, mean_absolute_error: 43.689022, mean_q: 88.261124
 13617/50000: episode: 96, duration: 1.293s, episode steps: 229, steps per second: 177, episode reward: 229.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.305 [-0.850, 2.401], loss: 2.157249, mean_absolute_error: 43.766731, mean_q: 88.402496
 13896/50000: episode: 97, duration: 1.564s, episode steps: 279, steps per second: 178, episode reward: 279.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.261 [-2.430, 0.721], loss: 2.440173, mean_absolute_error: 44.313122, mean_q: 89.476425
 14189/50000: episode: 98, duration: 1.639s, episode steps: 293, steps per second: 179, episode reward: 293.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.244 [-0.974, 2.423], loss: 2.518201, mean_absolute_error: 44.481556, mean_q: 89.728439
 14420/50000: episode: 99, duration: 1.302s, episode steps: 231, steps per second: 177, episode reward: 231.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.296 [-0.827, 2.412], loss: 2.444267, mean_absolute_error: 45.227093, mean_q: 91.300186
 14666/50000: episode: 100, duration: 1.389s, episode steps: 246, steps per second: 177, episode reward: 246.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.290 [-2.423, 0.597], loss: 3.328535, mean_absolute_error: 44.990242, mean_q: 90.761589
 14904/50000: episode: 101, duration: 1.359s, episode steps: 238, steps per second: 175, episode reward: 238.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.297 [-2.423, 0.513], loss: 2.466120, mean_absolute_error: 45.331490, mean_q: 91.437134
 15200/50000: episode: 102, duration: 1.663s, episode steps: 296, steps per second: 178, episode reward: 296.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.241 [-2.403, 0.787], loss: 2.758909, mean_absolute_error: 45.527988, mean_q: 91.854027
 15456/50000: episode: 103, duration: 1.439s, episode steps: 256, steps per second: 178, episode reward: 256.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.275 [-1.294, 2.612], loss: 2.267475, mean_absolute_error: 45.816299, mean_q: 92.520561
 15672/50000: episode: 104, duration: 1.215s, episode steps: 216, steps per second: 178, episode reward: 216.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.329 [-0.805, 2.420], loss: 3.147322, mean_absolute_error: 46.215385, mean_q: 93.152580
 16090/50000: episode: 105, duration: 2.363s, episode steps: 418, steps per second: 177, episode reward: 418.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.168 [-2.408, 0.758], loss: 2.949382, mean_absolute_error: 46.238323, mean_q: 93.266045
 16365/50000: episode: 106, duration: 1.555s, episode steps: 275, steps per second: 177, episode reward: 275.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.262 [-2.434, 0.666], loss: 2.878222, mean_absolute_error: 46.519741, mean_q: 93.861053
 16640/50000: episode: 107, duration: 1.577s, episode steps: 275, steps per second: 174, episode reward: 275.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.262 [-0.901, 2.437], loss: 3.535540, mean_absolute_error: 46.854961, mean_q: 94.546936
 16887/50000: episode: 108, duration: 1.384s, episode steps: 247, steps per second: 178, episode reward: 247.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.293 [-0.870, 2.437], loss: 2.413096, mean_absolute_error: 46.806126, mean_q: 94.420929
 17148/50000: episode: 109, duration: 1.476s, episode steps: 261, steps per second: 177, episode reward: 261.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.286 [-1.049, 2.408], loss: 3.130587, mean_absolute_error: 46.827854, mean_q: 94.459785
 17449/50000: episode: 110, duration: 1.708s, episode steps: 301, steps per second: 176, episode reward: 301.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.237 [-0.775, 2.425], loss: 2.527836, mean_absolute_error: 47.440929, mean_q: 95.711594
 17708/50000: episode: 111, duration: 1.476s, episode steps: 259, steps per second: 176, episode reward: 259.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.272 [-2.401, 0.555], loss: 3.152692, mean_absolute_error: 47.336575, mean_q: 95.365059
 18138/50000: episode: 112, duration: 2.452s, episode steps: 430, steps per second: 175, episode reward: 430.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.173 [-2.439, 0.861], loss: 2.839331, mean_absolute_error: 47.575485, mean_q: 95.986412
 18346/50000: episode: 113, duration: 1.214s, episode steps: 208, steps per second: 171, episode reward: 208.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.349 [-0.685, 2.428], loss: 2.459828, mean_absolute_error: 48.071918, mean_q: 97.055748
 18608/50000: episode: 114, duration: 1.506s, episode steps: 262, steps per second: 174, episode reward: 262.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.277 [-0.859, 2.404], loss: 3.192906, mean_absolute_error: 47.603374, mean_q: 96.024887
 18894/50000: episode: 115, duration: 1.617s, episode steps: 286, steps per second: 177, episode reward: 286.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.256 [-0.883, 2.421], loss: 2.050190, mean_absolute_error: 47.906723, mean_q: 96.725563
 19118/50000: episode: 116, duration: 1.273s, episode steps: 224, steps per second: 176, episode reward: 224.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.324 [-0.788, 2.404], loss: 2.988705, mean_absolute_error: 47.596966, mean_q: 96.004105
 19407/50000: episode: 117, duration: 1.635s, episode steps: 289, steps per second: 177, episode reward: 289.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.252 [-2.418, 0.776], loss: 2.585763, mean_absolute_error: 48.257706, mean_q: 97.302727
 19722/50000: episode: 118, duration: 1.787s, episode steps: 315, steps per second: 176, episode reward: 315.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.225 [-2.345, 0.819], loss: 2.640891, mean_absolute_error: 48.657112, mean_q: 98.053185
 19953/50000: episode: 119, duration: 1.322s, episode steps: 231, steps per second: 175, episode reward: 231.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.298 [-2.311, 0.796], loss: 2.070857, mean_absolute_error: 48.677986, mean_q: 98.163910
 20235/50000: episode: 120, duration: 1.626s, episode steps: 282, steps per second: 173, episode reward: 282.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.260 [-2.427, 0.679], loss: 2.623894, mean_absolute_error: 48.881668, mean_q: 98.584869
 20521/50000: episode: 121, duration: 1.619s, episode steps: 286, steps per second: 177, episode reward: 286.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.247 [-2.296, 0.607], loss: 3.342406, mean_absolute_error: 48.620525, mean_q: 97.922028
 20725/50000: episode: 122, duration: 1.170s, episode steps: 204, steps per second: 174, episode reward: 204.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.355 [-0.732, 2.429], loss: 2.688132, mean_absolute_error: 48.945251, mean_q: 98.633400
 20949/50000: episode: 123, duration: 1.267s, episode steps: 224, steps per second: 177, episode reward: 224.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.318 [-2.376, 1.046], loss: 3.009390, mean_absolute_error: 48.901875, mean_q: 98.671753
 21194/50000: episode: 124, duration: 1.400s, episode steps: 245, steps per second: 175, episode reward: 245.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.303 [-0.795, 2.415], loss: 3.226999, mean_absolute_error: 48.781155, mean_q: 98.339554
 21440/50000: episode: 125, duration: 1.414s, episode steps: 246, steps per second: 174, episode reward: 246.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.302 [-0.725, 2.407], loss: 3.461905, mean_absolute_error: 49.501652, mean_q: 99.684738
 21663/50000: episode: 126, duration: 1.276s, episode steps: 223, steps per second: 175, episode reward: 223.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.296 [-2.208, 0.596], loss: 2.905991, mean_absolute_error: 49.150742, mean_q: 99.023666
 22009/50000: episode: 127, duration: 1.962s, episode steps: 346, steps per second: 176, episode reward: 346.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: 0.217 [-0.804, 2.416], loss: 2.836584, mean_absolute_error: 48.978447, mean_q: 98.712395
 22245/50000: episode: 128, duration: 1.371s, episode steps: 236, steps per second: 172, episode reward: 236.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.313 [-0.808, 2.417], loss: 3.027142, mean_absolute_error: 49.238438, mean_q: 99.301605
 22478/50000: episode: 129, duration: 1.317s, episode steps: 233, steps per second: 177, episode reward: 233.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.320 [-0.954, 2.419], loss: 2.537298, mean_absolute_error: 49.201485, mean_q: 99.123573
 22686/50000: episode: 130, duration: 1.210s, episode steps: 208, steps per second: 172, episode reward: 208.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.356 [-0.960, 2.431], loss: 2.414416, mean_absolute_error: 49.487167, mean_q: 99.749100
 23045/50000: episode: 131, duration: 2.037s, episode steps: 359, steps per second: 176, episode reward: 359.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.199 [-2.304, 0.913], loss: 3.268449, mean_absolute_error: 49.263088, mean_q: 99.211113
 23265/50000: episode: 132, duration: 1.250s, episode steps: 220, steps per second: 176, episode reward: 220.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.340 [-0.955, 2.405], loss: 2.685675, mean_absolute_error: 49.486393, mean_q: 99.710793
 23495/50000: episode: 133, duration: 1.309s, episode steps: 230, steps per second: 176, episode reward: 230.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.322 [-0.712, 2.403], loss: 3.042479, mean_absolute_error: 49.186329, mean_q: 99.013687
 23850/50000: episode: 134, duration: 2.047s, episode steps: 355, steps per second: 173, episode reward: 355.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.209 [-0.894, 2.414], loss: 2.996777, mean_absolute_error: 49.724457, mean_q: 100.231888
 24255/50000: episode: 135, duration: 2.348s, episode steps: 405, steps per second: 172, episode reward: 405.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.188 [-0.766, 2.392], loss: 3.611172, mean_absolute_error: 49.621410, mean_q: 100.023483
 24529/50000: episode: 136, duration: 1.553s, episode steps: 274, steps per second: 176, episode reward: 274.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.274 [-1.063, 2.392], loss: 2.285308, mean_absolute_error: 49.587143, mean_q: 100.081963
 24834/50000: episode: 137, duration: 1.731s, episode steps: 305, steps per second: 176, episode reward: 305.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.251 [-0.816, 2.415], loss: 3.641432, mean_absolute_error: 49.698032, mean_q: 100.142105
 25155/50000: episode: 138, duration: 1.831s, episode steps: 321, steps per second: 175, episode reward: 321.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.228 [-0.799, 2.415], loss: 3.212594, mean_absolute_error: 50.048172, mean_q: 100.801193
 25418/50000: episode: 139, duration: 1.537s, episode steps: 263, steps per second: 171, episode reward: 263.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.285 [-0.852, 2.386], loss: 3.056973, mean_absolute_error: 49.837101, mean_q: 100.400696
 25642/50000: episode: 140, duration: 1.300s, episode steps: 224, steps per second: 172, episode reward: 224.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.295 [-2.178, 0.945], loss: 2.826450, mean_absolute_error: 49.665504, mean_q: 100.115807
 25900/50000: episode: 141, duration: 1.537s, episode steps: 258, steps per second: 168, episode reward: 258.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.280 [-0.839, 2.402], loss: 3.276107, mean_absolute_error: 50.201393, mean_q: 101.054543
 26119/50000: episode: 142, duration: 1.288s, episode steps: 219, steps per second: 170, episode reward: 219.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.325 [-2.403, 0.933], loss: 4.042814, mean_absolute_error: 50.377323, mean_q: 101.471642
 26326/50000: episode: 143, duration: 1.209s, episode steps: 207, steps per second: 171, episode reward: 207.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.357 [-0.882, 2.427], loss: 2.059538, mean_absolute_error: 50.118057, mean_q: 101.021721
 26625/50000: episode: 144, duration: 1.747s, episode steps: 299, steps per second: 171, episode reward: 299.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.241 [-1.035, 2.286], loss: 3.007793, mean_absolute_error: 49.628384, mean_q: 100.072968
 26870/50000: episode: 145, duration: 1.430s, episode steps: 245, steps per second: 171, episode reward: 245.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.309 [-0.907, 2.415], loss: 2.332101, mean_absolute_error: 50.193329, mean_q: 101.130974
 27087/50000: episode: 146, duration: 1.261s, episode steps: 217, steps per second: 172, episode reward: 217.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.333 [-2.409, 1.035], loss: 3.077052, mean_absolute_error: 49.330662, mean_q: 99.435165
 27326/50000: episode: 147, duration: 1.414s, episode steps: 239, steps per second: 169, episode reward: 239.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.301 [-2.435, 0.835], loss: 2.730754, mean_absolute_error: 49.997211, mean_q: 100.771782
 27545/50000: episode: 148, duration: 1.277s, episode steps: 219, steps per second: 171, episode reward: 219.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.319 [-1.117, 2.239], loss: 2.646991, mean_absolute_error: 50.181007, mean_q: 101.089790
 27838/50000: episode: 149, duration: 1.675s, episode steps: 293, steps per second: 175, episode reward: 293.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.258 [-0.884, 2.417], loss: 3.136711, mean_absolute_error: 49.799049, mean_q: 100.266106
 28076/50000: episode: 150, duration: 1.386s, episode steps: 238, steps per second: 172, episode reward: 238.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.293 [-2.255, 1.100], loss: 2.870753, mean_absolute_error: 49.597641, mean_q: 100.035683
 28340/50000: episode: 151, duration: 1.514s, episode steps: 264, steps per second: 174, episode reward: 264.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.286 [-0.886, 2.410], loss: 3.529980, mean_absolute_error: 49.936428, mean_q: 100.606300
 28603/50000: episode: 152, duration: 1.505s, episode steps: 263, steps per second: 175, episode reward: 263.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.282 [-2.406, 0.871], loss: 3.067802, mean_absolute_error: 50.029850, mean_q: 100.759598
 28864/50000: episode: 153, duration: 1.500s, episode steps: 261, steps per second: 174, episode reward: 261.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.285 [-0.983, 2.408], loss: 2.664972, mean_absolute_error: 49.480747, mean_q: 99.719856
 29110/50000: episode: 154, duration: 1.422s, episode steps: 246, steps per second: 173, episode reward: 246.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.302 [-1.095, 2.347], loss: 2.344900, mean_absolute_error: 49.867393, mean_q: 100.548828
 29355/50000: episode: 155, duration: 1.430s, episode steps: 245, steps per second: 171, episode reward: 245.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.295 [-0.781, 2.300], loss: 2.700966, mean_absolute_error: 49.774731, mean_q: 100.267349
 29740/50000: episode: 156, duration: 2.189s, episode steps: 385, steps per second: 176, episode reward: 385.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.197 [-2.415, 0.852], loss: 2.691336, mean_absolute_error: 49.405266, mean_q: 99.470596
 29998/50000: episode: 157, duration: 1.488s, episode steps: 258, steps per second: 173, episode reward: 258.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.285 [-0.758, 2.322], loss: 3.167597, mean_absolute_error: 49.512257, mean_q: 99.667969
 30290/50000: episode: 158, duration: 1.673s, episode steps: 292, steps per second: 175, episode reward: 292.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.253 [-2.426, 0.910], loss: 3.005404, mean_absolute_error: 49.564632, mean_q: 99.718025
 30564/50000: episode: 159, duration: 1.567s, episode steps: 274, steps per second: 175, episode reward: 274.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.270 [-0.845, 2.295], loss: 3.273933, mean_absolute_error: 49.629803, mean_q: 99.927338
 31007/50000: episode: 160, duration: 2.533s, episode steps: 443, steps per second: 175, episode reward: 443.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: 0.167 [-0.900, 2.283], loss: 3.022830, mean_absolute_error: 49.428417, mean_q: 99.548500
 31237/50000: episode: 161, duration: 1.351s, episode steps: 230, steps per second: 170, episode reward: 230.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.329 [-1.007, 2.405], loss: 2.092982, mean_absolute_error: 49.316906, mean_q: 99.428719
 31453/50000: episode: 162, duration: 1.237s, episode steps: 216, steps per second: 175, episode reward: 216.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.317 [-0.817, 2.157], loss: 2.346784, mean_absolute_error: 49.113174, mean_q: 99.019173
 31736/50000: episode: 163, duration: 1.627s, episode steps: 283, steps per second: 174, episode reward: 283.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.267 [-0.927, 2.337], loss: 2.673792, mean_absolute_error: 49.336643, mean_q: 99.438782
 31967/50000: episode: 164, duration: 1.329s, episode steps: 231, steps per second: 174, episode reward: 231.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.328 [-0.766, 2.348], loss: 2.875353, mean_absolute_error: 49.352982, mean_q: 99.412010
 32302/50000: episode: 165, duration: 1.930s, episode steps: 335, steps per second: 174, episode reward: 335.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.220 [-2.264, 0.797], loss: 2.520817, mean_absolute_error: 49.363102, mean_q: 99.480255
 32555/50000: episode: 166, duration: 1.480s, episode steps: 253, steps per second: 171, episode reward: 253.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.297 [-0.891, 2.342], loss: 2.927912, mean_absolute_error: 49.123676, mean_q: 98.938652
 32847/50000: episode: 167, duration: 1.687s, episode steps: 292, steps per second: 173, episode reward: 292.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.246 [-1.437, 2.622], loss: 2.244925, mean_absolute_error: 49.138664, mean_q: 99.042091
 33139/50000: episode: 168, duration: 1.710s, episode steps: 292, steps per second: 171, episode reward: 292.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.263 [-1.074, 2.415], loss: 2.394269, mean_absolute_error: 49.353291, mean_q: 99.347061
 33421/50000: episode: 169, duration: 1.624s, episode steps: 282, steps per second: 174, episode reward: 282.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.270 [-2.432, 0.782], loss: 2.657443, mean_absolute_error: 49.023201, mean_q: 98.702515
 33685/50000: episode: 170, duration: 1.523s, episode steps: 264, steps per second: 173, episode reward: 264.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.281 [-0.828, 2.315], loss: 2.343424, mean_absolute_error: 48.883121, mean_q: 98.362427
 33975/50000: episode: 171, duration: 1.679s, episode steps: 290, steps per second: 173, episode reward: 290.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.247 [-0.827, 2.254], loss: 3.086406, mean_absolute_error: 49.213902, mean_q: 99.032768
 34204/50000: episode: 172, duration: 1.326s, episode steps: 229, steps per second: 173, episode reward: 229.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.327 [-1.121, 2.449], loss: 2.812930, mean_absolute_error: 49.172302, mean_q: 99.005203
 34547/50000: episode: 173, duration: 2.004s, episode steps: 343, steps per second: 171, episode reward: 343.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.217 [-0.921, 2.418], loss: 2.038178, mean_absolute_error: 48.336708, mean_q: 97.351242
 34841/50000: episode: 174, duration: 1.706s, episode steps: 294, steps per second: 172, episode reward: 294.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.256 [-0.959, 2.376], loss: 2.217366, mean_absolute_error: 48.904152, mean_q: 98.481422
 35108/50000: episode: 175, duration: 1.539s, episode steps: 267, steps per second: 174, episode reward: 267.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.267 [-1.021, 2.409], loss: 2.425623, mean_absolute_error: 48.630405, mean_q: 97.981308
 35365/50000: episode: 176, duration: 1.479s, episode steps: 257, steps per second: 174, episode reward: 257.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.288 [-2.409, 0.965], loss: 2.435965, mean_absolute_error: 48.915382, mean_q: 98.398407
 35625/50000: episode: 177, duration: 1.487s, episode steps: 260, steps per second: 175, episode reward: 260.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.289 [-1.045, 2.401], loss: 1.980496, mean_absolute_error: 48.538536, mean_q: 97.756035
 35885/50000: episode: 178, duration: 1.503s, episode steps: 260, steps per second: 173, episode reward: 260.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.287 [-1.105, 2.389], loss: 1.650505, mean_absolute_error: 48.552151, mean_q: 97.740265
 36126/50000: episode: 179, duration: 1.404s, episode steps: 241, steps per second: 172, episode reward: 241.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.294 [-1.382, 2.423], loss: 1.906489, mean_absolute_error: 48.642269, mean_q: 97.942581
 36455/50000: episode: 180, duration: 1.876s, episode steps: 329, steps per second: 175, episode reward: 329.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.224 [-1.185, 2.418], loss: 2.098560, mean_absolute_error: 48.222332, mean_q: 97.028587
 36790/50000: episode: 181, duration: 1.957s, episode steps: 335, steps per second: 171, episode reward: 335.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.208 [-1.121, 2.216], loss: 1.815305, mean_absolute_error: 48.436714, mean_q: 97.436638
 37235/50000: episode: 182, duration: 2.554s, episode steps: 445, steps per second: 174, episode reward: 445.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.173 [-2.418, 0.907], loss: 3.154366, mean_absolute_error: 48.445190, mean_q: 97.359901
 37624/50000: episode: 183, duration: 2.245s, episode steps: 389, steps per second: 173, episode reward: 389.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.194 [-0.870, 2.419], loss: 1.987747, mean_absolute_error: 48.312359, mean_q: 97.249298
 37963/50000: episode: 184, duration: 1.956s, episode steps: 339, steps per second: 173, episode reward: 339.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.221 [-2.429, 1.012], loss: 1.781579, mean_absolute_error: 48.052925, mean_q: 96.726501
 38205/50000: episode: 185, duration: 1.395s, episode steps: 242, steps per second: 174, episode reward: 242.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.303 [-1.097, 2.318], loss: 1.980987, mean_absolute_error: 48.134354, mean_q: 96.891350
 38492/50000: episode: 186, duration: 1.684s, episode steps: 287, steps per second: 170, episode reward: 287.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.255 [-0.882, 2.284], loss: 2.162835, mean_absolute_error: 47.518360, mean_q: 95.616188
 38806/50000: episode: 187, duration: 1.814s, episode steps: 314, steps per second: 173, episode reward: 314.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.233 [-2.416, 0.839], loss: 1.868937, mean_absolute_error: 47.884041, mean_q: 96.391342
 39069/50000: episode: 188, duration: 1.535s, episode steps: 263, steps per second: 171, episode reward: 263.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.268 [-1.443, 2.204], loss: 2.046490, mean_absolute_error: 47.883259, mean_q: 96.398880
 39316/50000: episode: 189, duration: 1.441s, episode steps: 247, steps per second: 171, episode reward: 247.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.296 [-1.349, 2.341], loss: 1.944203, mean_absolute_error: 48.059921, mean_q: 96.656136
 39604/50000: episode: 190, duration: 1.694s, episode steps: 288, steps per second: 170, episode reward: 288.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.254 [-2.433, 0.753], loss: 2.155506, mean_absolute_error: 48.026165, mean_q: 96.559128
 39929/50000: episode: 191, duration: 1.971s, episode steps: 325, steps per second: 165, episode reward: 325.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.216 [-1.736, 2.807], loss: 1.333537, mean_absolute_error: 47.617878, mean_q: 95.811646
 40349/50000: episode: 192, duration: 2.516s, episode steps: 420, steps per second: 167, episode reward: 420.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.176 [-1.568, 2.808], loss: 1.540098, mean_absolute_error: 48.107544, mean_q: 96.727821
 40685/50000: episode: 193, duration: 1.999s, episode steps: 336, steps per second: 168, episode reward: 336.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.216 [-2.421, 0.841], loss: 1.592259, mean_absolute_error: 47.517776, mean_q: 95.517982
 40969/50000: episode: 194, duration: 1.690s, episode steps: 284, steps per second: 168, episode reward: 284.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.252 [-2.407, 0.873], loss: 1.462054, mean_absolute_error: 47.803684, mean_q: 96.054482
 41377/50000: episode: 195, duration: 2.399s, episode steps: 408, steps per second: 170, episode reward: 408.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.174 [-2.419, 0.976], loss: 1.623854, mean_absolute_error: 47.707672, mean_q: 95.936607
 41698/50000: episode: 196, duration: 1.859s, episode steps: 321, steps per second: 173, episode reward: 321.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.227 [-1.241, 2.429], loss: 1.319257, mean_absolute_error: 47.606861, mean_q: 95.770569
 42059/50000: episode: 197, duration: 2.107s, episode steps: 361, steps per second: 171, episode reward: 361.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.207 [-1.758, 2.408], loss: 2.011116, mean_absolute_error: 47.350956, mean_q: 95.203133
 42365/50000: episode: 198, duration: 1.764s, episode steps: 306, steps per second: 173, episode reward: 306.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.238 [-1.365, 2.580], loss: 1.260025, mean_absolute_error: 47.351204, mean_q: 95.278770
 42649/50000: episode: 199, duration: 1.660s, episode steps: 284, steps per second: 171, episode reward: 284.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.258 [-1.599, 2.416], loss: 2.597933, mean_absolute_error: 47.093967, mean_q: 94.600410
 42993/50000: episode: 200, duration: 2.005s, episode steps: 344, steps per second: 172, episode reward: 344.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.213 [-1.281, 2.573], loss: 2.099523, mean_absolute_error: 46.920616, mean_q: 94.329498
 43329/50000: episode: 201, duration: 1.964s, episode steps: 336, steps per second: 171, episode reward: 336.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.503 [0.000, 1.000], mean observation: 0.217 [-1.211, 2.227], loss: 2.298967, mean_absolute_error: 47.179169, mean_q: 94.798912
 43647/50000: episode: 202, duration: 1.860s, episode steps: 318, steps per second: 171, episode reward: 318.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: 0.245 [-1.420, 2.380], loss: 1.753959, mean_absolute_error: 46.971577, mean_q: 94.416206
 44059/50000: episode: 203, duration: 2.403s, episode steps: 412, steps per second: 171, episode reward: 412.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.184 [-1.459, 2.606], loss: 1.837437, mean_absolute_error: 46.690647, mean_q: 93.918037
 44441/50000: episode: 204, duration: 2.233s, episode steps: 382, steps per second: 171, episode reward: 382.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.191 [-1.217, 2.409], loss: 1.607076, mean_absolute_error: 46.796219, mean_q: 94.090050
 44892/50000: episode: 205, duration: 2.677s, episode steps: 451, steps per second: 168, episode reward: 451.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: 0.177 [-1.463, 2.408], loss: 2.142886, mean_absolute_error: 46.900898, mean_q: 94.325615
 45163/50000: episode: 206, duration: 1.665s, episode steps: 271, steps per second: 163, episode reward: 271.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.277 [-1.690, 2.488], loss: 1.765902, mean_absolute_error: 46.480183, mean_q: 93.500969
 45585/50000: episode: 207, duration: 2.591s, episode steps: 422, steps per second: 163, episode reward: 422.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.176 [-1.435, 2.406], loss: 1.478169, mean_absolute_error: 46.604275, mean_q: 93.722473
 45898/50000: episode: 208, duration: 1.953s, episode steps: 313, steps per second: 160, episode reward: 313.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.232 [-1.425, 2.217], loss: 1.318926, mean_absolute_error: 46.489990, mean_q: 93.547142
 46241/50000: episode: 209, duration: 2.157s, episode steps: 343, steps per second: 159, episode reward: 343.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.229 [-1.433, 2.472], loss: 1.063287, mean_absolute_error: 46.423088, mean_q: 93.382996
 46741/50000: episode: 210, duration: 3.105s, episode steps: 500, steps per second: 161, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.504 [0.000, 1.000], mean observation: 0.126 [-1.478, 2.202], loss: 1.274049, mean_absolute_error: 46.570122, mean_q: 93.680641
 47241/50000: episode: 211, duration: 3.102s, episode steps: 500, steps per second: 161, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.504 [0.000, 1.000], mean observation: 0.060 [-1.213, 1.470], loss: 1.101146, mean_absolute_error: 46.607311, mean_q: 93.656494
 47741/50000: episode: 212, duration: 2.916s, episode steps: 500, steps per second: 171, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.502 [0.000, 1.000], mean observation: 0.011 [-0.841, 0.953], loss: 1.203750, mean_absolute_error: 46.974464, mean_q: 94.278389
 48159/50000: episode: 213, duration: 2.393s, episode steps: 418, steps per second: 175, episode reward: 418.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.111 [-1.406, 1.915], loss: 1.355174, mean_absolute_error: 46.697861, mean_q: 93.724747
 48569/50000: episode: 214, duration: 2.369s, episode steps: 410, steps per second: 173, episode reward: 410.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.186 [-1.337, 2.427], loss: 2.382211, mean_absolute_error: 46.784107, mean_q: 93.855431
 49069/50000: episode: 215, duration: 2.928s, episode steps: 500, steps per second: 171, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.013 [-1.115, 0.868], loss: 1.845368, mean_absolute_error: 46.287579, mean_q: 92.877846
 49569/50000: episode: 216, duration: 2.896s, episode steps: 500, steps per second: 173, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.009 [-1.143, 1.141], loss: 2.643433, mean_absolute_error: 46.639584, mean_q: 93.514778