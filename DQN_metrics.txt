    31/50000: episode: 1, duration: 1.729s, episode steps: 31, steps per second: 18, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.419 [0.000, 1.000], mean observation: 0.013 [-1.185, 1.776], loss: 0.462173, mean_absolute_error: 0.520752, mean_q: 0.096439
    44/50000: episode: 2, duration: 0.084s, episode steps: 13, steps per second: 154, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.069 [-1.867, 1.224], loss: 0.352699, mean_absolute_error: 0.544416, mean_q: 0.290915
    64/50000: episode: 3, duration: 0.142s, episode steps: 20, steps per second: 141, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.070 [-1.375, 0.833], loss: 0.233226, mean_absolute_error: 0.553622, mean_q: 0.481963
    83/50000: episode: 4, duration: 0.120s, episode steps: 19, steps per second: 158, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.074 [-0.817, 1.505], loss: 0.115771, mean_absolute_error: 0.604906, mean_q: 0.841101
    98/50000: episode: 5, duration: 0.099s, episode steps: 15, steps per second: 151, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.090 [-1.368, 0.825], loss: 0.060949, mean_absolute_error: 0.689512, mean_q: 1.187350
   114/50000: episode: 6, duration: 0.103s, episode steps: 16, steps per second: 156, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.069 [-2.711, 1.766], loss: 0.041591, mean_absolute_error: 0.727885, mean_q: 1.316640
   128/50000: episode: 7, duration: 0.092s, episode steps: 14, steps per second: 153, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.111 [-0.934, 1.483], loss: 0.034899, mean_absolute_error: 0.761641, mean_q: 1.435475
   151/50000: episode: 8, duration: 0.148s, episode steps: 23, steps per second: 156, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.304 [0.000, 1.000], mean observation: 0.050 [-1.760, 2.679], loss: 0.020848, mean_absolute_error: 0.825447, mean_q: 1.621850
   162/50000: episode: 9, duration: 0.072s, episode steps: 11, steps per second: 153, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.133 [-1.133, 1.962], loss: 0.033472, mean_absolute_error: 0.876673, mean_q: 1.749073
   175/50000: episode: 10, duration: 0.085s, episode steps: 13, steps per second: 153, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.118 [-2.439, 1.549], loss: 0.040221, mean_absolute_error: 0.940421, mean_q: 1.900732
   208/50000: episode: 11, duration: 0.208s, episode steps: 33, steps per second: 159, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.424 [0.000, 1.000], mean observation: 0.004 [-1.381, 1.780], loss: 0.037214, mean_absolute_error: 0.993950, mean_q: 2.016667
   223/50000: episode: 12, duration: 0.107s, episode steps: 15, steps per second: 140, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.096 [-1.845, 1.032], loss: 0.049411, mean_absolute_error: 1.103708, mean_q: 2.210941
   235/50000: episode: 13, duration: 0.079s, episode steps: 12, steps per second: 152, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.124 [-1.329, 2.154], loss: 0.055344, mean_absolute_error: 1.173260, mean_q: 2.366916
   249/50000: episode: 14, duration: 0.096s, episode steps: 14, steps per second: 146, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.103 [-0.965, 1.601], loss: 0.058532, mean_absolute_error: 1.220065, mean_q: 2.465080
   264/50000: episode: 15, duration: 0.099s, episode steps: 15, steps per second: 152, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.086 [-1.187, 2.013], loss: 0.058335, mean_absolute_error: 1.309274, mean_q: 2.637853
   289/50000: episode: 16, duration: 0.163s, episode steps: 25, steps per second: 153, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.360 [0.000, 1.000], mean observation: 0.054 [-1.365, 2.263], loss: 0.086458, mean_absolute_error: 1.378768, mean_q: 2.748358
   308/50000: episode: 17, duration: 0.120s, episode steps: 19, steps per second: 158, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.684 [0.000, 1.000], mean observation: -0.100 [-2.458, 1.398], loss: 0.083784, mean_absolute_error: 1.489895, mean_q: 2.985679
   322/50000: episode: 18, duration: 0.094s, episode steps: 14, steps per second: 149, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.100 [-1.382, 2.208], loss: 0.130680, mean_absolute_error: 1.577877, mean_q: 3.059857
   359/50000: episode: 19, duration: 0.230s, episode steps: 37, steps per second: 161, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: -0.011 [-1.245, 0.625], loss: 0.128095, mean_absolute_error: 1.673273, mean_q: 3.294010
   377/50000: episode: 20, duration: 0.127s, episode steps: 18, steps per second: 141, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.085 [-0.971, 1.471], loss: 0.150520, mean_absolute_error: 1.778720, mean_q: 3.481179
   386/50000: episode: 21, duration: 0.060s, episode steps: 9, steps per second: 150, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.155 [-0.964, 1.743], loss: 0.160706, mean_absolute_error: 1.887498, mean_q: 3.698248
   429/50000: episode: 22, duration: 0.268s, episode steps: 43, steps per second: 160, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.395 [0.000, 1.000], mean observation: 0.043 [-1.770, 2.753], loss: 0.146749, mean_absolute_error: 1.952648, mean_q: 3.832726
   444/50000: episode: 23, duration: 0.097s, episode steps: 15, steps per second: 154, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.072 [-2.707, 1.794], loss: 0.179906, mean_absolute_error: 2.075476, mean_q: 4.121659
   460/50000: episode: 24, duration: 0.101s, episode steps: 16, steps per second: 159, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.100 [-0.586, 1.215], loss: 0.149710, mean_absolute_error: 2.145880, mean_q: 4.209060
   484/50000: episode: 25, duration: 0.156s, episode steps: 24, steps per second: 154, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: 0.040 [-0.805, 1.402], loss: 0.188366, mean_absolute_error: 2.244536, mean_q: 4.427539
   497/50000: episode: 26, duration: 0.082s, episode steps: 13, steps per second: 158, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.108 [-0.766, 1.343], loss: 0.239616, mean_absolute_error: 2.327628, mean_q: 4.509193
   510/50000: episode: 27, duration: 0.085s, episode steps: 13, steps per second: 152, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.085 [-1.384, 2.229], loss: 0.234737, mean_absolute_error: 2.413644, mean_q: 4.648050
   532/50000: episode: 28, duration: 0.150s, episode steps: 22, steps per second: 146, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.074 [-1.143, 0.733], loss: 0.181015, mean_absolute_error: 2.453977, mean_q: 4.747606
   547/50000: episode: 29, duration: 0.098s, episode steps: 15, steps per second: 153, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.115 [-0.942, 1.773], loss: 0.192081, mean_absolute_error: 2.530632, mean_q: 4.949361
   557/50000: episode: 30, duration: 0.068s, episode steps: 10, steps per second: 148, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.138 [-1.524, 2.546], loss: 0.299307, mean_absolute_error: 2.586266, mean_q: 4.984053
   605/50000: episode: 31, duration: 0.303s, episode steps: 48, steps per second: 158, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: -0.098 [-1.499, 0.480], loss: 0.223936, mean_absolute_error: 2.721186, mean_q: 5.230966
   619/50000: episode: 32, duration: 0.092s, episode steps: 14, steps per second: 152, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.085 [-0.644, 1.246], loss: 0.222922, mean_absolute_error: 2.832808, mean_q: 5.451890
   631/50000: episode: 33, duration: 0.077s, episode steps: 12, steps per second: 155, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.114 [-0.955, 1.508], loss: 0.280619, mean_absolute_error: 2.913803, mean_q: 5.594545
   654/50000: episode: 34, duration: 0.146s, episode steps: 23, steps per second: 158, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.089 [-0.552, 1.284], loss: 0.238156, mean_absolute_error: 2.958582, mean_q: 5.649810
   669/50000: episode: 35, duration: 0.096s, episode steps: 15, steps per second: 156, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.101 [-0.818, 1.565], loss: 0.282031, mean_absolute_error: 3.028489, mean_q: 5.751748
   690/50000: episode: 36, duration: 0.146s, episode steps: 21, steps per second: 144, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.381 [0.000, 1.000], mean observation: 0.065 [-1.013, 1.852], loss: 0.211309, mean_absolute_error: 3.096119, mean_q: 5.967826
   737/50000: episode: 37, duration: 0.291s, episode steps: 47, steps per second: 162, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.084 [-1.039, 0.965], loss: 0.269584, mean_absolute_error: 3.221706, mean_q: 6.217795
   748/50000: episode: 38, duration: 0.073s, episode steps: 11, steps per second: 152, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.117 [-1.159, 1.986], loss: 0.222184, mean_absolute_error: 3.326948, mean_q: 6.508374
   787/50000: episode: 39, duration: 0.250s, episode steps: 39, steps per second: 156, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.073 [-0.626, 1.726], loss: 0.298241, mean_absolute_error: 3.398300, mean_q: 6.575160
   876/50000: episode: 40, duration: 0.555s, episode steps: 89, steps per second: 160, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: 0.082 [-0.958, 1.180], loss: 0.364272, mean_absolute_error: 3.668661, mean_q: 7.175972
   932/50000: episode: 41, duration: 0.355s, episode steps: 56, steps per second: 158, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.174 [-0.882, 1.848], loss: 0.315008, mean_absolute_error: 3.977457, mean_q: 7.845994
   978/50000: episode: 42, duration: 0.289s, episode steps: 46, steps per second: 159, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: 0.011 [-0.847, 1.616], loss: 0.265045, mean_absolute_error: 4.197290, mean_q: 8.345032
  1005/50000: episode: 43, duration: 0.190s, episode steps: 27, steps per second: 142, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.005 [-1.245, 0.824], loss: 0.336632, mean_absolute_error: 4.344253, mean_q: 8.658427
  1123/50000: episode: 44, duration: 0.823s, episode steps: 118, steps per second: 143, episode reward: 118.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.122 [-1.103, 1.176], loss: 0.358444, mean_absolute_error: 4.652878, mean_q: 9.233859
  1159/50000: episode: 45, duration: 0.273s, episode steps: 36, steps per second: 132, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.097 [-0.549, 1.081], loss: 0.273760, mean_absolute_error: 5.033281, mean_q: 10.105188
  1241/50000: episode: 46, duration: 0.584s, episode steps: 82, steps per second: 140, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.074 [-1.273, 0.842], loss: 0.373407, mean_absolute_error: 5.276927, mean_q: 10.574589
  1328/50000: episode: 47, duration: 0.629s, episode steps: 87, steps per second: 138, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.218 [-1.813, 0.748], loss: 0.440016, mean_absolute_error: 5.681759, mean_q: 11.420763
  1464/50000: episode: 48, duration: 0.992s, episode steps: 136, steps per second: 137, episode reward: 136.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.179 [-1.319, 1.032], loss: 0.348960, mean_absolute_error: 6.169487, mean_q: 12.484807
  1603/50000: episode: 49, duration: 0.988s, episode steps: 139, steps per second: 141, episode reward: 139.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.250 [-1.317, 2.053], loss: 0.555011, mean_absolute_error: 6.824183, mean_q: 13.755137
  1683/50000: episode: 50, duration: 0.583s, episode steps: 80, steps per second: 137, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: -0.280 [-1.844, 0.590], loss: 0.708785, mean_absolute_error: 7.359047, mean_q: 14.836370
  1873/50000: episode: 51, duration: 1.346s, episode steps: 190, steps per second: 141, episode reward: 190.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.141 [-1.037, 1.354], loss: 0.902548, mean_absolute_error: 7.952768, mean_q: 16.040045
  1988/50000: episode: 52, duration: 0.820s, episode steps: 115, steps per second: 140, episode reward: 115.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.461 [0.000, 1.000], mean observation: -0.207 [-1.701, 1.052], loss: 0.943667, mean_absolute_error: 8.719868, mean_q: 17.599972
  2086/50000: episode: 53, duration: 0.706s, episode steps: 98, steps per second: 139, episode reward: 98.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.253 [-1.521, 1.105], loss: 0.656707, mean_absolute_error: 9.190284, mean_q: 18.648235
  2203/50000: episode: 54, duration: 0.842s, episode steps: 117, steps per second: 139, episode reward: 117.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.165 [-1.117, 0.884], loss: 1.004947, mean_absolute_error: 9.720198, mean_q: 19.663158
  2313/50000: episode: 55, duration: 0.789s, episode steps: 110, steps per second: 139, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.247 [-1.456, 0.667], loss: 0.985152, mean_absolute_error: 10.169116, mean_q: 20.541286
  2399/50000: episode: 56, duration: 0.602s, episode steps: 86, steps per second: 143, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.453 [0.000, 1.000], mean observation: -0.299 [-1.664, 0.955], loss: 1.342366, mean_absolute_error: 10.608091, mean_q: 21.390018
  2525/50000: episode: 57, duration: 0.775s, episode steps: 126, steps per second: 163, episode reward: 126.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.193 [-1.310, 0.843], loss: 1.244294, mean_absolute_error: 11.023990, mean_q: 22.256395
  2635/50000: episode: 58, duration: 0.678s, episode steps: 110, steps per second: 162, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.233 [-1.262, 0.819], loss: 1.042155, mean_absolute_error: 11.492437, mean_q: 23.283735
  2902/50000: episode: 59, duration: 1.662s, episode steps: 267, steps per second: 161, episode reward: 267.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.126 [-1.869, 0.887], loss: 1.176041, mean_absolute_error: 12.356016, mean_q: 25.056852
  3025/50000: episode: 60, duration: 0.764s, episode steps: 123, steps per second: 161, episode reward: 123.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.235 [-1.705, 0.713], loss: 1.065391, mean_absolute_error: 13.312965, mean_q: 27.054955
  3138/50000: episode: 61, duration: 0.703s, episode steps: 113, steps per second: 161, episode reward: 113.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.226 [-1.338, 1.151], loss: 0.972392, mean_absolute_error: 13.747091, mean_q: 28.004063
  3251/50000: episode: 62, duration: 0.692s, episode steps: 113, steps per second: 163, episode reward: 113.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.320 [-1.833, 0.767], loss: 1.314061, mean_absolute_error: 14.269607, mean_q: 28.984467
  3372/50000: episode: 63, duration: 0.761s, episode steps: 121, steps per second: 159, episode reward: 121.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.292 [-1.795, 0.709], loss: 1.246324, mean_absolute_error: 14.765546, mean_q: 30.033134
  3492/50000: episode: 64, duration: 0.746s, episode steps: 120, steps per second: 161, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.257 [-1.776, 0.741], loss: 1.543824, mean_absolute_error: 15.349784, mean_q: 31.205563
  3608/50000: episode: 65, duration: 0.716s, episode steps: 116, steps per second: 162, episode reward: 116.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.286 [-1.505, 0.787], loss: 0.965880, mean_absolute_error: 15.673398, mean_q: 31.941576
  3739/50000: episode: 66, duration: 0.801s, episode steps: 131, steps per second: 164, episode reward: 131.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.294 [-1.829, 0.799], loss: 1.647946, mean_absolute_error: 16.283972, mean_q: 33.066647
  3868/50000: episode: 67, duration: 0.802s, episode steps: 129, steps per second: 161, episode reward: 129.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.304 [-1.844, 0.868], loss: 1.891711, mean_absolute_error: 16.623253, mean_q: 33.813675
  4009/50000: episode: 68, duration: 0.875s, episode steps: 141, steps per second: 161, episode reward: 141.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.264 [-1.807, 0.833], loss: 1.241396, mean_absolute_error: 17.185675, mean_q: 34.949745
  4132/50000: episode: 69, duration: 0.765s, episode steps: 123, steps per second: 161, episode reward: 123.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.300 [-1.646, 0.771], loss: 1.240435, mean_absolute_error: 17.764503, mean_q: 36.197350
  4283/50000: episode: 70, duration: 0.936s, episode steps: 151, steps per second: 161, episode reward: 151.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.280 [-1.816, 0.729], loss: 1.568701, mean_absolute_error: 18.466053, mean_q: 37.530582
  4435/50000: episode: 71, duration: 0.944s, episode steps: 152, steps per second: 161, episode reward: 152.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.269 [-1.883, 1.028], loss: 1.594927, mean_absolute_error: 18.979239, mean_q: 38.589977
  4601/50000: episode: 72, duration: 1.044s, episode steps: 166, steps per second: 159, episode reward: 166.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.257 [-1.801, 0.824], loss: 1.363492, mean_absolute_error: 19.447481, mean_q: 39.564194
  4723/50000: episode: 73, duration: 0.750s, episode steps: 122, steps per second: 163, episode reward: 122.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.451 [0.000, 1.000], mean observation: -0.337 [-2.232, 0.567], loss: 1.587888, mean_absolute_error: 20.125023, mean_q: 40.867008
  4887/50000: episode: 74, duration: 1.012s, episode steps: 164, steps per second: 162, episode reward: 164.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.241 [-1.989, 0.912], loss: 1.735828, mean_absolute_error: 20.517803, mean_q: 41.782597
  5033/50000: episode: 75, duration: 0.909s, episode steps: 146, steps per second: 161, episode reward: 146.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.268 [-2.016, 0.812], loss: 1.342584, mean_absolute_error: 21.058399, mean_q: 42.885406
  5257/50000: episode: 76, duration: 1.395s, episode steps: 224, steps per second: 161, episode reward: 224.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.193 [-2.365, 0.993], loss: 1.706175, mean_absolute_error: 21.851994, mean_q: 44.299309
  5387/50000: episode: 77, duration: 0.798s, episode steps: 130, steps per second: 163, episode reward: 130.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.307 [-1.992, 0.856], loss: 1.523257, mean_absolute_error: 22.428846, mean_q: 45.553249
  5586/50000: episode: 78, duration: 1.237s, episode steps: 199, steps per second: 161, episode reward: 199.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.256 [-2.320, 0.797], loss: 1.798136, mean_absolute_error: 22.932577, mean_q: 46.612221
  5757/50000: episode: 79, duration: 1.062s, episode steps: 171, steps per second: 161, episode reward: 171.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.292 [-2.174, 0.883], loss: 1.627858, mean_absolute_error: 23.427000, mean_q: 47.651966
  5909/50000: episode: 80, duration: 0.946s, episode steps: 152, steps per second: 161, episode reward: 152.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.307 [-2.144, 0.699], loss: 1.927983, mean_absolute_error: 24.042112, mean_q: 48.805592
  6092/50000: episode: 81, duration: 1.131s, episode steps: 183, steps per second: 162, episode reward: 183.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.277 [-2.382, 0.923], loss: 2.003144, mean_absolute_error: 24.682772, mean_q: 50.121525
  6266/50000: episode: 82, duration: 1.075s, episode steps: 174, steps per second: 162, episode reward: 174.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.304 [-2.364, 0.997], loss: 2.027018, mean_absolute_error: 24.997238, mean_q: 50.735172
  6443/50000: episode: 83, duration: 1.118s, episode steps: 177, steps per second: 158, episode reward: 177.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.292 [-2.035, 0.897], loss: 2.397092, mean_absolute_error: 25.481846, mean_q: 51.701767
  6623/50000: episode: 84, duration: 1.120s, episode steps: 180, steps per second: 161, episode reward: 180.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.289 [-2.541, 0.873], loss: 1.726510, mean_absolute_error: 26.055420, mean_q: 52.955631
  6891/50000: episode: 85, duration: 1.648s, episode steps: 268, steps per second: 163, episode reward: 268.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.190 [-2.197, 0.957], loss: 1.677988, mean_absolute_error: 26.722757, mean_q: 54.278976
  7080/50000: episode: 86, duration: 1.168s, episode steps: 189, steps per second: 162, episode reward: 189.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.322 [-2.376, 0.824], loss: 1.744531, mean_absolute_error: 27.568590, mean_q: 56.006573
  7259/50000: episode: 87, duration: 1.104s, episode steps: 179, steps per second: 162, episode reward: 179.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.453 [0.000, 1.000], mean observation: -0.338 [-3.097, 1.150], loss: 1.989020, mean_absolute_error: 27.918995, mean_q: 56.639721
  7426/50000: episode: 88, duration: 1.045s, episode steps: 167, steps per second: 160, episode reward: 167.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.349 [-2.994, 1.097], loss: 2.377820, mean_absolute_error: 28.328569, mean_q: 57.343727
  7632/50000: episode: 89, duration: 1.279s, episode steps: 206, steps per second: 161, episode reward: 206.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.255 [-2.627, 1.272], loss: 2.063432, mean_absolute_error: 28.945963, mean_q: 58.720997
  7776/50000: episode: 90, duration: 0.894s, episode steps: 144, steps per second: 161, episode reward: 144.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.268 [-2.005, 0.962], loss: 2.174310, mean_absolute_error: 29.349663, mean_q: 59.513603
  7950/50000: episode: 91, duration: 1.103s, episode steps: 174, steps per second: 158, episode reward: 174.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.443 [0.000, 1.000], mean observation: -0.335 [-3.683, 2.062], loss: 2.551449, mean_absolute_error: 29.897852, mean_q: 60.494030
  8160/50000: episode: 92, duration: 1.296s, episode steps: 210, steps per second: 162, episode reward: 210.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.298 [-2.983, 1.291], loss: 2.263921, mean_absolute_error: 30.214550, mean_q: 61.201908
  8317/50000: episode: 93, duration: 0.982s, episode steps: 157, steps per second: 160, episode reward: 157.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.323 [-2.233, 1.059], loss: 2.099911, mean_absolute_error: 30.570278, mean_q: 61.997318
  8491/50000: episode: 94, duration: 1.078s, episode steps: 174, steps per second: 161, episode reward: 174.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.443 [0.000, 1.000], mean observation: -0.332 [-3.767, 2.381], loss: 2.580698, mean_absolute_error: 31.013582, mean_q: 62.963463
  8760/50000: episode: 95, duration: 1.671s, episode steps: 269, steps per second: 161, episode reward: 269.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.197 [-2.010, 0.934], loss: 3.365029, mean_absolute_error: 31.587200, mean_q: 64.088013
  8989/50000: episode: 96, duration: 1.432s, episode steps: 229, steps per second: 160, episode reward: 229.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.277 [-2.738, 1.252], loss: 2.531268, mean_absolute_error: 32.363853, mean_q: 65.658783
  9152/50000: episode: 97, duration: 1.010s, episode steps: 163, steps per second: 161, episode reward: 163.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.442 [0.000, 1.000], mean observation: -0.354 [-3.569, 2.198], loss: 3.215895, mean_absolute_error: 32.699547, mean_q: 66.243988
  9376/50000: episode: 98, duration: 1.406s, episode steps: 224, steps per second: 159, episode reward: 224.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.268 [-3.706, 2.182], loss: 2.673188, mean_absolute_error: 33.406178, mean_q: 67.655579
  9616/50000: episode: 99, duration: 1.483s, episode steps: 240, steps per second: 162, episode reward: 240.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.270 [-2.427, 0.824], loss: 3.707894, mean_absolute_error: 33.718098, mean_q: 68.339729
  9771/50000: episode: 100, duration: 0.989s, episode steps: 155, steps per second: 157, episode reward: 155.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.246 [-1.521, 0.830], loss: 2.967485, mean_absolute_error: 34.170456, mean_q: 69.382172
  9942/50000: episode: 101, duration: 1.072s, episode steps: 171, steps per second: 159, episode reward: 171.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.359 [-3.140, 1.225], loss: 4.259010, mean_absolute_error: 34.722195, mean_q: 70.224426
 10135/50000: episode: 102, duration: 1.203s, episode steps: 193, steps per second: 160, episode reward: 193.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.451 [0.000, 1.000], mean observation: -0.310 [-3.561, 2.076], loss: 3.226599, mean_absolute_error: 34.913147, mean_q: 70.680672
 10342/50000: episode: 103, duration: 1.297s, episode steps: 207, steps per second: 160, episode reward: 207.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.302 [-3.143, 1.309], loss: 2.782929, mean_absolute_error: 35.428909, mean_q: 71.811310
 10794/50000: episode: 104, duration: 2.798s, episode steps: 452, steps per second: 162, episode reward: 452.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.502 [0.000, 1.000], mean observation: 0.161 [-0.855, 1.974], loss: 3.203374, mean_absolute_error: 36.197533, mean_q: 73.241585
 11014/50000: episode: 105, duration: 1.386s, episode steps: 220, steps per second: 159, episode reward: 220.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.293 [-2.775, 0.881], loss: 3.822625, mean_absolute_error: 37.098331, mean_q: 75.023262
 11200/50000: episode: 106, duration: 1.159s, episode steps: 186, steps per second: 160, episode reward: 186.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.343 [-2.390, 0.924], loss: 4.236228, mean_absolute_error: 37.428974, mean_q: 75.686623
 11498/50000: episode: 107, duration: 1.886s, episode steps: 298, steps per second: 158, episode reward: 298.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.216 [-2.412, 0.822], loss: 3.213909, mean_absolute_error: 37.960251, mean_q: 76.851250
 11793/50000: episode: 108, duration: 1.825s, episode steps: 295, steps per second: 162, episode reward: 295.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.340 [-0.784, 2.364], loss: 4.313341, mean_absolute_error: 38.760372, mean_q: 78.396080
 12113/50000: episode: 109, duration: 1.986s, episode steps: 320, steps per second: 161, episode reward: 320.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.503 [0.000, 1.000], mean observation: 0.318 [-0.756, 2.407], loss: 4.328205, mean_absolute_error: 39.918327, mean_q: 80.651352
 12323/50000: episode: 110, duration: 1.313s, episode steps: 210, steps per second: 160, episode reward: 210.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.312 [-2.420, 0.721], loss: 5.033235, mean_absolute_error: 40.775799, mean_q: 82.596901
 12752/50000: episode: 111, duration: 2.650s, episode steps: 429, steps per second: 162, episode reward: 429.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.503 [0.000, 1.000], mean observation: 0.249 [-0.986, 2.409], loss: 4.987158, mean_absolute_error: 41.481129, mean_q: 83.862305
 13010/50000: episode: 112, duration: 1.639s, episode steps: 258, steps per second: 157, episode reward: 258.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.256 [-2.409, 0.865], loss: 4.505596, mean_absolute_error: 42.670341, mean_q: 86.261353
 13328/50000: episode: 113, duration: 1.989s, episode steps: 318, steps per second: 160, episode reward: 318.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.503 [0.000, 1.000], mean observation: 0.370 [-0.871, 2.407], loss: 7.236184, mean_absolute_error: 43.157227, mean_q: 87.113602
 13731/50000: episode: 114, duration: 2.514s, episode steps: 403, steps per second: 160, episode reward: 403.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.504 [0.000, 1.000], mean observation: 0.279 [-1.115, 2.401], loss: 7.445703, mean_absolute_error: 44.356007, mean_q: 89.487396
 14059/50000: episode: 115, duration: 2.364s, episode steps: 328, steps per second: 139, episode reward: 328.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: 0.344 [-0.761, 2.412], loss: 6.375701, mean_absolute_error: 45.284046, mean_q: 91.548927
 14402/50000: episode: 116, duration: 2.469s, episode steps: 343, steps per second: 139, episode reward: 343.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.504 [0.000, 1.000], mean observation: 0.356 [-0.860, 2.404], loss: 4.630236, mean_absolute_error: 46.188232, mean_q: 93.404755
 14799/50000: episode: 117, duration: 2.904s, episode steps: 397, steps per second: 137, episode reward: 397.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.504 [0.000, 1.000], mean observation: 0.321 [-0.817, 2.402], loss: 8.179825, mean_absolute_error: 47.240070, mean_q: 95.323227
 15291/50000: episode: 118, duration: 3.342s, episode steps: 492, steps per second: 147, episode reward: 492.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.502 [0.000, 1.000], mean observation: 0.312 [-0.689, 2.401], loss: 7.857806, mean_absolute_error: 48.487820, mean_q: 97.877922
 15759/50000: episode: 119, duration: 2.933s, episode steps: 468, steps per second: 160, episode reward: 468.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.502 [0.000, 1.000], mean observation: 0.320 [-0.863, 2.401], loss: 5.364316, mean_absolute_error: 49.516373, mean_q: 100.135735
 16191/50000: episode: 120, duration: 2.713s, episode steps: 432, steps per second: 159, episode reward: 432.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.502 [0.000, 1.000], mean observation: 0.343 [-0.918, 2.402], loss: 5.347921, mean_absolute_error: 50.844429, mean_q: 102.674225
 16609/50000: episode: 121, duration: 2.613s, episode steps: 418, steps per second: 160, episode reward: 418.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.361 [-0.658, 2.408], loss: 6.581341, mean_absolute_error: 51.713779, mean_q: 104.452927
 16963/50000: episode: 122, duration: 2.210s, episode steps: 354, steps per second: 160, episode reward: 354.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.503 [0.000, 1.000], mean observation: 0.386 [-0.697, 2.400], loss: 5.236181, mean_absolute_error: 52.439678, mean_q: 105.930511
 17397/50000: episode: 123, duration: 2.729s, episode steps: 434, steps per second: 159, episode reward: 434.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.350 [-0.775, 2.402], loss: 6.352950, mean_absolute_error: 53.019524, mean_q: 107.166168
 17802/50000: episode: 124, duration: 2.559s, episode steps: 405, steps per second: 158, episode reward: 405.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.501 [0.000, 1.000], mean observation: 0.391 [-0.789, 2.401], loss: 4.551521, mean_absolute_error: 53.598457, mean_q: 108.307762
 18238/50000: episode: 125, duration: 2.735s, episode steps: 436, steps per second: 159, episode reward: 436.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.388 [-0.651, 2.401], loss: 6.172012, mean_absolute_error: 54.385689, mean_q: 109.766907
 18630/50000: episode: 126, duration: 2.466s, episode steps: 392, steps per second: 159, episode reward: 392.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.375 [-0.655, 2.400], loss: 6.755196, mean_absolute_error: 55.021725, mean_q: 111.180328
 19034/50000: episode: 127, duration: 2.524s, episode steps: 404, steps per second: 160, episode reward: 404.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.363 [-0.709, 2.402], loss: 5.070217, mean_absolute_error: 55.589737, mean_q: 112.352402
 19450/50000: episode: 128, duration: 2.640s, episode steps: 416, steps per second: 158, episode reward: 416.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.413 [-0.682, 2.405], loss: 5.983955, mean_absolute_error: 56.272701, mean_q: 113.623177
 19814/50000: episode: 129, duration: 2.276s, episode steps: 364, steps per second: 160, episode reward: 364.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.503 [0.000, 1.000], mean observation: 0.378 [-0.801, 2.403], loss: 4.174394, mean_absolute_error: 56.506268, mean_q: 114.232018
 20192/50000: episode: 130, duration: 2.380s, episode steps: 378, steps per second: 159, episode reward: 378.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.401 [-0.757, 2.402], loss: 6.078301, mean_absolute_error: 57.095238, mean_q: 115.296173
 20659/50000: episode: 131, duration: 2.965s, episode steps: 467, steps per second: 157, episode reward: 467.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.501 [0.000, 1.000], mean observation: 0.408 [-0.791, 2.402], loss: 6.669725, mean_absolute_error: 57.707584, mean_q: 116.485741
 21159/50000: episode: 132, duration: 3.168s, episode steps: 500, steps per second: 158, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.498 [0.000, 1.000], mean observation: 0.424 [-0.693, 2.362], loss: 4.729326, mean_absolute_error: 57.635071, mean_q: 116.442787
 21659/50000: episode: 133, duration: 3.143s, episode steps: 500, steps per second: 159, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.403 [-0.821, 2.311], loss: 5.036013, mean_absolute_error: 57.700920, mean_q: 116.663795
 22075/50000: episode: 134, duration: 2.613s, episode steps: 416, steps per second: 159, episode reward: 416.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.352 [-0.974, 2.401], loss: 7.369270, mean_absolute_error: 57.960995, mean_q: 116.887787
 22575/50000: episode: 135, duration: 3.134s, episode steps: 500, steps per second: 160, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.498 [0.000, 1.000], mean observation: 0.376 [-0.828, 2.359], loss: 5.936100, mean_absolute_error: 58.215260, mean_q: 117.580521
 23075/50000: episode: 136, duration: 3.182s, episode steps: 500, steps per second: 157, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.391 [-0.822, 2.299], loss: 5.809545, mean_absolute_error: 57.976994, mean_q: 117.284454
 23575/50000: episode: 137, duration: 3.147s, episode steps: 500, steps per second: 159, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.358 [-0.904, 2.382], loss: 4.790256, mean_absolute_error: 58.269463, mean_q: 117.938515
 24075/50000: episode: 138, duration: 3.135s, episode steps: 500, steps per second: 159, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.387 [-0.956, 2.360], loss: 4.667628, mean_absolute_error: 58.719551, mean_q: 118.687004
 24474/50000: episode: 139, duration: 2.554s, episode steps: 399, steps per second: 156, episode reward: 399.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.501 [0.000, 1.000], mean observation: 0.340 [-1.043, 2.402], loss: 4.916703, mean_absolute_error: 58.614105, mean_q: 118.458801
 24848/50000: episode: 140, duration: 2.349s, episode steps: 374, steps per second: 159, episode reward: 374.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.503 [0.000, 1.000], mean observation: 0.313 [-0.934, 2.404], loss: 5.747376, mean_absolute_error: 58.734531, mean_q: 118.635338
 25275/50000: episode: 141, duration: 2.698s, episode steps: 427, steps per second: 158, episode reward: 427.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.501 [0.000, 1.000], mean observation: 0.345 [-1.159, 2.403], loss: 6.808571, mean_absolute_error: 58.877945, mean_q: 118.775085
 25775/50000: episode: 142, duration: 3.164s, episode steps: 500, steps per second: 158, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.502 [0.000, 1.000], mean observation: 0.292 [-0.936, 2.367], loss: 4.243455, mean_absolute_error: 59.070789, mean_q: 119.302307
 26091/50000: episode: 143, duration: 2.035s, episode steps: 316, steps per second: 155, episode reward: 316.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.503 [0.000, 1.000], mean observation: 0.378 [-1.142, 2.404], loss: 4.474892, mean_absolute_error: 58.989685, mean_q: 118.927231
 26484/50000: episode: 144, duration: 2.707s, episode steps: 393, steps per second: 145, episode reward: 393.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.504 [0.000, 1.000], mean observation: 0.324 [-0.811, 2.407], loss: 6.173388, mean_absolute_error: 59.190575, mean_q: 119.201515
 26812/50000: episode: 145, duration: 2.438s, episode steps: 328, steps per second: 135, episode reward: 328.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.503 [0.000, 1.000], mean observation: 0.376 [-1.048, 2.402], loss: 5.465988, mean_absolute_error: 59.256191, mean_q: 119.247543
 27250/50000: episode: 146, duration: 3.217s, episode steps: 438, steps per second: 136, episode reward: 438.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.502 [0.000, 1.000], mean observation: 0.279 [-1.050, 2.400], loss: 5.538434, mean_absolute_error: 58.917114, mean_q: 118.733772
 27671/50000: episode: 147, duration: 3.034s, episode steps: 421, steps per second: 139, episode reward: 421.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.501 [0.000, 1.000], mean observation: 0.305 [-0.822, 2.403], loss: 6.000508, mean_absolute_error: 58.645588, mean_q: 118.159012
 28001/50000: episode: 148, duration: 2.106s, episode steps: 330, steps per second: 157, episode reward: 330.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.503 [0.000, 1.000], mean observation: 0.368 [-1.123, 2.409], loss: 5.634782, mean_absolute_error: 58.481716, mean_q: 117.832092
 28364/50000: episode: 149, duration: 2.302s, episode steps: 363, steps per second: 158, episode reward: 363.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.504 [0.000, 1.000], mean observation: 0.326 [-0.952, 2.412], loss: 5.063807, mean_absolute_error: 58.597279, mean_q: 118.086388
 28721/50000: episode: 150, duration: 2.289s, episode steps: 357, steps per second: 156, episode reward: 357.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.504 [0.000, 1.000], mean observation: 0.324 [-1.223, 2.414], loss: 4.889977, mean_absolute_error: 58.595726, mean_q: 117.918114
 29073/50000: episode: 151, duration: 2.231s, episode steps: 352, steps per second: 158, episode reward: 352.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.503 [0.000, 1.000], mean observation: 0.325 [-0.984, 2.407], loss: 8.003345, mean_absolute_error: 58.194546, mean_q: 117.196625
 29489/50000: episode: 152, duration: 2.669s, episode steps: 416, steps per second: 156, episode reward: 416.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.502 [0.000, 1.000], mean observation: 0.297 [-1.019, 2.410], loss: 8.385610, mean_absolute_error: 57.755154, mean_q: 116.074829
 29856/50000: episode: 153, duration: 2.350s, episode steps: 367, steps per second: 156, episode reward: 367.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.501 [0.000, 1.000], mean observation: 0.325 [-1.241, 2.407], loss: 5.622941, mean_absolute_error: 57.645180, mean_q: 116.022324
 30287/50000: episode: 154, duration: 2.747s, episode steps: 431, steps per second: 157, episode reward: 431.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.503 [0.000, 1.000], mean observation: 0.291 [-1.021, 2.404], loss: 6.725824, mean_absolute_error: 57.383301, mean_q: 115.450333
 30658/50000: episode: 155, duration: 2.355s, episode steps: 371, steps per second: 158, episode reward: 371.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.501 [0.000, 1.000], mean observation: 0.338 [-1.246, 2.400], loss: 4.053256, mean_absolute_error: 56.873894, mean_q: 114.547318
 31087/50000: episode: 156, duration: 2.760s, episode steps: 429, steps per second: 155, episode reward: 429.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.501 [0.000, 1.000], mean observation: 0.312 [-0.958, 2.405], loss: 3.341397, mean_absolute_error: 57.095154, mean_q: 114.918106
 31433/50000: episode: 157, duration: 2.245s, episode steps: 346, steps per second: 154, episode reward: 346.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.503 [0.000, 1.000], mean observation: 0.332 [-1.290, 2.406], loss: 4.614096, mean_absolute_error: 56.782242, mean_q: 114.377281
 31891/50000: episode: 158, duration: 2.951s, episode steps: 458, steps per second: 155, episode reward: 458.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.502 [0.000, 1.000], mean observation: 0.291 [-1.214, 2.410], loss: 7.074708, mean_absolute_error: 56.650791, mean_q: 113.871704
 32247/50000: episode: 159, duration: 2.288s, episode steps: 356, steps per second: 156, episode reward: 356.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.503 [0.000, 1.000], mean observation: 0.339 [-1.235, 2.402], loss: 7.149241, mean_absolute_error: 56.349609, mean_q: 113.276276
 32674/50000: episode: 160, duration: 2.757s, episode steps: 427, steps per second: 155, episode reward: 427.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.504 [0.000, 1.000], mean observation: 0.312 [-0.978, 2.407], loss: 5.580923, mean_absolute_error: 56.293774, mean_q: 113.200951
 33006/50000: episode: 161, duration: 2.118s, episode steps: 332, steps per second: 157, episode reward: 332.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.503 [0.000, 1.000], mean observation: 0.353 [-1.229, 2.410], loss: 4.579061, mean_absolute_error: 56.028416, mean_q: 112.790619
 33319/50000: episode: 162, duration: 2.001s, episode steps: 313, steps per second: 156, episode reward: 313.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.355 [-1.141, 2.406], loss: 8.333429, mean_absolute_error: 55.917332, mean_q: 112.330887
 33701/50000: episode: 163, duration: 2.435s, episode steps: 382, steps per second: 157, episode reward: 382.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.327 [-0.962, 2.400], loss: 4.079334, mean_absolute_error: 55.588089, mean_q: 112.037430
 34029/50000: episode: 164, duration: 2.141s, episode steps: 328, steps per second: 153, episode reward: 328.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.503 [0.000, 1.000], mean observation: 0.341 [-1.382, 2.408], loss: 5.484010, mean_absolute_error: 55.531219, mean_q: 111.644554
 34460/50000: episode: 165, duration: 3.146s, episode steps: 431, steps per second: 137, episode reward: 431.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.503 [0.000, 1.000], mean observation: 0.298 [-1.392, 2.406], loss: 7.016966, mean_absolute_error: 55.541031, mean_q: 111.580917
 34801/50000: episode: 166, duration: 2.577s, episode steps: 341, steps per second: 132, episode reward: 341.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.504 [0.000, 1.000], mean observation: 0.330 [-1.578, 2.413], loss: 7.278677, mean_absolute_error: 55.330990, mean_q: 110.998260
 35199/50000: episode: 167, duration: 2.602s, episode steps: 398, steps per second: 153, episode reward: 398.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.503 [0.000, 1.000], mean observation: 0.302 [-1.137, 2.403], loss: 7.087126, mean_absolute_error: 55.133873, mean_q: 110.779327
 35670/50000: episode: 168, duration: 3.064s, episode steps: 471, steps per second: 154, episode reward: 471.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.503 [0.000, 1.000], mean observation: 0.303 [-1.504, 2.407], loss: 6.771763, mean_absolute_error: 54.837563, mean_q: 110.228409
 36166/50000: episode: 169, duration: 3.194s, episode steps: 496, steps per second: 155, episode reward: 496.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.502 [0.000, 1.000], mean observation: 0.238 [-1.318, 2.402], loss: 7.970106, mean_absolute_error: 54.635006, mean_q: 109.722847
 36657/50000: episode: 170, duration: 3.173s, episode steps: 491, steps per second: 155, episode reward: 491.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.503 [0.000, 1.000], mean observation: 0.250 [-1.706, 2.406], loss: 5.704204, mean_absolute_error: 54.260590, mean_q: 109.047600
 37157/50000: episode: 171, duration: 3.260s, episode steps: 500, steps per second: 153, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.504 [0.000, 1.000], mean observation: 0.220 [-1.271, 2.256], loss: 7.200768, mean_absolute_error: 54.098209, mean_q: 108.654381
 37657/50000: episode: 172, duration: 3.195s, episode steps: 500, steps per second: 157, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.502 [0.000, 1.000], mean observation: 0.041 [-1.220, 1.212], loss: 6.464919, mean_absolute_error: 54.360081, mean_q: 109.216179
 38157/50000: episode: 173, duration: 3.210s, episode steps: 500, steps per second: 156, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.496 [0.000, 1.000], mean observation: 0.097 [-1.409, 1.360], loss: 6.186774, mean_absolute_error: 53.783821, mean_q: 108.131325
 38586/50000: episode: 174, duration: 2.934s, episode steps: 429, steps per second: 146, episode reward: 429.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.503 [0.000, 1.000], mean observation: 0.271 [-1.061, 2.407], loss: 4.830243, mean_absolute_error: 54.135418, mean_q: 108.937035
 39086/50000: episode: 175, duration: 3.759s, episode steps: 500, steps per second: 133, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.504 [0.000, 1.000], mean observation: 0.118 [-1.387, 1.362], loss: 7.292828, mean_absolute_error: 54.103619, mean_q: 108.625534
 39563/50000: episode: 176, duration: 3.546s, episode steps: 477, steps per second: 135, episode reward: 477.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.501 [0.000, 1.000], mean observation: 0.286 [-1.548, 2.405], loss: 6.350655, mean_absolute_error: 53.979561, mean_q: 108.621803
 40063/50000: episode: 177, duration: 3.436s, episode steps: 500, steps per second: 146, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.502 [0.000, 1.000], mean observation: 0.268 [-1.399, 2.237], loss: 6.235330, mean_absolute_error: 54.053059, mean_q: 108.718117
 40562/50000: episode: 178, duration: 3.254s, episode steps: 499, steps per second: 153, episode reward: 499.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.503 [0.000, 1.000], mean observation: 0.270 [-1.397, 2.414], loss: 5.833961, mean_absolute_error: 54.018051, mean_q: 108.660820
 41062/50000: episode: 179, duration: 3.228s, episode steps: 500, steps per second: 155, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.504 [0.000, 1.000], mean observation: 0.076 [-1.292, 1.461], loss: 6.821904, mean_absolute_error: 54.205986, mean_q: 109.010910
 41562/50000: episode: 180, duration: 3.224s, episode steps: 500, steps per second: 155, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.498 [0.000, 1.000], mean observation: -0.022 [-1.176, 0.976], loss: 5.201680, mean_absolute_error: 54.211395, mean_q: 109.014732
 42062/50000: episode: 181, duration: 3.300s, episode steps: 500, steps per second: 152, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.132 [-1.236, 1.502], loss: 6.375962, mean_absolute_error: 54.281094, mean_q: 109.005981
 42562/50000: episode: 182, duration: 3.970s, episode steps: 500, steps per second: 126, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.015 [-1.237, 0.930], loss: 6.382715, mean_absolute_error: 54.226540, mean_q: 108.944748
 43062/50000: episode: 183, duration: 3.962s, episode steps: 500, steps per second: 126, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.502 [0.000, 1.000], mean observation: 0.031 [-1.233, 1.178], loss: 5.900445, mean_absolute_error: 54.141014, mean_q: 108.887863
 43562/50000: episode: 184, duration: 3.617s, episode steps: 500, steps per second: 138, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.498 [0.000, 1.000], mean observation: -0.003 [-1.152, 0.911], loss: 5.648526, mean_absolute_error: 54.440849, mean_q: 109.243797
 44002/50000: episode: 185, duration: 2.857s, episode steps: 440, steps per second: 154, episode reward: 440.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.182 [-2.410, 0.987], loss: 4.240374, mean_absolute_error: 53.966129, mean_q: 108.517586
 44329/50000: episode: 186, duration: 2.159s, episode steps: 327, steps per second: 151, episode reward: 327.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.207 [-2.424, 0.788], loss: 2.932429, mean_absolute_error: 54.022423, mean_q: 108.469894
 44829/50000: episode: 187, duration: 3.268s, episode steps: 500, steps per second: 153, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.498 [0.000, 1.000], mean observation: 0.103 [-1.397, 1.507], loss: 5.913231, mean_absolute_error: 53.785702, mean_q: 108.030357
 45329/50000: episode: 188, duration: 3.357s, episode steps: 500, steps per second: 149, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.502 [0.000, 1.000], mean observation: 0.182 [-1.576, 1.619], loss: 5.370584, mean_absolute_error: 53.921318, mean_q: 108.256805
 45782/50000: episode: 189, duration: 2.998s, episode steps: 453, steps per second: 151, episode reward: 453.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: 0.262 [-1.474, 2.401], loss: 4.394953, mean_absolute_error: 53.683739, mean_q: 107.851341
 46216/50000: episode: 190, duration: 2.901s, episode steps: 434, steps per second: 150, episode reward: 434.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.282 [-1.281, 2.410], loss: 4.740616, mean_absolute_error: 53.448002, mean_q: 107.293228
 46716/50000: episode: 191, duration: 3.267s, episode steps: 500, steps per second: 153, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.019 [-1.212, 1.065], loss: 5.950387, mean_absolute_error: 53.467194, mean_q: 107.309227
 47216/50000: episode: 192, duration: 3.276s, episode steps: 500, steps per second: 153, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.502 [0.000, 1.000], mean observation: 0.184 [-1.483, 2.013], loss: 3.836204, mean_absolute_error: 53.114296, mean_q: 106.746941
 47524/50000: episode: 193, duration: 2.010s, episode steps: 308, steps per second: 153, episode reward: 308.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.217 [-2.430, 1.068], loss: 3.644928, mean_absolute_error: 52.884850, mean_q: 106.207184
 48024/50000: episode: 194, duration: 3.304s, episode steps: 500, steps per second: 151, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.502 [0.000, 1.000], mean observation: 0.231 [-1.342, 2.188], loss: 4.523789, mean_absolute_error: 52.529846, mean_q: 105.522797
 48369/50000: episode: 195, duration: 2.282s, episode steps: 345, steps per second: 151, episode reward: 345.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.164 [-2.725, 0.979], loss: 5.533070, mean_absolute_error: 52.417603, mean_q: 105.200432
 48619/50000: episode: 196, duration: 1.627s, episode steps: 250, steps per second: 154, episode reward: 250.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.273 [-2.431, 1.368], loss: 3.804307, mean_absolute_error: 52.173004, mean_q: 104.850433
 49017/50000: episode: 197, duration: 2.597s, episode steps: 398, steps per second: 153, episode reward: 398.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.174 [-2.444, 0.990], loss: 4.982242, mean_absolute_error: 52.211079, mean_q: 104.764091
 49517/50000: episode: 198, duration: 3.312s, episode steps: 500, steps per second: 151, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.502 [0.000, 1.000], mean observation: 0.232 [-1.555, 2.110], loss: 6.029026, mean_absolute_error: 52.104034, mean_q: 104.495407