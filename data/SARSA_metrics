    22/50000: episode: 1, duration: 1.769s, episode steps: 22, steps per second: 12, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.105 [-0.935, 0.391], loss: 0.501320, mean_absolute_error: 0.514484, mean_q: 0.022006
    53/50000: episode: 2, duration: 0.156s, episode steps: 31, steps per second: 199, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.419 [0.000, 1.000], mean observation: 0.128 [-0.955, 2.162], loss: 0.497493, mean_absolute_error: 0.569760, mean_q: 0.159854
    74/50000: episode: 3, duration: 0.111s, episode steps: 21, steps per second: 189, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.087 [-0.551, 1.254], loss: 0.506150, mean_absolute_error: 0.702798, mean_q: 0.469041
   101/50000: episode: 4, duration: 0.138s, episode steps: 27, steps per second: 196, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.062 [-0.630, 1.390], loss: 0.517014, mean_absolute_error: 0.848516, mean_q: 0.728974
   111/50000: episode: 5, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.107 [-1.564, 1.016], loss: 0.438841, mean_absolute_error: 0.718016, mean_q: 0.564555
   132/50000: episode: 6, duration: 0.119s, episode steps: 21, steps per second: 176, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.066 [-1.337, 0.613], loss: 0.471766, mean_absolute_error: 0.828614, mean_q: 0.743832
   144/50000: episode: 7, duration: 0.061s, episode steps: 12, steps per second: 198, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.120 [-1.521, 2.424], loss: 1.639620, mean_absolute_error: 2.152740, mean_q: 2.791233
   156/50000: episode: 8, duration: 0.062s, episode steps: 12, steps per second: 192, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.118 [-1.570, 2.493], loss: 1.714486, mean_absolute_error: 2.242355, mean_q: 3.248948
   165/50000: episode: 9, duration: 0.047s, episode steps: 9, steps per second: 190, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.154 [-1.520, 2.476], loss: 3.250407, mean_absolute_error: 2.611840, mean_q: 4.115637
   179/50000: episode: 10, duration: 0.076s, episode steps: 14, steps per second: 184, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.100 [-1.201, 2.150], loss: 1.633779, mean_absolute_error: 2.193785, mean_q: 3.580006
   194/50000: episode: 11, duration: 0.078s, episode steps: 15, steps per second: 192, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.098 [-1.789, 2.835], loss: 2.769086, mean_absolute_error: 2.479344, mean_q: 4.139688
   205/50000: episode: 12, duration: 0.056s, episode steps: 11, steps per second: 195, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.120 [-1.719, 2.770], loss: 3.754115, mean_absolute_error: 2.798854, mean_q: 4.729946
   215/50000: episode: 13, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.150 [-1.912, 3.013], loss: 5.054962, mean_absolute_error: 3.245232, mean_q: 5.653773
   225/50000: episode: 14, duration: 0.059s, episode steps: 10, steps per second: 168, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.151 [-1.939, 3.082], loss: 5.485567, mean_absolute_error: 3.401763, mean_q: 6.055240
   236/50000: episode: 15, duration: 0.057s, episode steps: 11, steps per second: 193, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.109 [-1.768, 2.828], loss: 4.552793, mean_absolute_error: 3.064757, mean_q: 5.272343
   250/50000: episode: 16, duration: 0.071s, episode steps: 14, steps per second: 197, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.104 [-1.174, 2.082], loss: 1.969103, mean_absolute_error: 2.695046, mean_q: 4.684824
   261/50000: episode: 17, duration: 0.057s, episode steps: 11, steps per second: 194, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.108 [-1.743, 2.742], loss: 4.813157, mean_absolute_error: 3.347498, mean_q: 6.116877
   282/50000: episode: 18, duration: 0.111s, episode steps: 21, steps per second: 189, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.051 [-1.768, 2.704], loss: 2.861816, mean_absolute_error: 2.728772, mean_q: 4.982503
   292/50000: episode: 19, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.159 [-1.543, 2.539], loss: 5.147127, mean_absolute_error: 3.784653, mean_q: 6.790478
   306/50000: episode: 20, duration: 0.070s, episode steps: 14, steps per second: 199, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.214 [0.000, 1.000], mean observation: 0.114 [-1.565, 2.609], loss: 3.886618, mean_absolute_error: 3.212726, mean_q: 5.692239
   314/50000: episode: 21, duration: 0.053s, episode steps: 8, steps per second: 152, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.169 [-1.546, 2.557], loss: 6.003985, mean_absolute_error: 3.615062, mean_q: 6.276509
   323/50000: episode: 22, duration: 0.051s, episode steps: 9, steps per second: 175, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.145 [-1.773, 2.813], loss: 6.470397, mean_absolute_error: 3.819802, mean_q: 6.827749
   331/50000: episode: 23, duration: 0.043s, episode steps: 8, steps per second: 188, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.157 [-1.531, 2.553], loss: 6.035460, mean_absolute_error: 3.660173, mean_q: 6.402239
   340/50000: episode: 24, duration: 0.045s, episode steps: 9, steps per second: 198, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.158 [-1.746, 2.798], loss: 6.323212, mean_absolute_error: 3.765103, mean_q: 6.704114
   351/50000: episode: 25, duration: 0.056s, episode steps: 11, steps per second: 198, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.102 [-1.775, 2.759], loss: 5.420148, mean_absolute_error: 3.783063, mean_q: 6.947989
   359/50000: episode: 26, duration: 0.041s, episode steps: 8, steps per second: 195, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.143 [-1.551, 2.549], loss: 6.216682, mean_absolute_error: 3.758854, mean_q: 6.680033
   369/50000: episode: 27, duration: 0.054s, episode steps: 10, steps per second: 185, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.118 [-1.964, 3.004], loss: 6.826185, mean_absolute_error: 4.026547, mean_q: 7.465581
   380/50000: episode: 28, duration: 0.057s, episode steps: 11, steps per second: 192, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.119 [-1.811, 2.904], loss: 5.763514, mean_absolute_error: 3.793408, mean_q: 7.022328
   390/50000: episode: 29, duration: 0.052s, episode steps: 10, steps per second: 194, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.175 [-1.910, 3.113], loss: 6.880617, mean_absolute_error: 4.095374, mean_q: 7.619807
   403/50000: episode: 30, duration: 0.068s, episode steps: 13, steps per second: 190, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.154 [0.000, 1.000], mean observation: 0.110 [-1.717, 2.817], loss: 4.918793, mean_absolute_error: 3.720503, mean_q: 6.818241
   416/50000: episode: 31, duration: 0.070s, episode steps: 13, steps per second: 186, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.154 [0.000, 1.000], mean observation: 0.111 [-1.794, 2.839], loss: 5.082594, mean_absolute_error: 4.081427, mean_q: 7.609266
   430/50000: episode: 32, duration: 0.071s, episode steps: 14, steps per second: 197, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.214 [0.000, 1.000], mean observation: 0.084 [-1.613, 2.606], loss: 4.247045, mean_absolute_error: 3.803314, mean_q: 6.918387
   440/50000: episode: 33, duration: 0.051s, episode steps: 10, steps per second: 198, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.137 [-1.544, 2.516], loss: 5.347007, mean_absolute_error: 4.179523, mean_q: 7.621837
   452/50000: episode: 34, duration: 0.062s, episode steps: 12, steps per second: 193, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.119 [-1.322, 2.234], loss: 4.796986, mean_absolute_error: 4.179000, mean_q: 7.566499
   460/50000: episode: 35, duration: 0.045s, episode steps: 8, steps per second: 179, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.144 [-1.590, 2.565], loss: 6.853228, mean_absolute_error: 4.426909, mean_q: 7.891081
   470/50000: episode: 36, duration: 0.051s, episode steps: 10, steps per second: 196, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.124 [-1.588, 2.491], loss: 4.758997, mean_absolute_error: 4.647638, mean_q: 8.626129
   490/50000: episode: 37, duration: 0.100s, episode steps: 20, steps per second: 201, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.035 [-2.313, 3.303], loss: 4.558040, mean_absolute_error: 4.063258, mean_q: 7.697336
   508/50000: episode: 38, duration: 0.108s, episode steps: 18, steps per second: 167, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.082 [-1.156, 1.887], loss: 3.067427, mean_absolute_error: 4.127845, mean_q: 7.780141
   518/50000: episode: 39, duration: 0.051s, episode steps: 10, steps per second: 194, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.137 [-1.958, 3.050], loss: 7.762828, mean_absolute_error: 4.988780, mean_q: 9.459627
   531/50000: episode: 40, duration: 0.066s, episode steps: 13, steps per second: 198, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.154 [0.000, 1.000], mean observation: 0.108 [-1.776, 2.871], loss: 5.352213, mean_absolute_error: 4.496051, mean_q: 8.380520
   541/50000: episode: 41, duration: 0.052s, episode steps: 10, steps per second: 194, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.153 [-1.567, 2.627], loss: 5.968800, mean_absolute_error: 4.588213, mean_q: 8.375406
   550/50000: episode: 42, duration: 0.049s, episode steps: 9, steps per second: 184, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.118 [-1.420, 2.228], loss: 5.876084, mean_absolute_error: 4.848367, mean_q: 8.851908
   558/50000: episode: 43, duration: 0.043s, episode steps: 8, steps per second: 187, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.174 [-1.530, 2.589], loss: 7.023819, mean_absolute_error: 4.804167, mean_q: 8.707175
   569/50000: episode: 44, duration: 0.057s, episode steps: 11, steps per second: 194, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.128 [-1.720, 2.756], loss: 5.436021, mean_absolute_error: 4.676624, mean_q: 8.669613
   582/50000: episode: 45, duration: 0.067s, episode steps: 13, steps per second: 195, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.131 [-1.336, 2.361], loss: 3.901513, mean_absolute_error: 4.497845, mean_q: 8.323232
   591/50000: episode: 46, duration: 0.051s, episode steps: 9, steps per second: 178, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.187 [-1.727, 2.899], loss: 6.366715, mean_absolute_error: 4.846914, mean_q: 8.762592
   601/50000: episode: 47, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.151 [-1.528, 2.531], loss: 4.975075, mean_absolute_error: 4.651698, mean_q: 8.480175
   618/50000: episode: 48, duration: 0.087s, episode steps: 17, steps per second: 196, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.353 [0.000, 1.000], mean observation: 0.055 [-1.211, 1.946], loss: 3.130701, mean_absolute_error: 4.509286, mean_q: 8.281825
   633/50000: episode: 49, duration: 0.079s, episode steps: 15, steps per second: 190, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.101 [-1.771, 2.875], loss: 3.907100, mean_absolute_error: 4.706648, mean_q: 8.521780
   650/50000: episode: 50, duration: 0.087s, episode steps: 17, steps per second: 195, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.294 [0.000, 1.000], mean observation: 0.053 [-1.393, 2.204], loss: 3.140411, mean_absolute_error: 4.835611, mean_q: 8.794819
   661/50000: episode: 51, duration: 0.058s, episode steps: 11, steps per second: 189, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.124 [-1.532, 2.479], loss: 5.066042, mean_absolute_error: 5.204162, mean_q: 9.247837
   673/50000: episode: 52, duration: 0.066s, episode steps: 12, steps per second: 181, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.111 [-1.600, 2.508], loss: 4.047438, mean_absolute_error: 5.156096, mean_q: 9.203175
   693/50000: episode: 53, duration: 0.103s, episode steps: 20, steps per second: 193, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.058 [-1.216, 1.859], loss: 2.397840, mean_absolute_error: 4.901601, mean_q: 8.852916
   703/50000: episode: 54, duration: 0.059s, episode steps: 10, steps per second: 169, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.128 [-1.376, 2.185], loss: 4.812657, mean_absolute_error: 5.321640, mean_q: 9.349130
   714/50000: episode: 55, duration: 0.060s, episode steps: 11, steps per second: 183, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.117 [-1.355, 2.285], loss: 4.124665, mean_absolute_error: 5.176332, mean_q: 9.220166
   736/50000: episode: 56, duration: 0.113s, episode steps: 22, steps per second: 194, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.409 [0.000, 1.000], mean observation: 0.025 [-1.009, 1.569], loss: 2.488725, mean_absolute_error: 5.329115, mean_q: 9.694527
   750/50000: episode: 57, duration: 0.071s, episode steps: 14, steps per second: 198, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.097 [-1.000, 1.741], loss: 3.392993, mean_absolute_error: 5.411825, mean_q: 9.573600
   766/50000: episode: 58, duration: 0.084s, episode steps: 16, steps per second: 190, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.090 [-0.957, 1.730], loss: 3.040874, mean_absolute_error: 5.542445, mean_q: 9.821021
   797/50000: episode: 59, duration: 0.159s, episode steps: 31, steps per second: 195, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.645 [0.000, 1.000], mean observation: -0.012 [-2.503, 1.712], loss: 6.586161, mean_absolute_error: 6.741296, mean_q: 11.959174
   811/50000: episode: 60, duration: 0.076s, episode steps: 14, steps per second: 185, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.098 [-1.729, 0.945], loss: 10.450211, mean_absolute_error: 7.948302, mean_q: 13.930685
   831/50000: episode: 61, duration: 0.105s, episode steps: 20, steps per second: 191, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.098 [-1.669, 0.788], loss: 5.585475, mean_absolute_error: 7.346508, mean_q: 13.205322
   840/50000: episode: 62, duration: 0.047s, episode steps: 9, steps per second: 193, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.137 [-2.228, 1.377], loss: 16.265910, mean_absolute_error: 8.732241, mean_q: 14.452849
   870/50000: episode: 63, duration: 0.157s, episode steps: 30, steps per second: 191, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.004 [-1.507, 1.998], loss: 2.526128, mean_absolute_error: 6.625834, mean_q: 12.101927
   879/50000: episode: 64, duration: 0.046s, episode steps: 9, steps per second: 198, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.138 [-2.255, 1.415], loss: 19.444177, mean_absolute_error: 9.172511, mean_q: 14.784779
   892/50000: episode: 65, duration: 0.077s, episode steps: 13, steps per second: 168, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.128 [-2.046, 1.167], loss: 10.728410, mean_absolute_error: 7.870663, mean_q: 13.755870
   902/50000: episode: 66, duration: 0.052s, episode steps: 10, steps per second: 192, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.125 [-2.009, 1.197], loss: 11.535227, mean_absolute_error: 7.998349, mean_q: 13.981201
   918/50000: episode: 67, duration: 0.088s, episode steps: 16, steps per second: 182, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.110 [-2.692, 1.569], loss: 8.048493, mean_absolute_error: 7.340715, mean_q: 13.070874
   931/50000: episode: 68, duration: 0.066s, episode steps: 13, steps per second: 196, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.101 [-2.249, 1.395], loss: 8.898420, mean_absolute_error: 7.525298, mean_q: 13.232699
   944/50000: episode: 69, duration: 0.068s, episode steps: 13, steps per second: 192, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.114 [-2.343, 1.390], loss: 8.093827, mean_absolute_error: 7.210490, mean_q: 12.882505
   957/50000: episode: 70, duration: 0.064s, episode steps: 13, steps per second: 203, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.088 [-2.815, 1.777], loss: 8.525893, mean_absolute_error: 7.128374, mean_q: 12.750090
   968/50000: episode: 71, duration: 0.060s, episode steps: 11, steps per second: 182, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.115 [-2.409, 1.519], loss: 8.876464, mean_absolute_error: 7.071502, mean_q: 12.589895
   991/50000: episode: 72, duration: 0.118s, episode steps: 23, steps per second: 196, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.565 [0.000, 1.000], mean observation: -0.027 [-1.316, 1.001], loss: 3.106722, mean_absolute_error: 6.517372, mean_q: 12.030973
  1005/50000: episode: 73, duration: 0.071s, episode steps: 14, steps per second: 198, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.786 [0.000, 1.000], mean observation: -0.112 [-2.530, 1.524], loss: 6.284905, mean_absolute_error: 6.892419, mean_q: 12.440552
  1017/50000: episode: 74, duration: 0.064s, episode steps: 12, steps per second: 186, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.098 [-2.145, 1.405], loss: 6.676123, mean_absolute_error: 6.846725, mean_q: 12.283706
  1027/50000: episode: 75, duration: 0.053s, episode steps: 10, steps per second: 187, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.141 [-2.758, 1.781], loss: 8.682671, mean_absolute_error: 6.958608, mean_q: 12.194038
  1038/50000: episode: 76, duration: 0.056s, episode steps: 11, steps per second: 195, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.087 [-2.234, 1.416], loss: 5.995346, mean_absolute_error: 6.550921, mean_q: 11.579269
  1080/50000: episode: 77, duration: 0.220s, episode steps: 42, steps per second: 191, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.080 [-1.169, 1.152], loss: 2.050971, mean_absolute_error: 6.660167, mean_q: 12.286411
  1096/50000: episode: 78, duration: 0.087s, episode steps: 16, steps per second: 184, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.108 [-0.556, 1.179], loss: 5.143168, mean_absolute_error: 7.080706, mean_q: 12.628538
  1113/50000: episode: 79, duration: 0.085s, episode steps: 17, steps per second: 199, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.089 [-0.948, 1.664], loss: 6.402386, mean_absolute_error: 7.528000, mean_q: 13.316262
  1140/50000: episode: 80, duration: 0.138s, episode steps: 27, steps per second: 195, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.407 [0.000, 1.000], mean observation: 0.040 [-1.126, 1.717], loss: 3.912980, mean_absolute_error: 7.371483, mean_q: 13.603240
  1158/50000: episode: 81, duration: 0.093s, episode steps: 18, steps per second: 193, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.085 [-1.167, 2.023], loss: 6.088761, mean_absolute_error: 7.558949, mean_q: 13.925243
  1175/50000: episode: 82, duration: 0.085s, episode steps: 17, steps per second: 201, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.235 [0.000, 1.000], mean observation: 0.075 [-1.716, 2.733], loss: 7.673688, mean_absolute_error: 7.810623, mean_q: 14.527762
  1189/50000: episode: 83, duration: 0.074s, episode steps: 14, steps per second: 190, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.214 [0.000, 1.000], mean observation: 0.101 [-1.591, 2.572], loss: 8.036206, mean_absolute_error: 7.702017, mean_q: 14.238467
  1206/50000: episode: 84, duration: 0.086s, episode steps: 17, steps per second: 198, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.353 [0.000, 1.000], mean observation: 0.076 [-1.132, 1.887], loss: 5.667066, mean_absolute_error: 7.434825, mean_q: 13.765553
  1219/50000: episode: 85, duration: 0.068s, episode steps: 13, steps per second: 191, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.116 [-1.386, 2.318], loss: 6.792767, mean_absolute_error: 7.406761, mean_q: 13.393539
  1251/50000: episode: 86, duration: 0.166s, episode steps: 32, steps per second: 193, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.594 [0.000, 1.000], mean observation: -0.041 [-2.070, 1.192], loss: 3.728574, mean_absolute_error: 7.426241, mean_q: 13.730519
  1263/50000: episode: 87, duration: 0.061s, episode steps: 12, steps per second: 198, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.109 [-1.709, 2.649], loss: 8.349593, mean_absolute_error: 7.954684, mean_q: 13.939372
  1279/50000: episode: 88, duration: 0.092s, episode steps: 16, steps per second: 174, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.104 [-1.172, 0.563], loss: 6.062911, mean_absolute_error: 7.773385, mean_q: 13.913259
  1305/50000: episode: 89, duration: 0.136s, episode steps: 26, steps per second: 192, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.577 [0.000, 1.000], mean observation: -0.068 [-1.628, 0.775], loss: 3.832306, mean_absolute_error: 7.518321, mean_q: 13.925617
  1325/50000: episode: 90, duration: 0.124s, episode steps: 20, steps per second: 161, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.650 [0.000, 1.000], mean observation: -0.082 [-2.145, 1.225], loss: 5.405059, mean_absolute_error: 7.730449, mean_q: 14.317981
  1335/50000: episode: 91, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.126 [-2.092, 1.231], loss: 10.025330, mean_absolute_error: 8.002911, mean_q: 14.291685
  1348/50000: episode: 92, duration: 0.069s, episode steps: 13, steps per second: 187, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.097 [-2.306, 1.353], loss: 6.715427, mean_absolute_error: 7.616351, mean_q: 13.784216
  1367/50000: episode: 93, duration: 0.097s, episode steps: 19, steps per second: 195, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.077 [-0.978, 1.434], loss: 4.529795, mean_absolute_error: 7.523077, mean_q: 13.701276
  1386/50000: episode: 94, duration: 0.096s, episode steps: 19, steps per second: 199, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.368 [0.000, 1.000], mean observation: 0.070 [-1.203, 2.013], loss: 5.286047, mean_absolute_error: 7.739432, mean_q: 13.998727
  1406/50000: episode: 95, duration: 0.105s, episode steps: 20, steps per second: 190, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.099 [-1.143, 2.024], loss: 5.294816, mean_absolute_error: 7.584435, mean_q: 13.731946
  1417/50000: episode: 96, duration: 0.056s, episode steps: 11, steps per second: 196, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.105 [-1.940, 1.224], loss: 7.366562, mean_absolute_error: 7.528008, mean_q: 13.274985
  1427/50000: episode: 97, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.124 [-1.672, 0.970], loss: 7.986901, mean_absolute_error: 7.681474, mean_q: 13.435199
  1445/50000: episode: 98, duration: 0.091s, episode steps: 18, steps per second: 198, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.092 [-0.766, 1.525], loss: 4.880304, mean_absolute_error: 7.425073, mean_q: 13.464749
  1458/50000: episode: 99, duration: 0.070s, episode steps: 13, steps per second: 185, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.104 [-1.385, 2.321], loss: 7.076572, mean_absolute_error: 7.628462, mean_q: 13.723431
  1470/50000: episode: 100, duration: 0.075s, episode steps: 12, steps per second: 161, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.075 [-1.213, 1.747], loss: 6.863124, mean_absolute_error: 7.648062, mean_q: 13.800088
  1503/50000: episode: 101, duration: 0.168s, episode steps: 33, steps per second: 196, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.002 [-0.957, 1.206], loss: 2.988435, mean_absolute_error: 7.374576, mean_q: 13.880603
  1524/50000: episode: 102, duration: 0.105s, episode steps: 21, steps per second: 200, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.073 [-0.809, 1.553], loss: 4.292527, mean_absolute_error: 7.468526, mean_q: 13.822645
  1540/50000: episode: 103, duration: 0.084s, episode steps: 16, steps per second: 191, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.088 [-0.970, 1.520], loss: 5.402256, mean_absolute_error: 7.652518, mean_q: 13.878765
  1554/50000: episode: 104, duration: 0.074s, episode steps: 14, steps per second: 189, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.099 [-0.796, 1.588], loss: 5.963116, mean_absolute_error: 7.540902, mean_q: 13.616388
  1581/50000: episode: 105, duration: 0.136s, episode steps: 27, steps per second: 198, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.407 [0.000, 1.000], mean observation: 0.048 [-1.200, 2.021], loss: 3.417555, mean_absolute_error: 7.561060, mean_q: 13.969253
  1623/50000: episode: 106, duration: 0.218s, episode steps: 42, steps per second: 192, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.073 [-0.427, 1.249], loss: 2.309665, mean_absolute_error: 7.619464, mean_q: 14.176592
  1667/50000: episode: 107, duration: 0.238s, episode steps: 44, steps per second: 185, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: 0.029 [-0.958, 1.421], loss: 2.543570, mean_absolute_error: 8.165905, mean_q: 15.278205
  1685/50000: episode: 108, duration: 0.093s, episode steps: 18, steps per second: 194, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.095 [-0.939, 1.648], loss: 6.056310, mean_absolute_error: 8.600544, mean_q: 15.832375
  1713/50000: episode: 109, duration: 0.146s, episode steps: 28, steps per second: 192, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: 0.035 [-1.580, 1.380], loss: 4.875755, mean_absolute_error: 8.620508, mean_q: 15.959163
  1751/50000: episode: 110, duration: 0.191s, episode steps: 38, steps per second: 199, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.553 [0.000, 1.000], mean observation: -0.054 [-1.560, 0.815], loss: 4.223584, mean_absolute_error: 9.045844, mean_q: 16.987212
  1773/50000: episode: 111, duration: 0.115s, episode steps: 22, steps per second: 192, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.081 [-1.263, 0.591], loss: 6.806491, mean_absolute_error: 9.436285, mean_q: 17.374598
  1786/50000: episode: 112, duration: 0.067s, episode steps: 13, steps per second: 193, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.106 [-1.353, 2.219], loss: 10.197704, mean_absolute_error: 9.369766, mean_q: 16.934695
  1795/50000: episode: 113, duration: 0.046s, episode steps: 9, steps per second: 197, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.163 [-1.735, 2.799], loss: 13.966595, mean_absolute_error: 9.296889, mean_q: 16.307370
  1835/50000: episode: 114, duration: 0.209s, episode steps: 40, steps per second: 192, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.091 [-0.863, 0.551], loss: 4.451001, mean_absolute_error: 9.267688, mean_q: 17.332600
  1857/50000: episode: 115, duration: 0.112s, episode steps: 22, steps per second: 196, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.077 [-0.627, 1.019], loss: 5.774914, mean_absolute_error: 8.915163, mean_q: 16.538937
  1873/50000: episode: 116, duration: 0.096s, episode steps: 16, steps per second: 167, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.098 [-0.818, 1.392], loss: 6.949537, mean_absolute_error: 8.865497, mean_q: 16.212513
  1893/50000: episode: 117, duration: 0.103s, episode steps: 20, steps per second: 195, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.089 [-1.677, 0.809], loss: 8.058887, mean_absolute_error: 9.587154, mean_q: 17.580686
  1921/50000: episode: 118, duration: 0.144s, episode steps: 28, steps per second: 194, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.393 [0.000, 1.000], mean observation: 0.011 [-1.383, 1.864], loss: 4.448441, mean_absolute_error: 9.129225, mean_q: 17.090804
  1945/50000: episode: 119, duration: 0.130s, episode steps: 24, steps per second: 184, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.036 [-0.962, 1.471], loss: 5.175847, mean_absolute_error: 9.015135, mean_q: 16.760836
  1962/50000: episode: 120, duration: 0.090s, episode steps: 17, steps per second: 190, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.096 [-1.318, 0.647], loss: 9.912162, mean_absolute_error: 9.966031, mean_q: 18.154238
  1975/50000: episode: 121, duration: 0.067s, episode steps: 13, steps per second: 194, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.123 [-1.752, 0.950], loss: 11.201741, mean_absolute_error: 9.745738, mean_q: 17.557884
  1994/50000: episode: 122, duration: 0.099s, episode steps: 19, steps per second: 191, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.632 [0.000, 1.000], mean observation: -0.039 [-1.777, 1.158], loss: 6.692565, mean_absolute_error: 8.793614, mean_q: 16.448897
  2005/50000: episode: 123, duration: 0.060s, episode steps: 11, steps per second: 183, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.139 [-1.675, 0.945], loss: 10.017159, mean_absolute_error: 9.045535, mean_q: 16.583818
  2020/50000: episode: 124, duration: 0.078s, episode steps: 15, steps per second: 193, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.733 [0.000, 1.000], mean observation: -0.069 [-2.294, 1.560], loss: 5.566377, mean_absolute_error: 8.155498, mean_q: 15.391061
  2030/50000: episode: 125, duration: 0.050s, episode steps: 10, steps per second: 198, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.142 [-2.552, 1.563], loss: 7.087822, mean_absolute_error: 7.608772, mean_q: 14.097316
  2043/50000: episode: 126, duration: 0.066s, episode steps: 13, steps per second: 196, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.117 [-1.862, 1.018], loss: 6.580272, mean_absolute_error: 8.151994, mean_q: 14.792400
  2053/50000: episode: 127, duration: 0.066s, episode steps: 10, steps per second: 151, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.118 [-1.577, 1.014], loss: 8.371676, mean_absolute_error: 7.462347, mean_q: 13.142646
  2077/50000: episode: 128, duration: 0.121s, episode steps: 24, steps per second: 199, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.099 [-0.949, 1.944], loss: 4.752394, mean_absolute_error: 8.260105, mean_q: 15.160401
  2089/50000: episode: 129, duration: 0.062s, episode steps: 12, steps per second: 193, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.100 [-0.813, 1.521], loss: 8.987473, mean_absolute_error: 8.599007, mean_q: 15.283262
  2108/50000: episode: 130, duration: 0.101s, episode steps: 19, steps per second: 188, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.072 [-1.130, 0.758], loss: 4.929512, mean_absolute_error: 7.363917, mean_q: 13.335358
  2162/50000: episode: 131, duration: 0.273s, episode steps: 54, steps per second: 198, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.407 [0.000, 1.000], mean observation: -0.171 [-2.105, 1.923], loss: 3.846392, mean_absolute_error: 8.953281, mean_q: 17.013647
  2178/50000: episode: 132, duration: 0.081s, episode steps: 16, steps per second: 196, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.096 [-0.966, 1.599], loss: 7.205556, mean_absolute_error: 8.780042, mean_q: 16.054749
  2215/50000: episode: 133, duration: 0.190s, episode steps: 37, steps per second: 195, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: 0.103 [-0.783, 1.939], loss: 3.040665, mean_absolute_error: 8.493805, mean_q: 16.055618
  2238/50000: episode: 134, duration: 0.118s, episode steps: 23, steps per second: 195, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.391 [0.000, 1.000], mean observation: 0.072 [-1.161, 2.069], loss: 4.896946, mean_absolute_error: 8.934770, mean_q: 16.625316
  2251/50000: episode: 135, duration: 0.079s, episode steps: 13, steps per second: 165, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.111 [-1.710, 0.969], loss: 7.776305, mean_absolute_error: 8.793187, mean_q: 16.085929
  2284/50000: episode: 136, duration: 0.170s, episode steps: 33, steps per second: 195, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.033 [-1.401, 0.926], loss: 3.571844, mean_absolute_error: 8.590029, mean_q: 16.162812
  2322/50000: episode: 137, duration: 0.190s, episode steps: 38, steps per second: 200, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.553 [0.000, 1.000], mean observation: 0.025 [-1.518, 1.344], loss: 2.990625, mean_absolute_error: 8.602193, mean_q: 16.272082
  2343/50000: episode: 138, duration: 0.110s, episode steps: 21, steps per second: 190, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.069 [-0.961, 1.659], loss: 6.303631, mean_absolute_error: 9.489055, mean_q: 17.640144
  2364/50000: episode: 139, duration: 0.107s, episode steps: 21, steps per second: 197, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.075 [-0.827, 1.376], loss: 5.780928, mean_absolute_error: 9.301776, mean_q: 17.473495
  2406/50000: episode: 140, duration: 0.213s, episode steps: 42, steps per second: 197, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.381 [0.000, 1.000], mean observation: -0.136 [-1.908, 2.086], loss: 4.550694, mean_absolute_error: 9.799884, mean_q: 18.841309
  2424/50000: episode: 141, duration: 0.090s, episode steps: 18, steps per second: 201, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.102 [-0.805, 1.263], loss: 7.869169, mean_absolute_error: 9.498236, mean_q: 17.564216
  2457/50000: episode: 142, duration: 0.183s, episode steps: 33, steps per second: 181, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.097 [-1.599, 0.603], loss: 5.082826, mean_absolute_error: 10.116406, mean_q: 19.089681
  2485/50000: episode: 143, duration: 0.142s, episode steps: 28, steps per second: 198, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: 0.062 [-1.582, 1.391], loss: 4.072998, mean_absolute_error: 8.997161, mean_q: 17.043203
  2503/50000: episode: 144, duration: 0.098s, episode steps: 18, steps per second: 184, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.073 [-1.478, 0.955], loss: 6.957329, mean_absolute_error: 9.572942, mean_q: 17.940961
  2517/50000: episode: 145, duration: 0.070s, episode steps: 14, steps per second: 200, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.074 [-1.746, 1.191], loss: 8.138362, mean_absolute_error: 9.116995, mean_q: 16.740790
  2554/50000: episode: 146, duration: 0.194s, episode steps: 37, steps per second: 191, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.197 [-1.007, 0.650], loss: 7.276675, mean_absolute_error: 10.596929, mean_q: 19.772187
  2576/50000: episode: 147, duration: 0.110s, episode steps: 22, steps per second: 200, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.011 [-1.393, 1.887], loss: 6.707823, mean_absolute_error: 9.943064, mean_q: 18.733704
  2590/50000: episode: 148, duration: 0.070s, episode steps: 14, steps per second: 199, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.119 [-0.986, 1.777], loss: 9.585371, mean_absolute_error: 9.692100, mean_q: 18.021055
  2611/50000: episode: 149, duration: 0.109s, episode steps: 21, steps per second: 192, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.053 [-1.556, 2.340], loss: 5.774959, mean_absolute_error: 9.526193, mean_q: 18.055212
  2668/50000: episode: 150, duration: 0.302s, episode steps: 57, steps per second: 189, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.036 [-0.779, 0.911], loss: 3.298664, mean_absolute_error: 10.030634, mean_q: 19.141965
  2683/50000: episode: 151, duration: 0.077s, episode steps: 15, steps per second: 195, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.102 [-0.988, 1.804], loss: 8.630546, mean_absolute_error: 9.951829, mean_q: 18.548358
  2697/50000: episode: 152, duration: 0.071s, episode steps: 14, steps per second: 197, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.115 [-1.872, 1.129], loss: 9.926003, mean_absolute_error: 10.171782, mean_q: 18.790618
  2712/50000: episode: 153, duration: 0.080s, episode steps: 15, steps per second: 187, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.084 [-1.326, 0.794], loss: 9.510056, mean_absolute_error: 9.896985, mean_q: 18.116082
  2737/50000: episode: 154, duration: 0.125s, episode steps: 25, steps per second: 199, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.560 [0.000, 1.000], mean observation: -0.098 [-1.593, 0.757], loss: 6.256787, mean_absolute_error: 9.828942, mean_q: 18.254084
  2774/50000: episode: 155, duration: 0.191s, episode steps: 37, steps per second: 194, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: 0.055 [-0.767, 1.391], loss: 3.670348, mean_absolute_error: 9.609309, mean_q: 18.190365
  2803/50000: episode: 156, duration: 0.145s, episode steps: 29, steps per second: 201, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: 0.024 [-1.181, 1.567], loss: 5.533037, mean_absolute_error: 9.810822, mean_q: 18.438069
  2847/50000: episode: 157, duration: 0.236s, episode steps: 44, steps per second: 187, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.095 [-0.509, 0.886], loss: 3.859308, mean_absolute_error: 10.021898, mean_q: 19.039494
  2872/50000: episode: 158, duration: 0.128s, episode steps: 25, steps per second: 195, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: -0.092 [-1.227, 0.791], loss: 9.662655, mean_absolute_error: 10.900508, mean_q: 20.182262
  2891/50000: episode: 159, duration: 0.100s, episode steps: 19, steps per second: 191, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.368 [0.000, 1.000], mean observation: 0.090 [-0.962, 1.872], loss: 7.254680, mean_absolute_error: 10.253431, mean_q: 19.162149
  2902/50000: episode: 160, duration: 0.056s, episode steps: 11, steps per second: 196, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.102 [-1.335, 0.838], loss: 14.402993, mean_absolute_error: 10.307297, mean_q: 18.609846
  2934/50000: episode: 161, duration: 0.166s, episode steps: 32, steps per second: 193, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.594 [0.000, 1.000], mean observation: 0.032 [-1.802, 1.529], loss: 3.074315, mean_absolute_error: 9.007971, mean_q: 17.153140
  2994/50000: episode: 162, duration: 0.306s, episode steps: 60, steps per second: 196, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.086 [-1.243, 0.988], loss: 4.962105, mean_absolute_error: 10.423256, mean_q: 19.810870
  3021/50000: episode: 163, duration: 0.138s, episode steps: 27, steps per second: 196, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: 0.101 [-0.628, 1.169], loss: 6.166281, mean_absolute_error: 10.549830, mean_q: 20.010784
  3040/50000: episode: 164, duration: 0.119s, episode steps: 19, steps per second: 159, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.579 [0.000, 1.000], mean observation: -0.050 [-1.616, 1.005], loss: 7.301318, mean_absolute_error: 10.278060, mean_q: 19.261320
  3064/50000: episode: 165, duration: 0.139s, episode steps: 24, steps per second: 172, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.092 [-1.212, 0.575], loss: 9.250514, mean_absolute_error: 10.945860, mean_q: 20.316151
  3075/50000: episode: 166, duration: 0.069s, episode steps: 11, steps per second: 160, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.100 [-1.031, 1.644], loss: 13.207865, mean_absolute_error: 10.522887, mean_q: 19.203966
  3139/50000: episode: 167, duration: 0.374s, episode steps: 64, steps per second: 171, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.199 [-0.908, 1.508], loss: 2.488188, mean_absolute_error: 10.158240, mean_q: 19.466525
  3152/50000: episode: 168, duration: 0.078s, episode steps: 13, steps per second: 167, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.086 [-1.506, 1.019], loss: 16.408262, mean_absolute_error: 11.002942, mean_q: 19.886364
  3168/50000: episode: 169, duration: 0.094s, episode steps: 16, steps per second: 170, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.104 [-1.349, 0.780], loss: 11.469543, mean_absolute_error: 11.062606, mean_q: 20.434448
  3181/50000: episode: 170, duration: 0.080s, episode steps: 13, steps per second: 163, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.097 [-1.775, 1.008], loss: 11.233980, mean_absolute_error: 10.275191, mean_q: 18.719455
  3202/50000: episode: 171, duration: 0.123s, episode steps: 21, steps per second: 171, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.071 [-0.643, 1.347], loss: 6.913374, mean_absolute_error: 10.220895, mean_q: 19.344207
  3240/50000: episode: 172, duration: 0.244s, episode steps: 38, steps per second: 155, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.073 [-1.503, 0.624], loss: 4.539991, mean_absolute_error: 10.314885, mean_q: 19.524548
  3292/50000: episode: 173, duration: 0.333s, episode steps: 52, steps per second: 156, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.404 [0.000, 1.000], mean observation: -0.200 [-2.217, 1.732], loss: 4.399804, mean_absolute_error: 10.882760, mean_q: 20.923681
  3327/50000: episode: 174, duration: 0.211s, episode steps: 35, steps per second: 166, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.017 [-1.276, 0.916], loss: 6.401538, mean_absolute_error: 10.848373, mean_q: 20.513706
  3336/50000: episode: 175, duration: 0.054s, episode steps: 9, steps per second: 167, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.132 [-1.350, 2.300], loss: 15.146261, mean_absolute_error: 10.810083, mean_q: 19.418280
  3359/50000: episode: 176, duration: 0.138s, episode steps: 23, steps per second: 167, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.034 [-0.807, 1.240], loss: 6.569116, mean_absolute_error: 10.458669, mean_q: 19.604022
  3412/50000: episode: 177, duration: 0.329s, episode steps: 53, steps per second: 161, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.112 [-1.495, 1.262], loss: 4.103939, mean_absolute_error: 10.705370, mean_q: 20.471562
  3433/50000: episode: 178, duration: 0.124s, episode steps: 21, steps per second: 170, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.619 [0.000, 1.000], mean observation: -0.041 [-1.771, 1.157], loss: 7.773239, mean_absolute_error: 11.297621, mean_q: 21.303448
  3456/50000: episode: 179, duration: 0.139s, episode steps: 23, steps per second: 166, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.391 [0.000, 1.000], mean observation: 0.075 [-0.995, 1.871], loss: 6.526524, mean_absolute_error: 10.707233, mean_q: 20.061532
  3494/50000: episode: 180, duration: 0.227s, episode steps: 38, steps per second: 167, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.125 [-1.717, 0.453], loss: 5.961532, mean_absolute_error: 11.750078, mean_q: 22.411366
  3526/50000: episode: 181, duration: 0.197s, episode steps: 32, steps per second: 162, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: 0.037 [-0.944, 1.340], loss: 5.949431, mean_absolute_error: 11.139109, mean_q: 21.030004
  3561/50000: episode: 182, duration: 0.224s, episode steps: 35, steps per second: 157, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.114 [-1.186, 0.563], loss: 7.934447, mean_absolute_error: 11.820316, mean_q: 22.436499
  3582/50000: episode: 183, duration: 0.124s, episode steps: 21, steps per second: 169, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.050 [-1.183, 1.647], loss: 8.078343, mean_absolute_error: 10.948134, mean_q: 20.418672
  3603/50000: episode: 184, duration: 0.127s, episode steps: 21, steps per second: 165, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.381 [0.000, 1.000], mean observation: 0.040 [-1.224, 1.823], loss: 6.319400, mean_absolute_error: 10.848357, mean_q: 20.343560
  3663/50000: episode: 185, duration: 0.357s, episode steps: 60, steps per second: 168, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.055 [-0.566, 1.147], loss: 3.238905, mean_absolute_error: 11.395191, mean_q: 21.789113
  3677/50000: episode: 186, duration: 0.082s, episode steps: 14, steps per second: 170, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.106 [-1.775, 1.169], loss: 16.434987, mean_absolute_error: 11.921155, mean_q: 22.136405
  3701/50000: episode: 187, duration: 0.143s, episode steps: 24, steps per second: 168, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.052 [-0.790, 1.555], loss: 7.378927, mean_absolute_error: 11.254005, mean_q: 20.848042
  3730/50000: episode: 188, duration: 0.183s, episode steps: 29, steps per second: 159, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.069 [-1.012, 0.551], loss: 9.243029, mean_absolute_error: 11.689309, mean_q: 22.056472
  3758/50000: episode: 189, duration: 0.180s, episode steps: 28, steps per second: 156, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.102 [-0.454, 1.002], loss: 6.752675, mean_absolute_error: 11.161650, mean_q: 20.821235
  3782/50000: episode: 190, duration: 0.143s, episode steps: 24, steps per second: 167, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.100 [-0.758, 1.163], loss: 9.354690, mean_absolute_error: 11.028364, mean_q: 20.530108
  3801/50000: episode: 191, duration: 0.115s, episode steps: 19, steps per second: 165, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.057 [-1.437, 0.976], loss: 10.271051, mean_absolute_error: 11.108931, mean_q: 20.884535
  3813/50000: episode: 192, duration: 0.072s, episode steps: 12, steps per second: 167, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.107 [-1.407, 0.800], loss: 13.613678, mean_absolute_error: 10.837849, mean_q: 20.027088
  3824/50000: episode: 193, duration: 0.067s, episode steps: 11, steps per second: 164, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.121 [-1.294, 0.768], loss: 13.447240, mean_absolute_error: 10.378517, mean_q: 18.958699
  3836/50000: episode: 194, duration: 0.074s, episode steps: 12, steps per second: 161, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.126 [-1.694, 0.961], loss: 9.243366, mean_absolute_error: 9.829017, mean_q: 18.072136
  3853/50000: episode: 195, duration: 0.099s, episode steps: 17, steps per second: 172, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.082 [-1.247, 0.648], loss: 7.517388, mean_absolute_error: 9.980736, mean_q: 18.089924
  3896/50000: episode: 196, duration: 0.269s, episode steps: 43, steps per second: 160, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.019 [-1.353, 1.199], loss: 3.474013, mean_absolute_error: 9.182764, mean_q: 17.330009
  3939/50000: episode: 197, duration: 0.257s, episode steps: 43, steps per second: 167, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: 0.024 [-1.188, 1.614], loss: 3.867548, mean_absolute_error: 10.066898, mean_q: 19.119190
  3955/50000: episode: 198, duration: 0.092s, episode steps: 16, steps per second: 173, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.084 [-1.480, 0.766], loss: 6.934474, mean_absolute_error: 9.480577, mean_q: 17.872644
  3965/50000: episode: 199, duration: 0.059s, episode steps: 10, steps per second: 168, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.138 [-0.959, 1.706], loss: 11.820911, mean_absolute_error: 9.988375, mean_q: 18.186665
  3991/50000: episode: 200, duration: 0.157s, episode steps: 26, steps per second: 165, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.110 [-1.177, 0.751], loss: 7.933203, mean_absolute_error: 10.102282, mean_q: 18.658500
  4029/50000: episode: 201, duration: 0.228s, episode steps: 38, steps per second: 167, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.166 [-0.820, 1.718], loss: 3.060918, mean_absolute_error: 9.829425, mean_q: 18.731434
  4044/50000: episode: 202, duration: 0.101s, episode steps: 15, steps per second: 149, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.073 [-1.222, 1.828], loss: 6.808873, mean_absolute_error: 9.733778, mean_q: 18.141993
  4067/50000: episode: 203, duration: 0.138s, episode steps: 23, steps per second: 166, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.391 [0.000, 1.000], mean observation: 0.079 [-0.998, 1.874], loss: 4.097800, mean_absolute_error: 9.866369, mean_q: 18.585405
  4085/50000: episode: 204, duration: 0.104s, episode steps: 18, steps per second: 172, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.099 [-0.940, 1.727], loss: 5.523496, mean_absolute_error: 9.898183, mean_q: 18.214521
  4130/50000: episode: 205, duration: 0.270s, episode steps: 45, steps per second: 167, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.017 [-1.369, 1.653], loss: 4.485105, mean_absolute_error: 9.893716, mean_q: 18.780435
  4203/50000: episode: 206, duration: 0.430s, episode steps: 73, steps per second: 170, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.100 [-1.235, 0.968], loss: 4.579830, mean_absolute_error: 11.210487, mean_q: 21.557164
  4282/50000: episode: 207, duration: 0.474s, episode steps: 79, steps per second: 167, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: 0.125 [-1.062, 1.300], loss: 3.845616, mean_absolute_error: 11.806496, mean_q: 22.802676
  4327/50000: episode: 208, duration: 0.272s, episode steps: 45, steps per second: 165, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.052 [-0.849, 0.578], loss: 7.586606, mean_absolute_error: 13.151596, mean_q: 25.239852
  4343/50000: episode: 209, duration: 0.094s, episode steps: 16, steps per second: 171, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.079 [-1.333, 0.831], loss: 16.390223, mean_absolute_error: 13.377180, mean_q: 25.030261
  4368/50000: episode: 210, duration: 0.148s, episode steps: 25, steps per second: 169, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.054 [-1.436, 0.927], loss: 10.601359, mean_absolute_error: 12.455715, mean_q: 23.777188
  4396/50000: episode: 211, duration: 0.176s, episode steps: 28, steps per second: 159, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.059 [-1.167, 0.561], loss: 8.346882, mean_absolute_error: 12.247519, mean_q: 23.448384
  4425/50000: episode: 212, duration: 0.171s, episode steps: 29, steps per second: 170, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.586 [0.000, 1.000], mean observation: -0.019 [-1.655, 1.125], loss: 6.080991, mean_absolute_error: 12.024243, mean_q: 22.988648
  4466/50000: episode: 213, duration: 0.246s, episode steps: 41, steps per second: 167, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.080 [-1.027, 0.625], loss: 6.688483, mean_absolute_error: 12.089910, mean_q: 23.138053
  4490/50000: episode: 214, duration: 0.142s, episode steps: 24, steps per second: 169, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: 0.074 [-0.774, 1.356], loss: 8.660860, mean_absolute_error: 11.908966, mean_q: 22.334493
  4516/50000: episode: 215, duration: 0.157s, episode steps: 26, steps per second: 166, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.063 [-0.762, 1.434], loss: 6.380792, mean_absolute_error: 11.868915, mean_q: 22.462647
  4550/50000: episode: 216, duration: 0.198s, episode steps: 34, steps per second: 172, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.061 [-1.531, 0.615], loss: 4.907342, mean_absolute_error: 12.030740, mean_q: 23.240672
  4573/50000: episode: 217, duration: 0.153s, episode steps: 23, steps per second: 151, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: 0.060 [-1.003, 1.626], loss: 7.772258, mean_absolute_error: 11.762282, mean_q: 21.869332
  4683/50000: episode: 218, duration: 0.645s, episode steps: 110, steps per second: 171, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.336 [-1.204, 1.973], loss: 3.963657, mean_absolute_error: 12.799450, mean_q: 24.779445
  4704/50000: episode: 219, duration: 0.107s, episode steps: 21, steps per second: 196, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.087 [-1.171, 0.613], loss: 15.896070, mean_absolute_error: 13.900569, mean_q: 26.218661
  4734/50000: episode: 220, duration: 0.168s, episode steps: 30, steps per second: 179, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.113 [-0.951, 0.656], loss: 9.530269, mean_absolute_error: 13.056473, mean_q: 25.222844
  4761/50000: episode: 221, duration: 0.137s, episode steps: 27, steps per second: 197, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.078 [-1.124, 0.546], loss: 8.713241, mean_absolute_error: 13.078865, mean_q: 25.027926
  4836/50000: episode: 222, duration: 0.380s, episode steps: 75, steps per second: 197, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.203 [-1.495, 0.602], loss: 5.023845, mean_absolute_error: 13.107590, mean_q: 25.482927
  4860/50000: episode: 223, duration: 0.121s, episode steps: 24, steps per second: 199, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.076 [-0.900, 0.628], loss: 11.272414, mean_absolute_error: 13.269508, mean_q: 25.250442
  4894/50000: episode: 224, duration: 0.175s, episode steps: 34, steps per second: 194, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.056 [-1.163, 0.576], loss: 7.629195, mean_absolute_error: 13.316415, mean_q: 25.505872
  4924/50000: episode: 225, duration: 0.153s, episode steps: 30, steps per second: 196, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.433 [0.000, 1.000], mean observation: -0.118 [-0.802, 0.448], loss: 9.288909, mean_absolute_error: 13.155439, mean_q: 24.848313
  5018/50000: episode: 226, duration: 0.491s, episode steps: 94, steps per second: 191, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.089 [-1.659, 0.956], loss: 3.839509, mean_absolute_error: 12.911227, mean_q: 25.130569
  5070/50000: episode: 227, duration: 0.265s, episode steps: 52, steps per second: 196, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.044 [-0.956, 1.625], loss: 8.070400, mean_absolute_error: 14.718506, mean_q: 28.115586
  5105/50000: episode: 228, duration: 0.198s, episode steps: 35, steps per second: 177, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.055 [-1.289, 0.794], loss: 8.344136, mean_absolute_error: 14.476487, mean_q: 27.877124
  5125/50000: episode: 229, duration: 0.122s, episode steps: 20, steps per second: 164, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.093 [-1.187, 0.801], loss: 17.772566, mean_absolute_error: 14.201776, mean_q: 26.410940
  5159/50000: episode: 230, duration: 0.173s, episode steps: 34, steps per second: 197, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.008 [-1.018, 1.558], loss: 7.815434, mean_absolute_error: 13.963032, mean_q: 26.849614
  5255/50000: episode: 231, duration: 0.491s, episode steps: 96, steps per second: 195, episode reward: 96.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.024 [-1.257, 1.097], loss: 4.275634, mean_absolute_error: 14.628136, mean_q: 28.412809
  5281/50000: episode: 232, duration: 0.134s, episode steps: 26, steps per second: 194, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.080 [-1.142, 1.814], loss: 13.754470, mean_absolute_error: 14.980363, mean_q: 28.295526
  5331/50000: episode: 233, duration: 0.265s, episode steps: 50, steps per second: 189, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.022 [-1.154, 1.384], loss: 7.864478, mean_absolute_error: 14.935667, mean_q: 28.983259
  5378/50000: episode: 234, duration: 0.242s, episode steps: 47, steps per second: 194, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.107 [-1.108, 0.874], loss: 6.804948, mean_absolute_error: 14.298474, mean_q: 27.864576
  5414/50000: episode: 235, duration: 0.183s, episode steps: 36, steps per second: 197, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.071 [-0.910, 0.866], loss: 9.413532, mean_absolute_error: 14.496548, mean_q: 27.759242
  5458/50000: episode: 236, duration: 0.227s, episode steps: 44, steps per second: 194, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.065 [-1.076, 0.728], loss: 7.187080, mean_absolute_error: 14.354919, mean_q: 27.844046
  5472/50000: episode: 237, duration: 0.073s, episode steps: 14, steps per second: 193, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.079 [-1.514, 1.005], loss: 22.056462, mean_absolute_error: 14.191265, mean_q: 26.854828
  5496/50000: episode: 238, duration: 0.121s, episode steps: 24, steps per second: 198, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: -0.060 [-1.325, 0.744], loss: 9.465510, mean_absolute_error: 13.952819, mean_q: 26.742174
  5525/50000: episode: 239, duration: 0.172s, episode steps: 29, steps per second: 168, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.074 [-1.035, 0.560], loss: 8.524599, mean_absolute_error: 13.069809, mean_q: 24.988904
  5566/50000: episode: 240, duration: 0.212s, episode steps: 41, steps per second: 194, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: 0.024 [-0.794, 1.538], loss: 7.239532, mean_absolute_error: 13.876779, mean_q: 26.674135
  5643/50000: episode: 241, duration: 0.391s, episode steps: 77, steps per second: 197, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.259 [-1.504, 0.949], loss: 5.138805, mean_absolute_error: 13.170481, mean_q: 25.654580
  5685/50000: episode: 242, duration: 0.216s, episode steps: 42, steps per second: 195, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.006 [-0.825, 1.137], loss: 9.128573, mean_absolute_error: 14.997439, mean_q: 28.728851
  5719/50000: episode: 243, duration: 0.182s, episode steps: 34, steps per second: 187, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.441 [0.000, 1.000], mean observation: 0.071 [-0.948, 1.789], loss: 8.219800, mean_absolute_error: 14.970519, mean_q: 28.589689
  5807/50000: episode: 244, duration: 0.448s, episode steps: 88, steps per second: 196, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.330 [-2.062, 0.874], loss: 5.455659, mean_absolute_error: 13.914614, mean_q: 27.225396
  5845/50000: episode: 245, duration: 0.191s, episode steps: 38, steps per second: 199, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: 0.050 [-0.852, 1.666], loss: 8.261470, mean_absolute_error: 15.795210, mean_q: 30.348063
  5957/50000: episode: 246, duration: 0.579s, episode steps: 112, steps per second: 193, episode reward: 112.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.428 [-2.272, 1.119], loss: 6.694429, mean_absolute_error: 14.991267, mean_q: 29.418463
  6114/50000: episode: 247, duration: 0.803s, episode steps: 157, steps per second: 195, episode reward: 157.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.289 [-1.653, 1.014], loss: 6.000914, mean_absolute_error: 18.091205, mean_q: 35.548846
  6148/50000: episode: 248, duration: 0.171s, episode steps: 34, steps per second: 199, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.028 [-1.208, 0.923], loss: 15.830687, mean_absolute_error: 18.751788, mean_q: 35.938658
  6281/50000: episode: 249, duration: 0.675s, episode steps: 133, steps per second: 197, episode reward: 133.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: -0.118 [-1.355, 1.296], loss: 6.460854, mean_absolute_error: 19.775642, mean_q: 38.943156
  6307/50000: episode: 250, duration: 0.130s, episode steps: 26, steps per second: 200, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.112 [-0.739, 0.417], loss: 21.004972, mean_absolute_error: 18.907808, mean_q: 35.415352
  6382/50000: episode: 251, duration: 0.395s, episode steps: 75, steps per second: 190, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.453 [0.000, 1.000], mean observation: -0.120 [-1.689, 1.442], loss: 8.772578, mean_absolute_error: 17.901452, mean_q: 34.741180
  6396/50000: episode: 252, duration: 0.072s, episode steps: 14, steps per second: 195, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.082 [-1.182, 1.906], loss: 33.763839, mean_absolute_error: 18.848993, mean_q: 35.111934
  6406/50000: episode: 253, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.162 [-1.522, 2.599], loss: 37.733409, mean_absolute_error: 18.278784, mean_q: 33.251367
  6417/50000: episode: 254, duration: 0.056s, episode steps: 11, steps per second: 197, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.136 [-1.187, 2.041], loss: 27.710157, mean_absolute_error: 17.083847, mean_q: 30.751456
  6433/50000: episode: 255, duration: 0.085s, episode steps: 16, steps per second: 188, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.079 [-0.948, 1.598], loss: 19.508273, mean_absolute_error: 16.326584, mean_q: 29.706673
  6512/50000: episode: 256, duration: 0.417s, episode steps: 79, steps per second: 189, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: 0.043 [-1.031, 1.173], loss: 6.512031, mean_absolute_error: 16.973488, mean_q: 33.070334
  6534/50000: episode: 257, duration: 0.111s, episode steps: 22, steps per second: 198, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.043 [-1.278, 0.823], loss: 17.895246, mean_absolute_error: 16.883202, mean_q: 32.381655
  6550/50000: episode: 258, duration: 0.083s, episode steps: 16, steps per second: 193, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.087 [-1.475, 0.995], loss: 20.954494, mean_absolute_error: 15.202741, mean_q: 29.045163
  6610/50000: episode: 259, duration: 0.308s, episode steps: 60, steps per second: 195, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.222 [-1.304, 0.704], loss: 4.644210, mean_absolute_error: 13.192521, mean_q: 25.903751
  6689/50000: episode: 260, duration: 0.400s, episode steps: 79, steps per second: 198, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.259 [-1.507, 0.796], loss: 4.633128, mean_absolute_error: 14.213024, mean_q: 27.937184
  6797/50000: episode: 261, duration: 0.563s, episode steps: 108, steps per second: 192, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.454 [0.000, 1.000], mean observation: -0.180 [-2.030, 0.711], loss: 3.796402, mean_absolute_error: 14.909238, mean_q: 29.399095
  6922/50000: episode: 262, duration: 0.643s, episode steps: 125, steps per second: 194, episode reward: 125.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.164 [-1.707, 0.973], loss: 4.463585, mean_absolute_error: 16.719555, mean_q: 32.928847
  6962/50000: episode: 263, duration: 0.208s, episode steps: 40, steps per second: 193, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.036 [-1.250, 0.961], loss: 12.024947, mean_absolute_error: 19.116711, mean_q: 37.199869
  7063/50000: episode: 264, duration: 0.512s, episode steps: 101, steps per second: 197, episode reward: 101.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.016 [-0.984, 0.788], loss: 5.137657, mean_absolute_error: 19.280882, mean_q: 37.838151
  7367/50000: episode: 265, duration: 1.572s, episode steps: 304, steps per second: 193, episode reward: 304.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.272 [-0.861, 2.406], loss: 7.410807, mean_absolute_error: 24.262890, mean_q: 48.123412
  7403/50000: episode: 266, duration: 0.180s, episode steps: 36, steps per second: 200, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.047 [-0.774, 1.106], loss: 34.685778, mean_absolute_error: 26.007588, mean_q: 49.341252
  7419/50000: episode: 267, duration: 0.088s, episode steps: 16, steps per second: 182, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.312 [0.000, 1.000], mean observation: 0.052 [-1.193, 1.899], loss: 46.510831, mean_absolute_error: 22.967254, mean_q: 42.411943
  7489/50000: episode: 268, duration: 0.356s, episode steps: 70, steps per second: 196, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.059 [-0.961, 0.764], loss: 7.940228, mean_absolute_error: 21.195882, mean_q: 41.912162
  7523/50000: episode: 269, duration: 0.183s, episode steps: 34, steps per second: 186, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.102 [-0.931, 0.566], loss: 15.835978, mean_absolute_error: 20.598529, mean_q: 40.232519
  7684/50000: episode: 270, duration: 0.813s, episode steps: 161, steps per second: 198, episode reward: 161.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.153 [-2.004, 0.965], loss: 3.382684, mean_absolute_error: 19.870894, mean_q: 39.496378
  7817/50000: episode: 271, duration: 0.688s, episode steps: 133, steps per second: 193, episode reward: 133.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.268 [-1.151, 0.807], loss: 6.370655, mean_absolute_error: 21.287755, mean_q: 42.161000
  8015/50000: episode: 272, duration: 1.008s, episode steps: 198, steps per second: 197, episode reward: 198.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.335 [-2.286, 0.968], loss: 4.610339, mean_absolute_error: 22.684382, mean_q: 44.977245
  8043/50000: episode: 273, duration: 0.142s, episode steps: 28, steps per second: 197, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.067 [-0.841, 1.730], loss: 41.750006, mean_absolute_error: 27.339334, mean_q: 51.905111
  8202/50000: episode: 274, duration: 0.814s, episode steps: 159, steps per second: 195, episode reward: 159.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.350 [-1.804, 0.743], loss: 5.370888, mean_absolute_error: 22.152643, mean_q: 44.047214
  8485/50000: episode: 275, duration: 1.434s, episode steps: 283, steps per second: 197, episode reward: 283.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.241 [-2.433, 1.173], loss: 4.209606, mean_absolute_error: 25.491224, mean_q: 50.804621
  8584/50000: episode: 276, duration: 0.516s, episode steps: 99, steps per second: 192, episode reward: 99.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.434 [0.000, 1.000], mean observation: -0.496 [-2.594, 0.773], loss: 7.522884, mean_absolute_error: 22.911209, mean_q: 45.417559
  8764/50000: episode: 277, duration: 0.918s, episode steps: 180, steps per second: 196, episode reward: 180.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.317 [-1.883, 0.740], loss: 5.298725, mean_absolute_error: 26.725714, mean_q: 53.306356
  8799/50000: episode: 278, duration: 0.175s, episode steps: 35, steps per second: 199, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.045 [-1.087, 0.819], loss: 35.384023, mean_absolute_error: 30.931122, mean_q: 60.016027
  8870/50000: episode: 279, duration: 0.361s, episode steps: 71, steps per second: 196, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: -0.008 [-1.191, 1.330], loss: 24.669048, mean_absolute_error: 28.368313, mean_q: 55.316075
  8895/50000: episode: 280, duration: 0.128s, episode steps: 25, steps per second: 196, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.077 [-0.976, 1.839], loss: 51.301211, mean_absolute_error: 28.682052, mean_q: 54.623802
  9045/50000: episode: 281, duration: 0.767s, episode steps: 150, steps per second: 196, episode reward: 150.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.185 [-2.012, 1.280], loss: 4.861125, mean_absolute_error: 22.810947, mean_q: 45.325154
  9237/50000: episode: 282, duration: 1.000s, episode steps: 192, steps per second: 192, episode reward: 192.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.025 [-1.540, 1.481], loss: 7.130194, mean_absolute_error: 27.341834, mean_q: 54.144240
  9341/50000: episode: 283, duration: 0.534s, episode steps: 104, steps per second: 195, episode reward: 104.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.501 [-2.444, 0.722], loss: 6.339668, mean_absolute_error: 20.377591, mean_q: 40.042132
  9369/50000: episode: 284, duration: 0.142s, episode steps: 28, steps per second: 197, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.045 [-0.990, 1.462], loss: 58.553781, mean_absolute_error: 28.519016, mean_q: 54.431807
  9509/50000: episode: 285, duration: 0.726s, episode steps: 140, steps per second: 193, episode reward: 140.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.022 [-1.119, 1.181], loss: 8.990305, mean_absolute_error: 26.571360, mean_q: 52.354394
  9733/50000: episode: 286, duration: 1.139s, episode steps: 224, steps per second: 197, episode reward: 224.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.260 [-1.974, 0.936], loss: 2.839023, mean_absolute_error: 24.419195, mean_q: 48.675331
  9895/50000: episode: 287, duration: 0.816s, episode steps: 162, steps per second: 199, episode reward: 162.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: -0.191 [-1.919, 1.409], loss: 9.656522, mean_absolute_error: 27.470052, mean_q: 54.475032
  9908/50000: episode: 288, duration: 0.074s, episode steps: 13, steps per second: 176, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.106 [-1.469, 0.971], loss: 85.891771, mean_absolute_error: 28.971596, mean_q: 54.540037
  9921/50000: episode: 289, duration: 0.067s, episode steps: 13, steps per second: 194, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.083 [-1.470, 1.004], loss: 55.189323, mean_absolute_error: 24.395836, mean_q: 46.373929
  9937/50000: episode: 290, duration: 0.088s, episode steps: 16, steps per second: 182, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: -0.095 [-1.249, 0.772], loss: 41.461151, mean_absolute_error: 20.959124, mean_q: 38.746390
 10020/50000: episode: 291, duration: 0.423s, episode steps: 83, steps per second: 196, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.229 [-1.631, 0.819], loss: 13.541077, mean_absolute_error: 20.174694, mean_q: 38.423082
 10036/50000: episode: 292, duration: 0.081s, episode steps: 16, steps per second: 198, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.312 [0.000, 1.000], mean observation: 0.058 [-1.199, 1.954], loss: 62.191471, mean_absolute_error: 24.732947, mean_q: 46.395280
 10046/50000: episode: 293, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.136 [-1.338, 2.173], loss: 77.057846, mean_absolute_error: 24.129725, mean_q: 43.954927
 10186/50000: episode: 294, duration: 0.713s, episode steps: 140, steps per second: 196, episode reward: 140.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: 0.450 [-0.719, 2.439], loss: 20.612148, mean_absolute_error: 28.010856, mean_q: 55.286976
 10355/50000: episode: 295, duration: 0.863s, episode steps: 169, steps per second: 196, episode reward: 169.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: -0.189 [-1.053, 1.095], loss: 7.109954, mean_absolute_error: 20.426777, mean_q: 40.299426
 10531/50000: episode: 296, duration: 0.900s, episode steps: 176, steps per second: 196, episode reward: 176.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.368 [-0.822, 2.405], loss: 13.652960, mean_absolute_error: 27.668565, mean_q: 54.804418
 10880/50000: episode: 297, duration: 1.763s, episode steps: 349, steps per second: 198, episode reward: 349.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.233 [-2.436, 1.039], loss: 1.784935, mean_absolute_error: 23.196175, mean_q: 46.372831
 11201/50000: episode: 298, duration: 1.652s, episode steps: 321, steps per second: 194, episode reward: 321.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.276 [-2.233, 0.999], loss: 3.614032, mean_absolute_error: 27.159380, mean_q: 54.248386
 11399/50000: episode: 299, duration: 1.003s, episode steps: 198, steps per second: 197, episode reward: 198.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.441 [-2.428, 1.013], loss: 5.800153, mean_absolute_error: 27.106838, mean_q: 53.989509
 11570/50000: episode: 300, duration: 0.872s, episode steps: 171, steps per second: 196, episode reward: 171.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.372 [-2.404, 0.995], loss: 7.957836, mean_absolute_error: 27.901768, mean_q: 55.586887
 11743/50000: episode: 301, duration: 0.876s, episode steps: 173, steps per second: 197, episode reward: 173.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.310 [-1.632, 1.016], loss: 8.767390, mean_absolute_error: 29.449563, mean_q: 58.640610
 12139/50000: episode: 302, duration: 1.998s, episode steps: 396, steps per second: 198, episode reward: 396.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.156 [-1.566, 1.329], loss: 5.327407, mean_absolute_error: 33.766460, mean_q: 67.474190
 12155/50000: episode: 303, duration: 0.081s, episode steps: 16, steps per second: 198, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.051 [-1.030, 1.647], loss: 127.317019, mean_absolute_error: 37.885096, mean_q: 70.066109
 12282/50000: episode: 304, duration: 0.637s, episode steps: 127, steps per second: 199, episode reward: 127.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.427 [-2.531, 0.878], loss: 4.157730, mean_absolute_error: 25.397474, mean_q: 50.527612
 12379/50000: episode: 305, duration: 0.499s, episode steps: 97, steps per second: 195, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.433 [0.000, 1.000], mean observation: -0.518 [-2.598, 0.760], loss: 5.219965, mean_absolute_error: 22.258147, mean_q: 44.491812
 12496/50000: episode: 306, duration: 0.592s, episode steps: 117, steps per second: 198, episode reward: 117.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.460 [-2.539, 0.656], loss: 3.000854, mean_absolute_error: 23.138746, mean_q: 46.118610
 12515/50000: episode: 307, duration: 0.107s, episode steps: 19, steps per second: 177, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.058 [-1.156, 0.828], loss: 85.379845, mean_absolute_error: 33.170465, mean_q: 63.571681
 12529/50000: episode: 308, duration: 0.071s, episode steps: 14, steps per second: 196, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.097 [-1.383, 0.772], loss: 82.309233, mean_absolute_error: 29.534781, mean_q: 55.784705
 12803/50000: episode: 309, duration: 1.387s, episode steps: 274, steps per second: 198, episode reward: 274.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.431 [-2.404, 0.988], loss: 3.445793, mean_absolute_error: 22.267957, mean_q: 44.317367
 13037/50000: episode: 310, duration: 1.191s, episode steps: 234, steps per second: 197, episode reward: 234.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.379 [-2.418, 0.808], loss: 5.231896, mean_absolute_error: 25.786630, mean_q: 51.397389
 13405/50000: episode: 311, duration: 1.889s, episode steps: 368, steps per second: 195, episode reward: 368.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.293 [-2.020, 1.143], loss: 6.659344, mean_absolute_error: 31.556742, mean_q: 62.883718
 13464/50000: episode: 312, duration: 0.303s, episode steps: 59, steps per second: 195, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.153 [-1.656, 1.395], loss: 41.108480, mean_absolute_error: 33.625111, mean_q: 64.940179
 13555/50000: episode: 313, duration: 0.472s, episode steps: 91, steps per second: 193, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.339 [-1.009, 1.497], loss: 39.218723, mean_absolute_error: 37.669088, mean_q: 74.367986
 13567/50000: episode: 314, duration: 0.061s, episode steps: 12, steps per second: 196, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.108 [-1.533, 2.548], loss: 130.662524, mean_absolute_error: 34.117417, mean_q: 62.840752
 13582/50000: episode: 315, duration: 0.076s, episode steps: 15, steps per second: 196, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.110 [-0.957, 1.714], loss: 82.685306, mean_absolute_error: 31.281390, mean_q: 58.167048
 13596/50000: episode: 316, duration: 0.077s, episode steps: 14, steps per second: 181, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.110 [-0.745, 1.333], loss: 96.557300, mean_absolute_error: 29.315043, mean_q: 53.228642
 14055/50000: episode: 317, duration: 2.327s, episode steps: 459, steps per second: 197, episode reward: 459.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.497 [0.000, 1.000], mean observation: -0.214 [-2.406, 0.931], loss: 3.442564, mean_absolute_error: 26.365226, mean_q: 52.738161
 14190/50000: episode: 318, duration: 0.691s, episode steps: 135, steps per second: 195, episode reward: 135.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.447 [-2.427, 0.980], loss: 7.289306, mean_absolute_error: 25.418954, mean_q: 50.680919
 14333/50000: episode: 319, duration: 0.734s, episode steps: 143, steps per second: 195, episode reward: 143.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.413 [-2.404, 0.760], loss: 5.540647, mean_absolute_error: 25.819747, mean_q: 51.676082
 14469/50000: episode: 320, duration: 0.689s, episode steps: 136, steps per second: 197, episode reward: 136.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.444 [-2.421, 0.859], loss: 6.631186, mean_absolute_error: 25.334384, mean_q: 50.699868
 14488/50000: episode: 321, duration: 0.096s, episode steps: 19, steps per second: 199, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.069 [-1.088, 0.630], loss: 80.654219, mean_absolute_error: 31.737675, mean_q: 61.103940
 14599/50000: episode: 322, duration: 0.574s, episode steps: 111, steps per second: 194, episode reward: 111.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.372 [-1.942, 1.052], loss: 9.067487, mean_absolute_error: 23.805746, mean_q: 47.167969
 14867/50000: episode: 323, duration: 1.355s, episode steps: 268, steps per second: 198, episode reward: 268.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.367 [-2.405, 1.095], loss: 3.800612, mean_absolute_error: 25.671048, mean_q: 51.343161
 15070/50000: episode: 324, duration: 1.031s, episode steps: 203, steps per second: 197, episode reward: 203.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.322 [-2.409, 1.117], loss: 5.429200, mean_absolute_error: 28.333355, mean_q: 56.678314
 15291/50000: episode: 325, duration: 1.124s, episode steps: 221, steps per second: 197, episode reward: 221.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.245 [-2.426, 1.102], loss: 3.616990, mean_absolute_error: 30.306441, mean_q: 60.616921
 15407/50000: episode: 326, duration: 0.628s, episode steps: 116, steps per second: 185, episode reward: 116.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.468 [-2.402, 0.949], loss: 4.371313, mean_absolute_error: 26.426461, mean_q: 52.942038
 15555/50000: episode: 327, duration: 0.757s, episode steps: 148, steps per second: 195, episode reward: 148.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.370 [-2.421, 1.033], loss: 2.910673, mean_absolute_error: 27.560878, mean_q: 55.202097
 15676/50000: episode: 328, duration: 0.612s, episode steps: 121, steps per second: 198, episode reward: 121.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: -0.447 [-2.782, 1.121], loss: 2.463847, mean_absolute_error: 25.648991, mean_q: 51.284101
 15800/50000: episode: 329, duration: 0.636s, episode steps: 124, steps per second: 195, episode reward: 124.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.457 [-2.407, 0.941], loss: 5.993528, mean_absolute_error: 24.303884, mean_q: 48.414350
 15966/50000: episode: 330, duration: 0.847s, episode steps: 166, steps per second: 196, episode reward: 166.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.383 [-2.416, 1.109], loss: 2.375558, mean_absolute_error: 26.122391, mean_q: 52.307468
 16083/50000: episode: 331, duration: 0.592s, episode steps: 117, steps per second: 198, episode reward: 117.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.453 [0.000, 1.000], mean observation: -0.475 [-2.415, 0.858], loss: 1.662098, mean_absolute_error: 24.275615, mean_q: 48.701476
 16200/50000: episode: 332, duration: 0.601s, episode steps: 117, steps per second: 195, episode reward: 117.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.461 [-2.607, 0.886], loss: 1.241507, mean_absolute_error: 24.124635, mean_q: 48.415448
 16306/50000: episode: 333, duration: 0.537s, episode steps: 106, steps per second: 197, episode reward: 106.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.443 [0.000, 1.000], mean observation: -0.485 [-2.771, 0.786], loss: 0.909235, mean_absolute_error: 22.686248, mean_q: 45.567480
 16418/50000: episode: 334, duration: 0.577s, episode steps: 112, steps per second: 194, episode reward: 112.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.446 [0.000, 1.000], mean observation: -0.505 [-2.426, 0.665], loss: 0.554956, mean_absolute_error: 21.463090, mean_q: 43.080422
 16524/50000: episode: 335, duration: 0.548s, episode steps: 106, steps per second: 193, episode reward: 106.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.453 [0.000, 1.000], mean observation: -0.485 [-2.423, 1.208], loss: 1.673599, mean_absolute_error: 20.759078, mean_q: 41.511751
 16642/50000: episode: 336, duration: 0.595s, episode steps: 118, steps per second: 198, episode reward: 118.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.441 [0.000, 1.000], mean observation: -0.487 [-2.789, 0.731], loss: 0.462717, mean_absolute_error: 21.457509, mean_q: 43.151914
 16785/50000: episode: 337, duration: 0.736s, episode steps: 143, steps per second: 194, episode reward: 143.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.435 [-2.424, 1.155], loss: 0.465530, mean_absolute_error: 22.457430, mean_q: 45.065469
 16910/50000: episode: 338, duration: 0.635s, episode steps: 125, steps per second: 197, episode reward: 125.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: -0.465 [-2.426, 0.817], loss: 0.505222, mean_absolute_error: 22.364036, mean_q: 44.939964
 17037/50000: episode: 339, duration: 0.661s, episode steps: 127, steps per second: 192, episode reward: 127.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.425 [-2.777, 1.118], loss: 0.565568, mean_absolute_error: 23.432382, mean_q: 47.049172
 17101/50000: episode: 340, duration: 0.327s, episode steps: 64, steps per second: 196, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.059 [-0.995, 0.812], loss: 20.869780, mean_absolute_error: 33.703181, mean_q: 66.757748
 17355/50000: episode: 341, duration: 1.328s, episode steps: 254, steps per second: 191, episode reward: 254.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.151 [-1.676, 1.340], loss: 4.557832, mean_absolute_error: 28.623053, mean_q: 57.032796
 17373/50000: episode: 342, duration: 0.092s, episode steps: 18, steps per second: 197, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: 0.088 [-0.610, 1.126], loss: 124.422411, mean_absolute_error: 35.605295, mean_q: 65.620346
 17451/50000: episode: 343, duration: 0.394s, episode steps: 78, steps per second: 198, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.032 [-1.016, 0.783], loss: 12.861885, mean_absolute_error: 30.124598, mean_q: 59.490197
 17622/50000: episode: 344, duration: 0.874s, episode steps: 171, steps per second: 196, episode reward: 171.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.190 [-1.381, 1.315], loss: 2.711716, mean_absolute_error: 24.107831, mean_q: 48.215931
 17757/50000: episode: 345, duration: 0.697s, episode steps: 135, steps per second: 194, episode reward: 135.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.335 [-2.359, 0.623], loss: 0.486012, mean_absolute_error: 20.453268, mean_q: 41.234800
 17915/50000: episode: 346, duration: 0.806s, episode steps: 158, steps per second: 196, episode reward: 158.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.377 [-2.431, 1.133], loss: 0.406832, mean_absolute_error: 20.552660, mean_q: 41.353991
 18134/50000: episode: 347, duration: 1.112s, episode steps: 219, steps per second: 197, episode reward: 219.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.397 [-2.412, 0.962], loss: 1.341185, mean_absolute_error: 23.299952, mean_q: 46.567918
 18345/50000: episode: 348, duration: 1.075s, episode steps: 211, steps per second: 196, episode reward: 211.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.383 [-2.408, 1.052], loss: 0.975015, mean_absolute_error: 26.019670, mean_q: 52.122734
 18585/50000: episode: 349, duration: 1.348s, episode steps: 240, steps per second: 178, episode reward: 240.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.360 [-2.400, 1.089], loss: 3.315380, mean_absolute_error: 29.001612, mean_q: 57.950298
 19043/50000: episode: 350, duration: 2.711s, episode steps: 458, steps per second: 169, episode reward: 458.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: -0.169 [-2.558, 2.441], loss: 9.022722, mean_absolute_error: 39.413638, mean_q: 78.597223
 19061/50000: episode: 351, duration: 0.107s, episode steps: 18, steps per second: 168, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.086 [-1.436, 0.965], loss: 135.849855, mean_absolute_error: 42.082933, mean_q: 78.295357
 19097/50000: episode: 352, duration: 0.220s, episode steps: 36, steps per second: 164, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.045 [-1.151, 1.261], loss: 84.929625, mean_absolute_error: 38.105098, mean_q: 72.659828
 19106/50000: episode: 353, duration: 0.053s, episode steps: 9, steps per second: 169, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.150 [-1.400, 2.338], loss: 325.437313, mean_absolute_error: 41.216710, mean_q: 75.047310
 19115/50000: episode: 354, duration: 0.054s, episode steps: 9, steps per second: 168, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.136 [-1.329, 2.272], loss: 247.121970, mean_absolute_error: 36.880803, mean_q: 67.804426
 19127/50000: episode: 355, duration: 0.074s, episode steps: 12, steps per second: 162, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.100 [-1.194, 1.782], loss: 130.668938, mean_absolute_error: 31.580763, mean_q: 59.445213
 19140/50000: episode: 356, duration: 0.084s, episode steps: 13, steps per second: 156, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.077 [-1.153, 1.749], loss: 97.412608, mean_absolute_error: 28.845128, mean_q: 53.962729
 19153/50000: episode: 357, duration: 0.079s, episode steps: 13, steps per second: 165, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.103 [-0.743, 1.300], loss: 98.258586, mean_absolute_error: 28.255119, mean_q: 52.267099
 19165/50000: episode: 358, duration: 0.074s, episode steps: 12, steps per second: 162, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.095 [-1.025, 1.720], loss: 82.856890, mean_absolute_error: 25.862456, mean_q: 47.893622
 19176/50000: episode: 359, duration: 0.071s, episode steps: 11, steps per second: 155, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.129 [-1.150, 1.904], loss: 75.595760, mean_absolute_error: 24.642997, mean_q: 45.488785
 19191/50000: episode: 360, duration: 0.101s, episode steps: 15, steps per second: 148, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.079 [-1.126, 1.743], loss: 48.693496, mean_absolute_error: 22.976022, mean_q: 43.028008
 19203/50000: episode: 361, duration: 0.074s, episode steps: 12, steps per second: 163, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.105 [-1.010, 1.644], loss: 58.378815, mean_absolute_error: 22.522094, mean_q: 41.281799
 19217/50000: episode: 362, duration: 0.093s, episode steps: 14, steps per second: 150, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.080 [-0.828, 1.415], loss: 44.065942, mean_absolute_error: 21.860084, mean_q: 40.115242
 19229/50000: episode: 363, duration: 0.090s, episode steps: 12, steps per second: 133, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.125 [-0.817, 1.472], loss: 49.330063, mean_absolute_error: 22.054032, mean_q: 40.095268
 19241/50000: episode: 364, duration: 0.071s, episode steps: 12, steps per second: 168, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.106 [-0.946, 1.684], loss: 42.399244, mean_absolute_error: 20.902168, mean_q: 37.851753
 19260/50000: episode: 365, duration: 0.119s, episode steps: 19, steps per second: 160, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.103 [-0.735, 1.156], loss: 41.271052, mean_absolute_error: 20.835320, mean_q: 38.341690
 19277/50000: episode: 366, duration: 0.104s, episode steps: 17, steps per second: 163, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.087 [-0.770, 1.247], loss: 31.750298, mean_absolute_error: 20.172330, mean_q: 36.876786
 19293/50000: episode: 367, duration: 0.100s, episode steps: 16, steps per second: 160, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.100 [-0.555, 1.204], loss: 39.273483, mean_absolute_error: 20.555045, mean_q: 37.407545
 19308/50000: episode: 368, duration: 0.093s, episode steps: 15, steps per second: 161, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.083 [-0.786, 1.273], loss: 32.617651, mean_absolute_error: 18.383617, mean_q: 33.636732
 19332/50000: episode: 369, duration: 0.150s, episode steps: 24, steps per second: 160, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.109 [-0.556, 1.024], loss: 29.779845, mean_absolute_error: 20.368772, mean_q: 37.371528
 19434/50000: episode: 370, duration: 0.633s, episode steps: 102, steps per second: 161, episode reward: 102.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.539 [0.000, 1.000], mean observation: 0.496 [-1.093, 2.561], loss: 16.775047, mean_absolute_error: 26.377403, mean_q: 51.448592
 19475/50000: episode: 371, duration: 0.249s, episode steps: 41, steps per second: 164, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.179 [-1.096, 0.601], loss: 2.392325, mean_absolute_error: 12.863519, mean_q: 24.868995
 19571/50000: episode: 372, duration: 0.589s, episode steps: 96, steps per second: 163, episode reward: 96.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: -0.393 [-2.200, 0.791], loss: 0.824555, mean_absolute_error: 9.077445, mean_q: 17.908004
 19767/50000: episode: 373, duration: 1.167s, episode steps: 196, steps per second: 168, episode reward: 196.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.358 [-2.414, 0.743], loss: 0.657805, mean_absolute_error: 12.142919, mean_q: 24.270389
 19972/50000: episode: 374, duration: 1.200s, episode steps: 205, steps per second: 171, episode reward: 205.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.350 [-2.406, 0.661], loss: 0.425138, mean_absolute_error: 14.521887, mean_q: 29.211253
 20135/50000: episode: 375, duration: 0.920s, episode steps: 163, steps per second: 177, episode reward: 163.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.140 [-1.602, 0.727], loss: 1.504956, mean_absolute_error: 20.257023, mean_q: 40.666655
 20284/50000: episode: 376, duration: 0.768s, episode steps: 149, steps per second: 194, episode reward: 149.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.397 [-2.406, 0.592], loss: 0.385657, mean_absolute_error: 16.734633, mean_q: 33.699093
 20449/50000: episode: 377, duration: 0.843s, episode steps: 165, steps per second: 196, episode reward: 165.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.377 [-2.434, 0.766], loss: 0.575666, mean_absolute_error: 19.006023, mean_q: 38.220963
 20613/50000: episode: 378, duration: 0.826s, episode steps: 164, steps per second: 199, episode reward: 164.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.347 [-2.441, 0.895], loss: 0.664092, mean_absolute_error: 21.036724, mean_q: 42.197672
 20744/50000: episode: 379, duration: 0.675s, episode steps: 131, steps per second: 194, episode reward: 131.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.425 [-2.745, 0.655], loss: 0.636535, mean_absolute_error: 20.618747, mean_q: 41.338585
 20898/50000: episode: 380, duration: 0.795s, episode steps: 154, steps per second: 194, episode reward: 154.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.378 [-2.418, 0.750], loss: 0.970793, mean_absolute_error: 22.678010, mean_q: 45.602591
 21022/50000: episode: 381, duration: 0.627s, episode steps: 124, steps per second: 198, episode reward: 124.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.436 [-2.788, 0.651], loss: 0.488433, mean_absolute_error: 21.554149, mean_q: 43.314040
 21134/50000: episode: 382, duration: 0.600s, episode steps: 112, steps per second: 187, episode reward: 112.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.420 [0.000, 1.000], mean observation: -0.494 [-3.322, 0.674], loss: 0.360281, mean_absolute_error: 20.365046, mean_q: 41.062978
 21244/50000: episode: 383, duration: 0.566s, episode steps: 110, steps per second: 194, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.436 [0.000, 1.000], mean observation: -0.483 [-2.579, 0.728], loss: 0.662738, mean_absolute_error: 20.869898, mean_q: 41.917271
 21347/50000: episode: 384, duration: 0.518s, episode steps: 103, steps per second: 199, episode reward: 103.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: -0.324 [-2.065, 0.534], loss: 2.130593, mean_absolute_error: 24.677295, mean_q: 49.528409
 21451/50000: episode: 385, duration: 0.540s, episode steps: 104, steps per second: 193, episode reward: 104.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.453 [-2.191, 0.933], loss: 0.373143, mean_absolute_error: 19.591682, mean_q: 39.462739
 21587/50000: episode: 386, duration: 0.689s, episode steps: 136, steps per second: 197, episode reward: 136.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.410 [-2.429, 0.618], loss: 0.459149, mean_absolute_error: 20.807672, mean_q: 41.728726
 21717/50000: episode: 387, duration: 0.672s, episode steps: 130, steps per second: 194, episode reward: 130.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.454 [0.000, 1.000], mean observation: -0.427 [-2.567, 0.626], loss: 0.393319, mean_absolute_error: 21.348416, mean_q: 42.922921
 21856/50000: episode: 388, duration: 0.712s, episode steps: 139, steps per second: 195, episode reward: 139.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.446 [0.000, 1.000], mean observation: -0.395 [-3.014, 1.238], loss: 0.569642, mean_absolute_error: 23.019677, mean_q: 46.231033
 21978/50000: episode: 389, duration: 0.612s, episode steps: 122, steps per second: 199, episode reward: 122.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.451 [0.000, 1.000], mean observation: -0.477 [-2.435, 0.740], loss: 0.330096, mean_absolute_error: 20.793521, mean_q: 41.786063
 22095/50000: episode: 390, duration: 0.604s, episode steps: 117, steps per second: 194, episode reward: 117.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.453 [0.000, 1.000], mean observation: -0.479 [-2.620, 0.828], loss: 0.415380, mean_absolute_error: 20.503759, mean_q: 41.290566
 22236/50000: episode: 391, duration: 0.718s, episode steps: 141, steps per second: 196, episode reward: 141.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.436 [-2.419, 0.700], loss: 0.720381, mean_absolute_error: 22.509756, mean_q: 45.164248
 22357/50000: episode: 392, duration: 0.615s, episode steps: 121, steps per second: 197, episode reward: 121.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.447 [-2.520, 0.854], loss: 0.389595, mean_absolute_error: 21.650504, mean_q: 43.574419
 22498/50000: episode: 393, duration: 0.724s, episode steps: 141, steps per second: 195, episode reward: 141.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.402 [-2.420, 1.001], loss: 0.556162, mean_absolute_error: 22.491100, mean_q: 45.111513
 22665/50000: episode: 394, duration: 0.852s, episode steps: 167, steps per second: 196, episode reward: 167.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.421 [-2.413, 1.071], loss: 3.089063, mean_absolute_error: 23.655902, mean_q: 47.234183
 22902/50000: episode: 395, duration: 1.200s, episode steps: 237, steps per second: 197, episode reward: 237.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.366 [-2.406, 1.124], loss: 4.697004, mean_absolute_error: 28.131422, mean_q: 56.156264
 23134/50000: episode: 396, duration: 1.195s, episode steps: 232, steps per second: 194, episode reward: 232.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.391 [-2.420, 1.124], loss: 3.542736, mean_absolute_error: 28.852666, mean_q: 57.651080
 23426/50000: episode: 397, duration: 1.485s, episode steps: 292, steps per second: 197, episode reward: 292.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.400 [-2.416, 0.932], loss: 2.083598, mean_absolute_error: 30.362684, mean_q: 60.773901
 23670/50000: episode: 398, duration: 1.239s, episode steps: 244, steps per second: 197, episode reward: 244.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.390 [-2.418, 1.159], loss: 3.378861, mean_absolute_error: 31.276090, mean_q: 62.557701
 23680/50000: episode: 399, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.121 [-1.013, 1.536], loss: 346.913896, mean_absolute_error: 46.325600, mean_q: 82.076098
 23712/50000: episode: 400, duration: 0.163s, episode steps: 32, steps per second: 197, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: 0.051 [-1.876, 1.739], loss: 96.048038, mean_absolute_error: 41.754573, mean_q: 80.620620
 23737/50000: episode: 401, duration: 0.125s, episode steps: 25, steps per second: 201, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.121 [-1.690, 1.123], loss: 42.902590, mean_absolute_error: 28.853349, mean_q: 55.739040
 23971/50000: episode: 402, duration: 1.184s, episode steps: 234, steps per second: 198, episode reward: 234.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.278 [-0.929, 2.419], loss: 20.630144, mean_absolute_error: 34.772681, mean_q: 69.295375
 24228/50000: episode: 403, duration: 1.297s, episode steps: 257, steps per second: 198, episode reward: 257.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.137 [-0.862, 1.845], loss: 15.419969, mean_absolute_error: 31.687613, mean_q: 62.948766
 24402/50000: episode: 404, duration: 0.885s, episode steps: 174, steps per second: 197, episode reward: 174.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.248 [-0.892, 1.817], loss: 17.775280, mean_absolute_error: 32.401069, mean_q: 64.464035
 24529/50000: episode: 405, duration: 0.646s, episode steps: 127, steps per second: 196, episode reward: 127.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.233 [-0.670, 1.280], loss: 20.235879, mean_absolute_error: 31.753335, mean_q: 62.899330
 24660/50000: episode: 406, duration: 0.671s, episode steps: 131, steps per second: 195, episode reward: 131.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.534 [0.000, 1.000], mean observation: 0.270 [-0.752, 1.650], loss: 19.537729, mean_absolute_error: 31.114791, mean_q: 61.751872
 24683/50000: episode: 407, duration: 0.116s, episode steps: 23, steps per second: 198, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.032 [-1.013, 1.541], loss: 60.368096, mean_absolute_error: 28.757252, mean_q: 54.825017
 24702/50000: episode: 408, duration: 0.101s, episode steps: 19, steps per second: 189, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.073 [-0.792, 1.161], loss: 72.210168, mean_absolute_error: 27.372789, mean_q: 51.645119
 24714/50000: episode: 409, duration: 0.063s, episode steps: 12, steps per second: 192, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.101 [-0.815, 1.375], loss: 80.002854, mean_absolute_error: 26.187254, mean_q: 48.221253
 24726/50000: episode: 410, duration: 0.063s, episode steps: 12, steps per second: 190, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.087 [-1.199, 1.766], loss: 67.085529, mean_absolute_error: 23.818871, mean_q: 43.673671
 24741/50000: episode: 411, duration: 0.077s, episode steps: 15, steps per second: 194, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.095 [-0.739, 1.209], loss: 54.004070, mean_absolute_error: 23.514066, mean_q: 43.324450
 24752/50000: episode: 412, duration: 0.057s, episode steps: 11, steps per second: 193, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.129 [-1.153, 1.982], loss: 43.795457, mean_absolute_error: 22.453625, mean_q: 40.577450
 24767/50000: episode: 413, duration: 0.079s, episode steps: 15, steps per second: 191, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.112 [-0.574, 1.207], loss: 44.136004, mean_absolute_error: 23.026042, mean_q: 41.817589
 24779/50000: episode: 414, duration: 0.075s, episode steps: 12, steps per second: 160, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.128 [-0.742, 1.305], loss: 64.245516, mean_absolute_error: 21.756126, mean_q: 38.695258
 24801/50000: episode: 415, duration: 0.113s, episode steps: 22, steps per second: 194, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.090 [-0.623, 1.421], loss: 26.384623, mean_absolute_error: 21.677165, mean_q: 39.630561
 24826/50000: episode: 416, duration: 0.140s, episode steps: 25, steps per second: 178, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.085 [-0.564, 0.859], loss: 26.919720, mean_absolute_error: 20.763278, mean_q: 38.878431
 24857/50000: episode: 417, duration: 0.162s, episode steps: 31, steps per second: 191, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: 0.152 [-0.567, 0.898], loss: 32.201316, mean_absolute_error: 22.061905, mean_q: 41.082762
 25226/50000: episode: 418, duration: 1.892s, episode steps: 369, steps per second: 195, episode reward: 369.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: 0.192 [-1.603, 1.554], loss: 1.746702, mean_absolute_error: 20.717267, mean_q: 41.368380
 25240/50000: episode: 419, duration: 0.070s, episode steps: 14, steps per second: 200, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.071 [-1.725, 1.217], loss: 47.161444, mean_absolute_error: 23.577098, mean_q: 45.394451
 25259/50000: episode: 420, duration: 0.096s, episode steps: 19, steps per second: 198, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.113 [-1.441, 0.943], loss: 28.253849, mean_absolute_error: 19.870000, mean_q: 37.771188
 25354/50000: episode: 421, duration: 0.482s, episode steps: 95, steps per second: 197, episode reward: 95.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.442 [0.000, 1.000], mean observation: -0.343 [-1.996, 0.811], loss: 2.095564, mean_absolute_error: 15.105962, mean_q: 29.606718
 25694/50000: episode: 422, duration: 1.719s, episode steps: 340, steps per second: 198, episode reward: 340.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.317 [-2.408, 1.086], loss: 1.600857, mean_absolute_error: 18.817710, mean_q: 37.780181
 25896/50000: episode: 423, duration: 1.027s, episode steps: 202, steps per second: 197, episode reward: 202.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.342 [-2.419, 0.780], loss: 1.371149, mean_absolute_error: 22.495983, mean_q: 45.354305
 26147/50000: episode: 424, duration: 1.273s, episode steps: 251, steps per second: 197, episode reward: 251.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.258 [-2.586, 0.799], loss: 1.241350, mean_absolute_error: 26.018415, mean_q: 52.360767
 26283/50000: episode: 425, duration: 0.697s, episode steps: 136, steps per second: 195, episode reward: 136.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.456 [0.000, 1.000], mean observation: -0.440 [-2.573, 0.530], loss: 0.950831, mean_absolute_error: 23.588837, mean_q: 47.486246
 26389/50000: episode: 426, duration: 0.536s, episode steps: 106, steps per second: 198, episode reward: 106.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.443 [0.000, 1.000], mean observation: -0.525 [-2.589, 0.597], loss: 0.611004, mean_absolute_error: 20.714371, mean_q: 41.856320
 26517/50000: episode: 427, duration: 0.658s, episode steps: 128, steps per second: 195, episode reward: 128.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.453 [0.000, 1.000], mean observation: -0.413 [-2.348, 0.554], loss: 0.551460, mean_absolute_error: 23.382792, mean_q: 47.203013
 26686/50000: episode: 428, duration: 0.860s, episode steps: 169, steps per second: 196, episode reward: 169.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.364 [-2.403, 0.780], loss: 0.738579, mean_absolute_error: 24.407664, mean_q: 49.151375
 26854/50000: episode: 429, duration: 0.869s, episode steps: 168, steps per second: 193, episode reward: 168.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.333 [-2.433, 0.976], loss: 0.672155, mean_absolute_error: 25.405826, mean_q: 51.105045
 26999/50000: episode: 430, duration: 0.735s, episode steps: 145, steps per second: 197, episode reward: 145.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.410 [-2.411, 0.737], loss: 0.712762, mean_absolute_error: 24.637375, mean_q: 49.673585
 27130/50000: episode: 431, duration: 0.670s, episode steps: 131, steps per second: 195, episode reward: 131.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.458 [-2.387, 0.654], loss: 0.495792, mean_absolute_error: 22.779786, mean_q: 45.902482
 27273/50000: episode: 432, duration: 0.754s, episode steps: 143, steps per second: 190, episode reward: 143.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.393 [-2.323, 0.763], loss: 0.373553, mean_absolute_error: 24.107954, mean_q: 48.608533
 27416/50000: episode: 433, duration: 0.720s, episode steps: 143, steps per second: 199, episode reward: 143.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.440 [-2.406, 0.819], loss: 0.345650, mean_absolute_error: 22.087977, mean_q: 44.507652
 27592/50000: episode: 434, duration: 0.898s, episode steps: 176, steps per second: 196, episode reward: 176.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.395 [-2.406, 0.826], loss: 0.517674, mean_absolute_error: 25.099653, mean_q: 50.440316
 27784/50000: episode: 435, duration: 0.979s, episode steps: 192, steps per second: 196, episode reward: 192.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.181 [-1.669, 1.584], loss: 4.009737, mean_absolute_error: 33.441466, mean_q: 66.844312
 27905/50000: episode: 436, duration: 0.629s, episode steps: 121, steps per second: 192, episode reward: 121.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.408 [-2.393, 0.784], loss: 1.204369, mean_absolute_error: 23.325580, mean_q: 46.626402
 27995/50000: episode: 437, duration: 0.457s, episode steps: 90, steps per second: 197, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.456 [0.000, 1.000], mean observation: -0.370 [-1.819, 0.658], loss: 0.361642, mean_absolute_error: 22.360756, mean_q: 45.103197
 28129/50000: episode: 438, duration: 0.691s, episode steps: 134, steps per second: 194, episode reward: 134.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.316 [-1.627, 0.799], loss: 0.292904, mean_absolute_error: 23.149300, mean_q: 46.648576
 28257/50000: episode: 439, duration: 0.663s, episode steps: 128, steps per second: 193, episode reward: 128.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.337 [-2.007, 0.883], loss: 0.363480, mean_absolute_error: 22.732120, mean_q: 45.728983
 28398/50000: episode: 440, duration: 0.709s, episode steps: 141, steps per second: 199, episode reward: 141.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.315 [-1.645, 0.759], loss: 0.430855, mean_absolute_error: 23.843166, mean_q: 47.899494
 28579/50000: episode: 441, duration: 0.922s, episode steps: 181, steps per second: 196, episode reward: 181.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.285 [-1.782, 0.856], loss: 0.455206, mean_absolute_error: 25.432057, mean_q: 51.119636
 28926/50000: episode: 442, duration: 1.768s, episode steps: 347, steps per second: 196, episode reward: 347.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.496 [0.000, 1.000], mean observation: -0.348 [-2.268, 1.053], loss: 1.086810, mean_absolute_error: 29.167233, mean_q: 58.488864
 29279/50000: episode: 443, duration: 1.807s, episode steps: 353, steps per second: 195, episode reward: 353.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.162 [-0.991, 1.959], loss: 25.420835, mean_absolute_error: 49.620126, mean_q: 99.050345
 29399/50000: episode: 444, duration: 0.608s, episode steps: 120, steps per second: 198, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.259 [-0.640, 2.218], loss: 43.732780, mean_absolute_error: 44.524256, mean_q: 88.059341
 29513/50000: episode: 445, duration: 0.588s, episode steps: 114, steps per second: 194, episode reward: 114.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.544 [0.000, 1.000], mean observation: 0.316 [-0.792, 1.786], loss: 31.158628, mean_absolute_error: 40.473628, mean_q: 80.407085
 29634/50000: episode: 446, duration: 0.626s, episode steps: 121, steps per second: 193, episode reward: 121.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.170 [-0.699, 1.670], loss: 25.003520, mean_absolute_error: 37.271939, mean_q: 74.132607
 29712/50000: episode: 447, duration: 0.398s, episode steps: 78, steps per second: 196, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.108 [-0.564, 1.113], loss: 32.031043, mean_absolute_error: 35.466669, mean_q: 70.152312
 29793/50000: episode: 448, duration: 0.410s, episode steps: 81, steps per second: 197, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: 0.243 [-0.469, 1.626], loss: 27.473724, mean_absolute_error: 34.078970, mean_q: 67.572429
 29882/50000: episode: 449, duration: 0.460s, episode steps: 89, steps per second: 193, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.539 [0.000, 1.000], mean observation: 0.286 [-0.346, 1.878], loss: 21.625134, mean_absolute_error: 32.553542, mean_q: 64.623423
 29908/50000: episode: 450, duration: 0.132s, episode steps: 26, steps per second: 198, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.081 [-0.562, 0.939], loss: 70.604027, mean_absolute_error: 32.384425, mean_q: 62.415513
 29923/50000: episode: 451, duration: 0.080s, episode steps: 15, steps per second: 188, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.097 [-0.958, 1.612], loss: 75.811647, mean_absolute_error: 30.274108, mean_q: 56.540566
 29933/50000: episode: 452, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.110 [-1.205, 2.019], loss: 105.121930, mean_absolute_error: 29.266254, mean_q: 52.740670
 29945/50000: episode: 453, duration: 0.063s, episode steps: 12, steps per second: 191, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.099 [-0.756, 1.344], loss: 71.075416, mean_absolute_error: 27.359759, mean_q: 49.755993
 29959/50000: episode: 454, duration: 0.071s, episode steps: 14, steps per second: 198, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.090 [-0.646, 1.226], loss: 64.890844, mean_absolute_error: 26.460279, mean_q: 47.630545
 29971/50000: episode: 455, duration: 0.065s, episode steps: 12, steps per second: 185, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.122 [-0.787, 1.322], loss: 76.907798, mean_absolute_error: 24.037062, mean_q: 42.181598
 29984/50000: episode: 456, duration: 0.066s, episode steps: 13, steps per second: 196, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.109 [-0.988, 1.448], loss: 52.178526, mean_absolute_error: 22.501796, mean_q: 39.755417
 29999/50000: episode: 457, duration: 0.075s, episode steps: 15, steps per second: 199, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.101 [-0.621, 1.092], loss: 54.384450, mean_absolute_error: 23.728414, mean_q: 41.838353
 30033/50000: episode: 458, duration: 0.184s, episode steps: 34, steps per second: 185, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.135 [-0.437, 1.008], loss: 28.072681, mean_absolute_error: 24.171294, mean_q: 44.522352
 30175/50000: episode: 459, duration: 0.716s, episode steps: 142, steps per second: 198, episode reward: 142.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.011 [-1.321, 0.734], loss: 4.188694, mean_absolute_error: 22.292843, mean_q: 43.976603
 30220/50000: episode: 460, duration: 0.234s, episode steps: 45, steps per second: 192, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.422 [0.000, 1.000], mean observation: -0.228 [-1.322, 0.752], loss: 5.225630, mean_absolute_error: 17.500557, mean_q: 34.335388
 30302/50000: episode: 461, duration: 0.428s, episode steps: 82, steps per second: 192, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.451 [0.000, 1.000], mean observation: -0.383 [-1.876, 0.775], loss: 0.291780, mean_absolute_error: 13.530873, mean_q: 27.260100
 30438/50000: episode: 462, duration: 0.697s, episode steps: 136, steps per second: 195, episode reward: 136.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.313 [-2.563, 0.811], loss: 0.356997, mean_absolute_error: 16.650304, mean_q: 33.427435
 30520/50000: episode: 463, duration: 0.420s, episode steps: 82, steps per second: 195, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.267 [-1.492, 0.500], loss: 1.980851, mean_absolute_error: 19.421683, mean_q: 38.857054
 30654/50000: episode: 464, duration: 0.691s, episode steps: 134, steps per second: 194, episode reward: 134.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.397 [-2.136, 0.912], loss: 0.570184, mean_absolute_error: 15.804103, mean_q: 31.775726
 30799/50000: episode: 465, duration: 0.740s, episode steps: 145, steps per second: 196, episode reward: 145.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.318 [-1.829, 0.668], loss: 0.499442, mean_absolute_error: 19.814509, mean_q: 39.830603
 30929/50000: episode: 466, duration: 0.668s, episode steps: 130, steps per second: 195, episode reward: 130.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.434 [-2.231, 0.574], loss: 0.443188, mean_absolute_error: 18.171275, mean_q: 36.553106
 31056/50000: episode: 467, duration: 0.655s, episode steps: 127, steps per second: 194, episode reward: 127.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.306 [-2.149, 0.619], loss: 0.499426, mean_absolute_error: 22.001585, mean_q: 44.204387
 31136/50000: episode: 468, duration: 0.407s, episode steps: 80, steps per second: 196, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.356 [-1.996, 0.716], loss: 1.209281, mean_absolute_error: 20.341997, mean_q: 41.044756
 31283/50000: episode: 469, duration: 0.774s, episode steps: 147, steps per second: 190, episode reward: 147.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.349 [-2.027, 1.083], loss: 0.333393, mean_absolute_error: 20.095113, mean_q: 40.447062
 31420/50000: episode: 470, duration: 0.716s, episode steps: 137, steps per second: 191, episode reward: 137.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.366 [-1.909, 0.585], loss: 0.420413, mean_absolute_error: 21.466079, mean_q: 43.160928
 31548/50000: episode: 471, duration: 0.662s, episode steps: 128, steps per second: 193, episode reward: 128.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.287 [-2.570, 0.961], loss: 0.770549, mean_absolute_error: 23.242945, mean_q: 46.461810
 31679/50000: episode: 472, duration: 0.679s, episode steps: 131, steps per second: 193, episode reward: 131.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.462 [-2.261, 1.072], loss: 0.459833, mean_absolute_error: 18.660686, mean_q: 37.490924
 31830/50000: episode: 473, duration: 0.774s, episode steps: 151, steps per second: 195, episode reward: 151.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.436 [-2.409, 0.667], loss: 1.643647, mean_absolute_error: 21.648106, mean_q: 43.430904
 32107/50000: episode: 474, duration: 1.458s, episode steps: 277, steps per second: 190, episode reward: 277.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.464 [-2.403, 0.793], loss: 5.157108, mean_absolute_error: 25.272752, mean_q: 50.529458
 32249/50000: episode: 475, duration: 0.883s, episode steps: 142, steps per second: 161, episode reward: 142.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.383 [-2.423, 0.808], loss: 5.120121, mean_absolute_error: 29.024449, mean_q: 57.853349
 32503/50000: episode: 476, duration: 1.587s, episode steps: 254, steps per second: 160, episode reward: 254.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.451 [-2.410, 0.936], loss: 2.626995, mean_absolute_error: 26.955375, mean_q: 54.006472
 32517/50000: episode: 477, duration: 0.103s, episode steps: 14, steps per second: 137, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.085 [-1.483, 0.994], loss: 149.502073, mean_absolute_error: 38.511773, mean_q: 72.918622
 32533/50000: episode: 478, duration: 0.107s, episode steps: 16, steps per second: 149, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: -0.107 [-1.031, 0.586], loss: 76.004744, mean_absolute_error: 30.767857, mean_q: 57.467074
 33033/50000: episode: 479, duration: 3.133s, episode steps: 500, steps per second: 160, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.138 [-1.100, 0.861], loss: 6.417332, mean_absolute_error: 31.693440, mean_q: 63.240244
 33057/50000: episode: 480, duration: 0.149s, episode steps: 24, steps per second: 162, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.118 [-0.357, 0.922], loss: 86.578457, mean_absolute_error: 36.555443, mean_q: 69.195041
 33082/50000: episode: 481, duration: 0.181s, episode steps: 25, steps per second: 138, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.092 [-0.404, 0.997], loss: 55.395489, mean_absolute_error: 32.837503, mean_q: 62.027546
 33129/50000: episode: 482, duration: 0.294s, episode steps: 47, steps per second: 160, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.553 [0.000, 1.000], mean observation: 0.166 [-0.431, 1.066], loss: 31.256577, mean_absolute_error: 29.675536, mean_q: 57.546165
 33629/50000: episode: 483, duration: 3.103s, episode steps: 500, steps per second: 161, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.498 [0.000, 1.000], mean observation: -0.182 [-0.930, 0.719], loss: 5.334565, mean_absolute_error: 30.586928, mean_q: 61.490366
 33757/50000: episode: 484, duration: 0.781s, episode steps: 128, steps per second: 164, episode reward: 128.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.539 [0.000, 1.000], mean observation: 0.393 [-0.602, 1.956], loss: 9.671736, mean_absolute_error: 30.416167, mean_q: 60.314345
 34257/50000: episode: 485, duration: 2.933s, episode steps: 500, steps per second: 170, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.502 [0.000, 1.000], mean observation: -0.178 [-1.460, 1.127], loss: 5.388931, mean_absolute_error: 35.776565, mean_q: 71.715189
 34442/50000: episode: 486, duration: 1.089s, episode steps: 185, steps per second: 170, episode reward: 185.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.115 [-0.927, 1.629], loss: 7.893290, mean_absolute_error: 35.056961, mean_q: 69.989901
 34481/50000: episode: 487, duration: 0.232s, episode steps: 39, steps per second: 168, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.564 [0.000, 1.000], mean observation: 0.164 [-0.389, 1.111], loss: 40.484943, mean_absolute_error: 34.088655, mean_q: 66.478749
 34573/50000: episode: 488, duration: 0.546s, episode steps: 92, steps per second: 169, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.141 [-0.449, 1.107], loss: 13.010734, mean_absolute_error: 31.251251, mean_q: 62.304555
 34654/50000: episode: 489, duration: 0.480s, episode steps: 81, steps per second: 169, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.189 [-0.488, 1.132], loss: 10.192977, mean_absolute_error: 28.710746, mean_q: 57.321478
 34684/50000: episode: 490, duration: 0.175s, episode steps: 30, steps per second: 171, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.053 [-0.427, 1.000], loss: 46.520109, mean_absolute_error: 31.540947, mean_q: 61.334996
 34774/50000: episode: 491, duration: 0.536s, episode steps: 90, steps per second: 168, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: 0.268 [-0.592, 1.800], loss: 3.231428, mean_absolute_error: 23.542786, mean_q: 47.356746
 34870/50000: episode: 492, duration: 0.596s, episode steps: 96, steps per second: 161, episode reward: 96.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.198 [-0.669, 1.264], loss: 4.064359, mean_absolute_error: 24.616485, mean_q: 49.420963
 35018/50000: episode: 493, duration: 0.868s, episode steps: 148, steps per second: 171, episode reward: 148.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.127 [-0.719, 1.506], loss: 3.055739, mean_absolute_error: 26.845194, mean_q: 53.864250
 35101/50000: episode: 494, duration: 0.500s, episode steps: 83, steps per second: 166, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.554 [0.000, 1.000], mean observation: 0.241 [-0.460, 1.672], loss: 3.059517, mean_absolute_error: 23.594614, mean_q: 47.356646
 35207/50000: episode: 495, duration: 0.619s, episode steps: 106, steps per second: 171, episode reward: 106.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.166 [-0.470, 1.413], loss: 2.753163, mean_absolute_error: 25.546709, mean_q: 51.380649
 35281/50000: episode: 496, duration: 0.414s, episode steps: 74, steps per second: 179, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: 0.201 [-0.486, 1.151], loss: 4.376143, mean_absolute_error: 24.196632, mean_q: 48.431463
 35348/50000: episode: 497, duration: 0.339s, episode steps: 67, steps per second: 198, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.170 [-0.932, 1.252], loss: 6.256355, mean_absolute_error: 24.003763, mean_q: 47.946546
 35456/50000: episode: 498, duration: 0.559s, episode steps: 108, steps per second: 193, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.238 [-0.490, 1.789], loss: 0.667029, mean_absolute_error: 21.041331, mean_q: 42.293814
 35956/50000: episode: 499, duration: 2.534s, episode steps: 500, steps per second: 197, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.131 [-1.047, 0.839], loss: 7.320520, mean_absolute_error: 36.619241, mean_q: 73.375081
 35980/50000: episode: 500, duration: 0.121s, episode steps: 24, steps per second: 199, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: 0.068 [-0.610, 1.290], loss: 56.222142, mean_absolute_error: 35.892413, mean_q: 68.627534
 36149/50000: episode: 501, duration: 0.860s, episode steps: 169, steps per second: 196, episode reward: 169.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.497 [0.000, 1.000], mean observation: -0.219 [-1.606, 0.739], loss: 10.722708, mean_absolute_error: 36.169373, mean_q: 72.476257
 36327/50000: episode: 502, duration: 0.910s, episode steps: 178, steps per second: 196, episode reward: 178.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.242 [-2.369, 0.911], loss: 7.292184, mean_absolute_error: 34.943732, mean_q: 69.699080
 36406/50000: episode: 503, duration: 0.403s, episode steps: 79, steps per second: 196, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.430 [0.000, 1.000], mean observation: -0.297 [-2.188, 0.656], loss: 16.575922, mean_absolute_error: 33.310211, mean_q: 66.048845
 36416/50000: episode: 504, duration: 0.063s, episode steps: 10, steps per second: 160, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.130 [-1.736, 0.949], loss: 149.516650, mean_absolute_error: 34.521988, mean_q: 62.659628
 36432/50000: episode: 505, duration: 0.083s, episode steps: 16, steps per second: 192, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.069 [-1.186, 0.808], loss: 58.114193, mean_absolute_error: 27.679051, mean_q: 52.296678
 36510/50000: episode: 506, duration: 0.397s, episode steps: 78, steps per second: 197, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.449 [0.000, 1.000], mean observation: -0.291 [-2.208, 1.221], loss: 11.169937, mean_absolute_error: 22.180210, mean_q: 42.552268
 36584/50000: episode: 507, duration: 0.379s, episode steps: 74, steps per second: 195, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: 0.293 [-0.488, 1.467], loss: 3.054611, mean_absolute_error: 17.940509, mean_q: 35.028253
 37084/50000: episode: 508, duration: 2.556s, episode steps: 500, steps per second: 196, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.502 [0.000, 1.000], mean observation: -0.186 [-1.133, 0.902], loss: 4.192653, mean_absolute_error: 25.908045, mean_q: 52.139363
 37584/50000: episode: 509, duration: 2.527s, episode steps: 500, steps per second: 198, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.502 [0.000, 1.000], mean observation: -0.205 [-1.134, 0.977], loss: 6.026508, mean_absolute_error: 33.686575, mean_q: 67.709418
 38084/50000: episode: 510, duration: 2.539s, episode steps: 500, steps per second: 197, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.215 [-1.236, 1.283], loss: 6.911623, mean_absolute_error: 37.221013, mean_q: 74.622100
 38584/50000: episode: 511, duration: 2.526s, episode steps: 500, steps per second: 198, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.502 [0.000, 1.000], mean observation: -0.218 [-1.282, 0.934], loss: 7.550807, mean_absolute_error: 39.227290, mean_q: 78.678752
 39084/50000: episode: 512, duration: 2.560s, episode steps: 500, steps per second: 195, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.121 [-1.144, 0.986], loss: 7.558611, mean_absolute_error: 42.525973, mean_q: 85.412811
 39584/50000: episode: 513, duration: 2.533s, episode steps: 500, steps per second: 197, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.498 [0.000, 1.000], mean observation: -0.263 [-1.486, 0.981], loss: 5.943124, mean_absolute_error: 37.916233, mean_q: 75.952699
 39603/50000: episode: 514, duration: 0.097s, episode steps: 19, steps per second: 195, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.074 [-1.152, 0.732], loss: 170.789281, mean_absolute_error: 46.444457, mean_q: 88.754861
 39666/50000: episode: 515, duration: 0.333s, episode steps: 63, steps per second: 189, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.229 [-1.856, 1.084], loss: 51.587010, mean_absolute_error: 41.795579, mean_q: 81.229348
 40166/50000: episode: 516, duration: 2.534s, episode steps: 500, steps per second: 197, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.150 [-1.043, 0.934], loss: 4.956744, mean_absolute_error: 34.878797, mean_q: 70.000670
 40270/50000: episode: 517, duration: 0.536s, episode steps: 104, steps per second: 194, episode reward: 104.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.260 [-0.705, 1.417], loss: 3.345198, mean_absolute_error: 28.410077, mean_q: 56.371654
 40770/50000: episode: 518, duration: 2.532s, episode steps: 500, steps per second: 197, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.202 [-1.042, 0.924], loss: 6.530767, mean_absolute_error: 35.846156, mean_q: 71.922222
 41270/50000: episode: 519, duration: 2.564s, episode steps: 500, steps per second: 195, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.498 [0.000, 1.000], mean observation: -0.159 [-1.357, 1.091], loss: 7.794283, mean_absolute_error: 39.518393, mean_q: 79.112336
 41770/50000: episode: 520, duration: 2.534s, episode steps: 500, steps per second: 197, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.498 [0.000, 1.000], mean observation: -0.402 [-2.160, 0.930], loss: 6.195643, mean_absolute_error: 34.111500, mean_q: 68.442517
 42052/50000: episode: 521, duration: 1.443s, episode steps: 282, steps per second: 195, episode reward: 282.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: -0.454 [-2.405, 0.872], loss: 7.135824, mean_absolute_error: 36.222204, mean_q: 72.360706
 42216/50000: episode: 522, duration: 0.825s, episode steps: 164, steps per second: 199, episode reward: 164.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.503 [-2.401, 1.085], loss: 3.753623, mean_absolute_error: 32.761565, mean_q: 65.392511
 42228/50000: episode: 523, duration: 0.062s, episode steps: 12, steps per second: 194, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.107 [-1.289, 0.818], loss: 261.777945, mean_absolute_error: 42.549164, mean_q: 79.611214
 42728/50000: episode: 524, duration: 2.542s, episode steps: 500, steps per second: 197, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.502 [0.000, 1.000], mean observation: -0.265 [-1.275, 1.000], loss: 5.865029, mean_absolute_error: 33.965031, mean_q: 68.141587
 42829/50000: episode: 525, duration: 0.510s, episode steps: 101, steps per second: 198, episode reward: 101.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.195 [-0.448, 1.444], loss: 3.203557, mean_absolute_error: 32.019280, mean_q: 63.960596
 42892/50000: episode: 526, duration: 0.334s, episode steps: 63, steps per second: 188, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.540 [0.000, 1.000], mean observation: 0.165 [-0.393, 0.963], loss: 9.902680, mean_absolute_error: 31.052033, mean_q: 61.696825
 42949/50000: episode: 527, duration: 0.311s, episode steps: 57, steps per second: 184, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.544 [0.000, 1.000], mean observation: 0.195 [-0.490, 1.442], loss: 4.210873, mean_absolute_error: 26.540544, mean_q: 52.786677
 43009/50000: episode: 528, duration: 0.307s, episode steps: 60, steps per second: 195, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.165 [-0.431, 1.055], loss: 3.275655, mean_absolute_error: 25.747732, mean_q: 51.291784
 43088/50000: episode: 529, duration: 0.412s, episode steps: 79, steps per second: 192, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.126 [-0.540, 1.086], loss: 1.854996, mean_absolute_error: 25.781581, mean_q: 51.475781
 43141/50000: episode: 530, duration: 0.270s, episode steps: 53, steps per second: 196, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.158 [-0.405, 1.338], loss: 3.223837, mean_absolute_error: 23.040389, mean_q: 45.926795
 43228/50000: episode: 531, duration: 0.441s, episode steps: 87, steps per second: 198, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.120 [-0.762, 1.168], loss: 1.115018, mean_absolute_error: 23.231596, mean_q: 46.561505
 43354/50000: episode: 532, duration: 0.646s, episode steps: 126, steps per second: 195, episode reward: 126.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.064 [-0.482, 1.095], loss: 1.415563, mean_absolute_error: 26.882050, mean_q: 53.844584
 43404/50000: episode: 533, duration: 0.260s, episode steps: 50, steps per second: 193, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.163 [-0.432, 1.263], loss: 0.842801, mean_absolute_error: 20.609360, mean_q: 41.121937
 43469/50000: episode: 534, duration: 0.344s, episode steps: 65, steps per second: 189, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.153 [-0.824, 1.176], loss: 0.855351, mean_absolute_error: 19.319325, mean_q: 38.632718
 43499/50000: episode: 535, duration: 0.150s, episode steps: 30, steps per second: 200, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.056 [-0.763, 1.403], loss: 22.575854, mean_absolute_error: 26.944432, mean_q: 52.500343
 43602/50000: episode: 536, duration: 0.523s, episode steps: 103, steps per second: 197, episode reward: 103.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.169 [-0.697, 1.452], loss: 0.705218, mean_absolute_error: 16.911081, mean_q: 33.821267
 43696/50000: episode: 537, duration: 0.482s, episode steps: 94, steps per second: 195, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.196 [-0.651, 1.466], loss: 0.495205, mean_absolute_error: 15.732549, mean_q: 31.527834
 43838/50000: episode: 538, duration: 0.729s, episode steps: 142, steps per second: 195, episode reward: 142.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: 0.165 [-2.529, 3.025], loss: 0.487018, mean_absolute_error: 17.720339, mean_q: 35.512371
 43932/50000: episode: 539, duration: 0.483s, episode steps: 94, steps per second: 195, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.197 [-0.587, 1.365], loss: 0.444535, mean_absolute_error: 15.207736, mean_q: 30.385362
 44411/50000: episode: 540, duration: 2.437s, episode steps: 479, steps per second: 197, episode reward: 479.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.501 [0.000, 1.000], mean observation: -0.312 [-1.901, 1.091], loss: 6.520885, mean_absolute_error: 34.036708, mean_q: 68.061836
 44427/50000: episode: 541, duration: 0.080s, episode steps: 16, steps per second: 201, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.087 [-1.186, 0.810], loss: 111.455947, mean_absolute_error: 36.021094, mean_q: 66.582940
 44477/50000: episode: 542, duration: 0.271s, episode steps: 50, steps per second: 184, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.140 [-1.523, 1.267], loss: 51.765383, mean_absolute_error: 39.325597, mean_q: 76.404091
 44527/50000: episode: 543, duration: 0.256s, episode steps: 50, steps per second: 195, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.112 [-0.617, 0.920], loss: 2.087484, mean_absolute_error: 22.046918, mean_q: 43.671725
 44570/50000: episode: 544, duration: 0.221s, episode steps: 43, steps per second: 194, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: 0.079 [-0.596, 1.620], loss: 5.277349, mean_absolute_error: 23.302178, mean_q: 46.004911
 44617/50000: episode: 545, duration: 0.239s, episode steps: 47, steps per second: 196, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: 0.089 [-0.415, 1.094], loss: 3.016798, mean_absolute_error: 21.246501, mean_q: 42.318506
 44657/50000: episode: 546, duration: 0.216s, episode steps: 40, steps per second: 185, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.133 [-0.398, 1.505], loss: 1.024143, mean_absolute_error: 17.031239, mean_q: 33.861674
 44694/50000: episode: 547, duration: 0.189s, episode steps: 37, steps per second: 196, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.092 [-0.361, 1.039], loss: 6.162520, mean_absolute_error: 20.018518, mean_q: 39.520999
 44769/50000: episode: 548, duration: 0.385s, episode steps: 75, steps per second: 195, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.115 [-0.550, 1.171], loss: 0.646337, mean_absolute_error: 16.508811, mean_q: 33.132757
 44962/50000: episode: 549, duration: 1.004s, episode steps: 193, steps per second: 192, episode reward: 193.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.503 [0.000, 1.000], mean observation: 0.210 [-1.005, 1.640], loss: 0.585379, mean_absolute_error: 12.973896, mean_q: 25.815619
 45072/50000: episode: 550, duration: 0.569s, episode steps: 110, steps per second: 193, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.088 [-1.132, 0.939], loss: 12.864563, mean_absolute_error: 21.763152, mean_q: 42.900015
 45295/50000: episode: 551, duration: 1.135s, episode steps: 223, steps per second: 196, episode reward: 223.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.226 [-0.855, 1.823], loss: 1.089616, mean_absolute_error: 19.550533, mean_q: 38.978486
 45795/50000: episode: 552, duration: 2.529s, episode steps: 500, steps per second: 198, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.203 [-1.129, 0.925], loss: 6.859883, mean_absolute_error: 39.451731, mean_q: 79.064650
 46295/50000: episode: 553, duration: 2.531s, episode steps: 500, steps per second: 198, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.498 [0.000, 1.000], mean observation: -0.202 [-1.163, 0.941], loss: 8.611946, mean_absolute_error: 41.446680, mean_q: 83.054537
 46325/50000: episode: 554, duration: 0.150s, episode steps: 30, steps per second: 200, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.081 [-0.565, 1.344], loss: 24.180920, mean_absolute_error: 33.679257, mean_q: 64.582464
 46537/50000: episode: 555, duration: 1.080s, episode steps: 212, steps per second: 196, episode reward: 212.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.381 [-2.261, 1.052], loss: 6.078090, mean_absolute_error: 34.112503, mean_q: 67.747555
 46548/50000: episode: 556, duration: 0.059s, episode steps: 11, steps per second: 188, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.120 [-1.507, 0.935], loss: 205.804636, mean_absolute_error: 38.122335, mean_q: 69.846090
 46559/50000: episode: 557, duration: 0.056s, episode steps: 11, steps per second: 198, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.107 [-1.798, 1.183], loss: 68.035685, mean_absolute_error: 28.433645, mean_q: 53.109172
 46573/50000: episode: 558, duration: 0.071s, episode steps: 14, steps per second: 197, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.123 [-1.473, 0.943], loss: 66.660945, mean_absolute_error: 25.971184, mean_q: 45.180214
 46753/50000: episode: 559, duration: 0.925s, episode steps: 180, steps per second: 195, episode reward: 180.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.055 [-1.306, 1.499], loss: 3.331926, mean_absolute_error: 20.147743, mean_q: 39.706300
 46826/50000: episode: 560, duration: 0.367s, episode steps: 73, steps per second: 199, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.534 [0.000, 1.000], mean observation: 0.180 [-0.359, 1.139], loss: 1.405414, mean_absolute_error: 16.766731, mean_q: 33.261667
 46897/50000: episode: 561, duration: 0.378s, episode steps: 71, steps per second: 188, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.549 [0.000, 1.000], mean observation: 0.295 [-0.581, 1.850], loss: 1.449418, mean_absolute_error: 13.287025, mean_q: 26.165176
 46940/50000: episode: 562, duration: 0.224s, episode steps: 43, steps per second: 192, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.049 [-0.561, 1.146], loss: 8.562577, mean_absolute_error: 24.143050, mean_q: 47.805327
 46997/50000: episode: 563, duration: 0.312s, episode steps: 57, steps per second: 183, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.544 [0.000, 1.000], mean observation: 0.200 [-0.416, 0.975], loss: 1.165934, mean_absolute_error: 13.793797, mean_q: 27.432939
 47081/50000: episode: 564, duration: 0.439s, episode steps: 84, steps per second: 191, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.188 [-0.790, 1.649], loss: 1.062236, mean_absolute_error: 15.594968, mean_q: 31.068014
 47136/50000: episode: 565, duration: 0.286s, episode steps: 55, steps per second: 192, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: 0.103 [-0.614, 1.371], loss: 2.217443, mean_absolute_error: 18.837429, mean_q: 37.680186
 47175/50000: episode: 566, duration: 0.202s, episode steps: 39, steps per second: 193, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.119 [-0.655, 1.186], loss: 2.349731, mean_absolute_error: 17.812378, mean_q: 35.214113
 47217/50000: episode: 567, duration: 0.219s, episode steps: 42, steps per second: 192, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: 0.138 [-0.452, 0.893], loss: 1.348342, mean_absolute_error: 15.077106, mean_q: 30.061069
 47257/50000: episode: 568, duration: 0.215s, episode steps: 40, steps per second: 186, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.169 [-0.738, 1.245], loss: 1.699452, mean_absolute_error: 13.161250, mean_q: 25.912946
 47328/50000: episode: 569, duration: 0.360s, episode steps: 71, steps per second: 197, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: 0.128 [-0.853, 2.145], loss: 1.328602, mean_absolute_error: 15.350177, mean_q: 30.843943
 47422/50000: episode: 570, duration: 0.477s, episode steps: 94, steps per second: 197, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: 0.258 [-0.594, 1.252], loss: 1.147500, mean_absolute_error: 11.223954, mean_q: 22.340272
 47559/50000: episode: 571, duration: 0.705s, episode steps: 137, steps per second: 194, episode reward: 137.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.504 [0.000, 1.000], mean observation: 0.174 [-0.856, 1.578], loss: 0.820375, mean_absolute_error: 15.888460, mean_q: 31.835053
 48059/50000: episode: 572, duration: 2.552s, episode steps: 500, steps per second: 196, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.498 [0.000, 1.000], mean observation: -0.084 [-0.964, 0.850], loss: 6.384794, mean_absolute_error: 33.043638, mean_q: 66.255566
 48559/50000: episode: 573, duration: 2.522s, episode steps: 500, steps per second: 198, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.267 [-1.484, 0.732], loss: 6.527032, mean_absolute_error: 36.086839, mean_q: 72.427367
 48926/50000: episode: 574, duration: 1.860s, episode steps: 367, steps per second: 197, episode reward: 367.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.501 [0.000, 1.000], mean observation: -0.382 [-1.968, 0.973], loss: 6.520733, mean_absolute_error: 35.750814, mean_q: 71.607785
 48946/50000: episode: 575, duration: 0.103s, episode steps: 20, steps per second: 193, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.105 [-1.190, 0.818], loss: 187.574619, mean_absolute_error: 45.445188, mean_q: 85.175928
 49446/50000: episode: 576, duration: 2.903s, episode steps: 500, steps per second: 172, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.136 [-0.910, 0.763], loss: 7.181937, mean_absolute_error: 38.157528, mean_q: 76.682695
 49946/50000: episode: 577, duration: 2.937s, episode steps: 500, steps per second: 170, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.502 [0.000, 1.000], mean observation: -0.260 [-1.369, 0.930], loss: 5.798377, mean_absolute_error: 37.153808, mean_q: 74.630629