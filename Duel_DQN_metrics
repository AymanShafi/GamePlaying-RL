    20/50000: episode: 1, duration: 2.144s, episode steps: 20, steps per second: 9, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.059 [-0.998, 1.652], loss: 0.467027, mean_absolute_error: 0.514025, mean_q: -0.058345
    73/50000: episode: 2, duration: 0.366s, episode steps: 53, steps per second: 145, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.566 [0.000, 1.000], mean observation: 0.218 [-1.248, 1.743], loss: 0.341551, mean_absolute_error: 0.471468, mean_q: 0.132281
   106/50000: episode: 3, duration: 0.237s, episode steps: 33, steps per second: 139, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: -0.029 [-1.201, 0.833], loss: 0.125812, mean_absolute_error: 0.516086, mean_q: 0.589520
   116/50000: episode: 4, duration: 0.072s, episode steps: 10, steps per second: 140, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.152 [-2.770, 1.738], loss: 0.033075, mean_absolute_error: 0.600775, mean_q: 0.999000
   139/50000: episode: 5, duration: 0.163s, episode steps: 23, steps per second: 141, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.565 [0.000, 1.000], mean observation: -0.042 [-1.852, 1.210], loss: 0.023084, mean_absolute_error: 0.654777, mean_q: 1.151750
   198/50000: episode: 6, duration: 0.400s, episode steps: 59, steps per second: 148, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: 0.033 [-1.335, 1.555], loss: 0.007228, mean_absolute_error: 0.746033, mean_q: 1.434596
   225/50000: episode: 7, duration: 0.182s, episode steps: 27, steps per second: 148, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.593 [0.000, 1.000], mean observation: -0.054 [-1.836, 1.034], loss: 0.008476, mean_absolute_error: 0.876513, mean_q: 1.710291
   237/50000: episode: 8, duration: 0.097s, episode steps: 12, steps per second: 123, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.113 [-1.946, 1.174], loss: 0.009621, mean_absolute_error: 0.939235, mean_q: 1.846861
   286/50000: episode: 9, duration: 0.339s, episode steps: 49, steps per second: 145, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.109 [-0.551, 1.143], loss: 0.013150, mean_absolute_error: 1.045291, mean_q: 2.042729
   339/50000: episode: 10, duration: 0.364s, episode steps: 53, steps per second: 145, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: 0.085 [-1.006, 1.486], loss: 0.023447, mean_absolute_error: 1.239868, mean_q: 2.413984
   373/50000: episode: 11, duration: 0.235s, episode steps: 34, steps per second: 145, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.010 [-1.968, 1.334], loss: 0.038374, mean_absolute_error: 1.417582, mean_q: 2.733847
   391/50000: episode: 12, duration: 0.137s, episode steps: 18, steps per second: 132, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.069 [-1.275, 0.826], loss: 0.021746, mean_absolute_error: 1.523953, mean_q: 2.992713
   410/50000: episode: 13, duration: 0.136s, episode steps: 19, steps per second: 140, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.684 [0.000, 1.000], mean observation: -0.062 [-2.334, 1.416], loss: 0.056421, mean_absolute_error: 1.596116, mean_q: 3.080279
   425/50000: episode: 14, duration: 0.106s, episode steps: 15, steps per second: 142, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.098 [-1.158, 1.849], loss: 0.033325, mean_absolute_error: 1.667770, mean_q: 3.259011
   437/50000: episode: 15, duration: 0.088s, episode steps: 12, steps per second: 137, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.095 [-1.379, 2.237], loss: 0.066478, mean_absolute_error: 1.731899, mean_q: 3.313225
   453/50000: episode: 16, duration: 0.115s, episode steps: 16, steps per second: 139, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.062 [-1.830, 1.181], loss: 0.076821, mean_absolute_error: 1.798553, mean_q: 3.457601
   488/50000: episode: 17, duration: 0.240s, episode steps: 35, steps per second: 146, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.062 [-1.284, 0.846], loss: 0.071272, mean_absolute_error: 1.905804, mean_q: 3.679135
   505/50000: episode: 18, duration: 0.118s, episode steps: 17, steps per second: 144, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.647 [0.000, 1.000], mean observation: -0.088 [-1.843, 0.980], loss: 0.065861, mean_absolute_error: 2.004016, mean_q: 3.912066
   525/50000: episode: 19, duration: 0.152s, episode steps: 20, steps per second: 132, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.086 [-0.956, 1.582], loss: 0.053533, mean_absolute_error: 2.093760, mean_q: 4.079123
   566/50000: episode: 20, duration: 0.281s, episode steps: 41, steps per second: 146, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.366 [0.000, 1.000], mean observation: -0.023 [-2.117, 2.729], loss: 0.146776, mean_absolute_error: 2.211040, mean_q: 4.220528
   601/50000: episode: 21, duration: 0.241s, episode steps: 35, steps per second: 145, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: 0.063 [-1.782, 1.375], loss: 0.115999, mean_absolute_error: 2.364227, mean_q: 4.579822
   630/50000: episode: 22, duration: 0.200s, episode steps: 29, steps per second: 145, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: -0.047 [-1.089, 0.766], loss: 0.166667, mean_absolute_error: 2.513445, mean_q: 4.815845
   676/50000: episode: 23, duration: 0.325s, episode steps: 46, steps per second: 142, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.211 [-0.550, 1.152], loss: 0.187802, mean_absolute_error: 2.644397, mean_q: 5.081532
   692/50000: episode: 24, duration: 0.115s, episode steps: 16, steps per second: 139, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.100 [-0.773, 1.322], loss: 0.148999, mean_absolute_error: 2.768580, mean_q: 5.335445
   716/50000: episode: 25, duration: 0.171s, episode steps: 24, steps per second: 140, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.020 [-1.185, 1.726], loss: 0.162856, mean_absolute_error: 2.847738, mean_q: 5.514137
   731/50000: episode: 26, duration: 0.104s, episode steps: 15, steps per second: 144, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.078 [-1.451, 0.992], loss: 0.131934, mean_absolute_error: 2.948329, mean_q: 5.760526
   746/50000: episode: 27, duration: 0.110s, episode steps: 15, steps per second: 136, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.110 [-1.136, 0.585], loss: 0.204029, mean_absolute_error: 2.997764, mean_q: 5.804896
   761/50000: episode: 28, duration: 0.111s, episode steps: 15, steps per second: 135, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.081 [-1.424, 2.361], loss: 0.227823, mean_absolute_error: 3.037138, mean_q: 5.864668
   819/50000: episode: 29, duration: 0.402s, episode steps: 58, steps per second: 144, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.053 [-1.621, 0.705], loss: 0.173444, mean_absolute_error: 3.198209, mean_q: 6.232941
   830/50000: episode: 30, duration: 0.081s, episode steps: 11, steps per second: 136, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.097 [-2.407, 1.607], loss: 0.260918, mean_absolute_error: 3.329475, mean_q: 6.475878
   841/50000: episode: 31, duration: 0.077s, episode steps: 11, steps per second: 142, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.117 [-1.596, 2.440], loss: 0.277863, mean_absolute_error: 3.345793, mean_q: 6.466576
   870/50000: episode: 32, duration: 0.201s, episode steps: 29, steps per second: 144, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: -0.086 [-1.022, 0.548], loss: 0.204944, mean_absolute_error: 3.449117, mean_q: 6.704974
   885/50000: episode: 33, duration: 0.106s, episode steps: 15, steps per second: 142, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.092 [-0.764, 1.309], loss: 0.296483, mean_absolute_error: 3.528633, mean_q: 6.808377
   918/50000: episode: 34, duration: 0.228s, episode steps: 33, steps per second: 145, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.059 [-1.058, 0.645], loss: 0.193660, mean_absolute_error: 3.626792, mean_q: 7.064159
   939/50000: episode: 35, duration: 0.141s, episode steps: 21, steps per second: 149, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.069 [-1.005, 1.663], loss: 0.206284, mean_absolute_error: 3.737151, mean_q: 7.312356
   966/50000: episode: 36, duration: 0.196s, episode steps: 27, steps per second: 138, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.370 [0.000, 1.000], mean observation: -0.013 [-1.573, 2.140], loss: 0.197073, mean_absolute_error: 3.807410, mean_q: 7.495815
   987/50000: episode: 37, duration: 0.149s, episode steps: 21, steps per second: 141, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.381 [0.000, 1.000], mean observation: 0.043 [-1.347, 2.004], loss: 0.294135, mean_absolute_error: 3.942260, mean_q: 7.727055
  1002/50000: episode: 38, duration: 0.111s, episode steps: 15, steps per second: 135, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.733 [0.000, 1.000], mean observation: -0.095 [-2.265, 1.394], loss: 0.121405, mean_absolute_error: 4.032370, mean_q: 8.057088
  1050/50000: episode: 39, duration: 0.327s, episode steps: 48, steps per second: 147, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.041 [-1.154, 1.310], loss: 0.300255, mean_absolute_error: 4.107165, mean_q: 8.076711
  1140/50000: episode: 40, duration: 0.615s, episode steps: 90, steps per second: 146, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.237 [-1.329, 0.962], loss: 0.343319, mean_absolute_error: 4.377688, mean_q: 8.648244
  1191/50000: episode: 41, duration: 0.346s, episode steps: 51, steps per second: 147, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.096 [-1.194, 1.150], loss: 0.358932, mean_absolute_error: 4.668140, mean_q: 9.258649
  1261/50000: episode: 42, duration: 0.481s, episode steps: 70, steps per second: 146, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: 0.091 [-1.475, 1.655], loss: 0.356165, mean_absolute_error: 4.914287, mean_q: 9.770793
  1290/50000: episode: 43, duration: 0.199s, episode steps: 29, steps per second: 145, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.621 [0.000, 1.000], mean observation: 0.039 [-2.264, 1.779], loss: 0.313776, mean_absolute_error: 5.134114, mean_q: 10.333065
  1359/50000: episode: 44, duration: 0.467s, episode steps: 69, steps per second: 148, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: -0.040 [-1.346, 1.240], loss: 0.350650, mean_absolute_error: 5.335874, mean_q: 10.733848
  1444/50000: episode: 45, duration: 0.598s, episode steps: 85, steps per second: 142, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: 0.124 [-1.126, 1.183], loss: 0.397091, mean_absolute_error: 5.702925, mean_q: 11.476181
  1511/50000: episode: 46, duration: 0.452s, episode steps: 67, steps per second: 148, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: 0.014 [-1.119, 1.464], loss: 0.419408, mean_absolute_error: 6.007440, mean_q: 12.158275
  1635/50000: episode: 47, duration: 0.842s, episode steps: 124, steps per second: 147, episode reward: 124.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.131 [-1.098, 1.541], loss: 0.508668, mean_absolute_error: 6.462300, mean_q: 13.040202
  1772/50000: episode: 48, duration: 0.935s, episode steps: 137, steps per second: 147, episode reward: 137.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.188 [-1.348, 1.158], loss: 0.521320, mean_absolute_error: 7.040308, mean_q: 14.310108
  2027/50000: episode: 49, duration: 1.714s, episode steps: 255, steps per second: 149, episode reward: 255.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.149 [-1.837, 1.310], loss: 0.616752, mean_absolute_error: 7.933895, mean_q: 16.130676
  2204/50000: episode: 50, duration: 1.188s, episode steps: 177, steps per second: 149, episode reward: 177.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.230 [-1.680, 1.110], loss: 0.683640, mean_absolute_error: 8.898702, mean_q: 18.148314
  2379/50000: episode: 51, duration: 1.166s, episode steps: 175, steps per second: 150, episode reward: 175.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.267 [-1.966, 0.814], loss: 0.735913, mean_absolute_error: 9.730418, mean_q: 19.952053
  2595/50000: episode: 52, duration: 1.455s, episode steps: 216, steps per second: 148, episode reward: 216.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.096 [-1.013, 1.282], loss: 0.920298, mean_absolute_error: 10.649718, mean_q: 21.854118
  2792/50000: episode: 53, duration: 1.332s, episode steps: 197, steps per second: 148, episode reward: 197.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.246 [-1.865, 0.821], loss: 0.952905, mean_absolute_error: 11.743236, mean_q: 24.106222
  3101/50000: episode: 54, duration: 2.172s, episode steps: 309, steps per second: 142, episode reward: 309.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.172 [-2.023, 1.212], loss: 1.432439, mean_absolute_error: 13.009823, mean_q: 26.603991
  3269/50000: episode: 55, duration: 1.292s, episode steps: 168, steps per second: 130, episode reward: 168.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.281 [-1.799, 0.844], loss: 1.194277, mean_absolute_error: 14.115640, mean_q: 28.841602
  3404/50000: episode: 56, duration: 1.046s, episode steps: 135, steps per second: 129, episode reward: 135.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.368 [-2.030, 0.876], loss: 1.250672, mean_absolute_error: 14.710758, mean_q: 30.151930
  3610/50000: episode: 57, duration: 1.604s, episode steps: 206, steps per second: 128, episode reward: 206.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.219 [-1.656, 0.821], loss: 1.220790, mean_absolute_error: 15.478260, mean_q: 31.786768
  3805/50000: episode: 58, duration: 1.500s, episode steps: 195, steps per second: 130, episode reward: 195.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.267 [-1.970, 0.833], loss: 1.550908, mean_absolute_error: 16.469158, mean_q: 33.709900
  4129/50000: episode: 59, duration: 2.476s, episode steps: 324, steps per second: 131, episode reward: 324.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.157 [-1.852, 0.844], loss: 1.791962, mean_absolute_error: 17.499374, mean_q: 35.832211
  4361/50000: episode: 60, duration: 1.758s, episode steps: 232, steps per second: 132, episode reward: 232.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.225 [-1.923, 0.875], loss: 1.707018, mean_absolute_error: 18.778526, mean_q: 38.416790
  4533/50000: episode: 61, duration: 1.154s, episode steps: 172, steps per second: 149, episode reward: 172.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.292 [-1.837, 0.690], loss: 1.392811, mean_absolute_error: 19.540743, mean_q: 40.086819
  4714/50000: episode: 62, duration: 1.240s, episode steps: 181, steps per second: 146, episode reward: 181.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.288 [-1.877, 0.780], loss: 2.128438, mean_absolute_error: 20.319500, mean_q: 41.620548
  4896/50000: episode: 63, duration: 1.219s, episode steps: 182, steps per second: 149, episode reward: 182.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.281 [-1.823, 0.878], loss: 1.852514, mean_absolute_error: 21.280115, mean_q: 43.499508
  5088/50000: episode: 64, duration: 1.288s, episode steps: 192, steps per second: 149, episode reward: 192.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.280 [-1.960, 0.712], loss: 1.657616, mean_absolute_error: 21.810463, mean_q: 44.627941
  5273/50000: episode: 65, duration: 1.246s, episode steps: 185, steps per second: 148, episode reward: 185.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.270 [-1.778, 0.769], loss: 2.514138, mean_absolute_error: 22.388046, mean_q: 45.792103
  5482/50000: episode: 66, duration: 1.415s, episode steps: 209, steps per second: 148, episode reward: 209.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.258 [-1.981, 0.677], loss: 2.085976, mean_absolute_error: 23.246632, mean_q: 47.512741
  5660/50000: episode: 67, duration: 1.201s, episode steps: 178, steps per second: 148, episode reward: 178.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.285 [-1.911, 0.850], loss: 2.954693, mean_absolute_error: 23.857904, mean_q: 48.780262
  5899/50000: episode: 68, duration: 1.627s, episode steps: 239, steps per second: 147, episode reward: 239.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.214 [-1.981, 0.897], loss: 2.643826, mean_absolute_error: 24.666710, mean_q: 50.385448
  6080/50000: episode: 69, duration: 1.229s, episode steps: 181, steps per second: 147, episode reward: 181.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.282 [-1.850, 0.884], loss: 2.256950, mean_absolute_error: 25.543959, mean_q: 52.072235
  6318/50000: episode: 70, duration: 1.583s, episode steps: 238, steps per second: 150, episode reward: 238.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.213 [-1.993, 0.919], loss: 2.516296, mean_absolute_error: 26.200142, mean_q: 53.409359
  6818/50000: episode: 71, duration: 3.355s, episode steps: 500, steps per second: 149, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.504 [0.000, 1.000], mean observation: 0.056 [-0.970, 0.887], loss: 2.589974, mean_absolute_error: 27.310734, mean_q: 55.715782
  7098/50000: episode: 72, duration: 1.879s, episode steps: 280, steps per second: 149, episode reward: 280.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.171 [-1.787, 0.828], loss: 2.686921, mean_absolute_error: 28.934328, mean_q: 58.993038
  7278/50000: episode: 73, duration: 1.212s, episode steps: 180, steps per second: 149, episode reward: 180.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.286 [-1.869, 0.669], loss: 2.445377, mean_absolute_error: 29.491995, mean_q: 60.101109
  7501/50000: episode: 74, duration: 1.532s, episode steps: 223, steps per second: 146, episode reward: 223.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.205 [-1.034, 1.880], loss: 2.812719, mean_absolute_error: 30.272825, mean_q: 61.664246
  7751/50000: episode: 75, duration: 1.694s, episode steps: 250, steps per second: 148, episode reward: 250.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.178 [-0.807, 1.666], loss: 4.054873, mean_absolute_error: 31.035669, mean_q: 63.093464
  7964/50000: episode: 76, duration: 1.443s, episode steps: 213, steps per second: 148, episode reward: 213.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.188 [-0.762, 1.526], loss: 3.513931, mean_absolute_error: 31.661867, mean_q: 64.414978
  8159/50000: episode: 77, duration: 1.409s, episode steps: 195, steps per second: 138, episode reward: 195.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.446 [0.000, 1.000], mean observation: -0.314 [-3.906, 2.419], loss: 4.456357, mean_absolute_error: 32.581150, mean_q: 66.126160
  8434/50000: episode: 78, duration: 2.209s, episode steps: 275, steps per second: 124, episode reward: 275.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.191 [-0.867, 1.881], loss: 6.272793, mean_absolute_error: 33.221954, mean_q: 67.454414
  8665/50000: episode: 79, duration: 1.828s, episode steps: 231, steps per second: 126, episode reward: 231.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.229 [-1.968, 0.585], loss: 4.357824, mean_absolute_error: 33.835312, mean_q: 68.674980
  8873/50000: episode: 80, duration: 1.515s, episode steps: 208, steps per second: 137, episode reward: 208.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: -0.293 [-4.081, 2.706], loss: 5.195983, mean_absolute_error: 34.266785, mean_q: 69.505867
  9090/50000: episode: 81, duration: 1.471s, episode steps: 217, steps per second: 148, episode reward: 217.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.205 [-1.638, 0.680], loss: 5.782835, mean_absolute_error: 34.782024, mean_q: 70.522263
  9296/50000: episode: 82, duration: 1.412s, episode steps: 206, steps per second: 146, episode reward: 206.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.247 [-1.810, 0.858], loss: 4.591060, mean_absolute_error: 35.200089, mean_q: 71.414833
  9495/50000: episode: 83, duration: 1.357s, episode steps: 199, steps per second: 147, episode reward: 199.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.248 [-1.765, 0.905], loss: 3.729419, mean_absolute_error: 35.639370, mean_q: 72.363373
  9734/50000: episode: 84, duration: 1.617s, episode steps: 239, steps per second: 148, episode reward: 239.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.240 [-4.296, 3.521], loss: 4.101839, mean_absolute_error: 36.033024, mean_q: 73.106972
 10072/50000: episode: 85, duration: 2.287s, episode steps: 338, steps per second: 148, episode reward: 338.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.182 [-4.139, 2.838], loss: 4.297651, mean_absolute_error: 36.757572, mean_q: 74.508034
 10512/50000: episode: 86, duration: 3.000s, episode steps: 440, steps per second: 147, episode reward: 440.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.141 [-4.126, 2.989], loss: 3.557126, mean_absolute_error: 37.562500, mean_q: 76.165382
 10753/50000: episode: 87, duration: 1.625s, episode steps: 241, steps per second: 148, episode reward: 241.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.243 [-4.287, 3.371], loss: 3.733444, mean_absolute_error: 38.311852, mean_q: 77.744751
 10968/50000: episode: 88, duration: 1.470s, episode steps: 215, steps per second: 146, episode reward: 215.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.252 [-2.035, 0.735], loss: 2.948142, mean_absolute_error: 38.438736, mean_q: 77.938354
 11243/50000: episode: 89, duration: 1.873s, episode steps: 275, steps per second: 147, episode reward: 275.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.191 [-2.028, 0.809], loss: 3.276109, mean_absolute_error: 39.056015, mean_q: 79.103767
 11489/50000: episode: 90, duration: 1.667s, episode steps: 246, steps per second: 148, episode reward: 246.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.244 [-4.091, 2.862], loss: 4.952256, mean_absolute_error: 39.739685, mean_q: 80.530647
 11740/50000: episode: 91, duration: 1.708s, episode steps: 251, steps per second: 147, episode reward: 251.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.454 [0.000, 1.000], mean observation: -0.234 [-4.288, 3.417], loss: 3.685268, mean_absolute_error: 39.693371, mean_q: 80.488297
 12026/50000: episode: 92, duration: 1.961s, episode steps: 286, steps per second: 146, episode reward: 286.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.201 [-4.170, 3.274], loss: 3.706007, mean_absolute_error: 40.131790, mean_q: 81.366241
 12246/50000: episode: 93, duration: 1.496s, episode steps: 220, steps per second: 147, episode reward: 220.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.267 [-4.144, 3.099], loss: 3.029695, mean_absolute_error: 40.514328, mean_q: 82.048866
 12486/50000: episode: 94, duration: 1.626s, episode steps: 240, steps per second: 148, episode reward: 240.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.454 [0.000, 1.000], mean observation: -0.253 [-4.157, 2.974], loss: 3.608521, mean_absolute_error: 40.675789, mean_q: 82.337372
 12726/50000: episode: 95, duration: 1.626s, episode steps: 240, steps per second: 148, episode reward: 240.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.207 [-0.852, 1.614], loss: 2.984421, mean_absolute_error: 40.939102, mean_q: 82.964737
 13059/50000: episode: 96, duration: 2.265s, episode steps: 333, steps per second: 147, episode reward: 333.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.193 [-3.880, 2.362], loss: 3.301999, mean_absolute_error: 41.732510, mean_q: 84.554314
 13258/50000: episode: 97, duration: 1.357s, episode steps: 199, steps per second: 147, episode reward: 199.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.267 [-2.030, 0.804], loss: 2.421541, mean_absolute_error: 42.076893, mean_q: 85.179001
 13555/50000: episode: 98, duration: 2.046s, episode steps: 297, steps per second: 145, episode reward: 297.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.173 [-1.972, 1.149], loss: 3.509611, mean_absolute_error: 42.140388, mean_q: 85.298271
 13821/50000: episode: 99, duration: 1.817s, episode steps: 266, steps per second: 146, episode reward: 266.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.226 [-4.145, 3.008], loss: 3.302931, mean_absolute_error: 42.364883, mean_q: 85.718338
 14009/50000: episode: 100, duration: 1.287s, episode steps: 188, steps per second: 146, episode reward: 188.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.267 [-1.858, 0.766], loss: 2.336285, mean_absolute_error: 42.272709, mean_q: 85.461136
 14195/50000: episode: 101, duration: 1.260s, episode steps: 186, steps per second: 148, episode reward: 186.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.262 [-1.814, 0.790], loss: 3.279550, mean_absolute_error: 43.018742, mean_q: 86.842659
 14425/50000: episode: 102, duration: 1.565s, episode steps: 230, steps per second: 147, episode reward: 230.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.221 [-1.834, 0.803], loss: 3.772170, mean_absolute_error: 42.629509, mean_q: 86.221588
 14704/50000: episode: 103, duration: 2.031s, episode steps: 279, steps per second: 137, episode reward: 279.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.210 [-4.323, 3.300], loss: 2.167167, mean_absolute_error: 43.314262, mean_q: 87.580627
 15003/50000: episode: 104, duration: 2.364s, episode steps: 299, steps per second: 126, episode reward: 299.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.198 [-4.340, 3.578], loss: 3.709774, mean_absolute_error: 43.262638, mean_q: 87.582542
 15199/50000: episode: 105, duration: 1.533s, episode steps: 196, steps per second: 128, episode reward: 196.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.307 [-4.150, 2.894], loss: 2.438033, mean_absolute_error: 43.595104, mean_q: 88.090401
 15454/50000: episode: 106, duration: 1.995s, episode steps: 255, steps per second: 128, episode reward: 255.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.248 [-3.937, 2.434], loss: 2.616461, mean_absolute_error: 43.556648, mean_q: 88.295128
 15661/50000: episode: 107, duration: 1.638s, episode steps: 207, steps per second: 126, episode reward: 207.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.245 [-0.821, 1.955], loss: 4.410822, mean_absolute_error: 43.703876, mean_q: 88.340485
 16030/50000: episode: 108, duration: 2.691s, episode steps: 369, steps per second: 137, episode reward: 369.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.504 [0.000, 1.000], mean observation: 0.132 [-0.744, 1.786], loss: 4.306642, mean_absolute_error: 43.857708, mean_q: 88.508850
 16491/50000: episode: 109, duration: 3.384s, episode steps: 461, steps per second: 136, episode reward: 461.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.503 [0.000, 1.000], mean observation: 0.121 [-0.817, 1.796], loss: 2.597558, mean_absolute_error: 44.057186, mean_q: 89.097260
 16701/50000: episode: 110, duration: 1.760s, episode steps: 210, steps per second: 119, episode reward: 210.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.250 [-1.860, 0.747], loss: 3.390662, mean_absolute_error: 44.427502, mean_q: 89.906097
 16935/50000: episode: 111, duration: 1.947s, episode steps: 234, steps per second: 120, episode reward: 234.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.217 [-1.827, 1.007], loss: 2.608956, mean_absolute_error: 44.679241, mean_q: 90.308510
 17164/50000: episode: 112, duration: 1.909s, episode steps: 229, steps per second: 120, episode reward: 229.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.222 [-1.987, 0.875], loss: 2.354201, mean_absolute_error: 44.681957, mean_q: 90.391624
 17368/50000: episode: 113, duration: 1.698s, episode steps: 204, steps per second: 120, episode reward: 204.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.267 [-0.819, 2.092], loss: 2.931126, mean_absolute_error: 44.941570, mean_q: 90.867592
 17603/50000: episode: 114, duration: 1.919s, episode steps: 235, steps per second: 122, episode reward: 235.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.451 [0.000, 1.000], mean observation: -0.243 [-4.379, 3.686], loss: 2.444113, mean_absolute_error: 45.364918, mean_q: 91.590179
 17963/50000: episode: 115, duration: 2.463s, episode steps: 360, steps per second: 146, episode reward: 360.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.167 [-4.174, 3.045], loss: 2.692386, mean_absolute_error: 45.087444, mean_q: 90.957924
 18179/50000: episode: 116, duration: 1.482s, episode steps: 216, steps per second: 146, episode reward: 216.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.242 [-0.854, 2.496], loss: 2.490793, mean_absolute_error: 45.426067, mean_q: 91.495842
 18414/50000: episode: 117, duration: 1.607s, episode steps: 235, steps per second: 146, episode reward: 235.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.205 [-1.673, 0.816], loss: 3.081079, mean_absolute_error: 45.566971, mean_q: 91.765419
 18627/50000: episode: 118, duration: 1.469s, episode steps: 213, steps per second: 145, episode reward: 213.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.446 [0.000, 1.000], mean observation: -0.270 [-4.366, 3.539], loss: 3.681045, mean_absolute_error: 45.486149, mean_q: 91.566238
 18791/50000: episode: 119, duration: 1.132s, episode steps: 164, steps per second: 145, episode reward: 164.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.274 [-1.598, 0.880], loss: 1.385629, mean_absolute_error: 45.104851, mean_q: 90.969070
 18996/50000: episode: 120, duration: 1.405s, episode steps: 205, steps per second: 146, episode reward: 205.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.293 [-4.286, 3.254], loss: 1.900458, mean_absolute_error: 45.247631, mean_q: 91.207657
 19165/50000: episode: 121, duration: 1.194s, episode steps: 169, steps per second: 142, episode reward: 169.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: -0.282 [-4.012, 3.515], loss: 1.489807, mean_absolute_error: 45.413845, mean_q: 91.362938
 19382/50000: episode: 122, duration: 1.493s, episode steps: 217, steps per second: 145, episode reward: 217.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.327 [-0.836, 2.372], loss: 1.358151, mean_absolute_error: 45.379417, mean_q: 91.379227
 19566/50000: episode: 123, duration: 1.266s, episode steps: 184, steps per second: 145, episode reward: 184.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.239 [-1.690, 0.970], loss: 5.335725, mean_absolute_error: 45.093563, mean_q: 90.614830
 19791/50000: episode: 124, duration: 1.554s, episode steps: 225, steps per second: 145, episode reward: 225.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.453 [0.000, 1.000], mean observation: -0.207 [-3.977, 3.641], loss: 4.802876, mean_absolute_error: 45.379887, mean_q: 91.215469
 19986/50000: episode: 125, duration: 1.343s, episode steps: 195, steps per second: 145, episode reward: 195.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.441 [0.000, 1.000], mean observation: -0.291 [-4.343, 3.507], loss: 2.007382, mean_absolute_error: 45.230629, mean_q: 90.913399
 20161/50000: episode: 126, duration: 1.202s, episode steps: 175, steps per second: 146, episode reward: 175.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.434 [0.000, 1.000], mean observation: -0.327 [-4.308, 3.403], loss: 3.185939, mean_absolute_error: 45.219765, mean_q: 90.959908
 20383/50000: episode: 127, duration: 1.520s, episode steps: 222, steps per second: 146, episode reward: 222.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.211 [-1.785, 0.759], loss: 1.553778, mean_absolute_error: 45.016495, mean_q: 90.560760
 20629/50000: episode: 128, duration: 1.699s, episode steps: 246, steps per second: 145, episode reward: 246.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.451 [0.000, 1.000], mean observation: -0.236 [-4.475, 3.522], loss: 2.332985, mean_absolute_error: 45.046364, mean_q: 90.593086
 20902/50000: episode: 129, duration: 1.873s, episode steps: 273, steps per second: 146, episode reward: 273.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.258 [-0.855, 2.407], loss: 2.709118, mean_absolute_error: 45.151894, mean_q: 90.912308
 21105/50000: episode: 130, duration: 1.403s, episode steps: 203, steps per second: 145, episode reward: 203.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.347 [-1.112, 2.418], loss: 3.285883, mean_absolute_error: 45.356449, mean_q: 91.126503
 21290/50000: episode: 131, duration: 1.267s, episode steps: 185, steps per second: 146, episode reward: 185.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.443 [0.000, 1.000], mean observation: -0.252 [-4.000, 3.688], loss: 1.755030, mean_absolute_error: 45.038651, mean_q: 90.346054
 21504/50000: episode: 132, duration: 1.469s, episode steps: 214, steps per second: 146, episode reward: 214.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.330 [-0.808, 2.417], loss: 3.737114, mean_absolute_error: 45.317913, mean_q: 90.944862
 21814/50000: episode: 133, duration: 2.119s, episode steps: 310, steps per second: 146, episode reward: 310.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.233 [-0.936, 2.423], loss: 2.249883, mean_absolute_error: 45.060452, mean_q: 90.458534
 22016/50000: episode: 134, duration: 1.393s, episode steps: 202, steps per second: 145, episode reward: 202.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.446 [0.000, 1.000], mean observation: -0.241 [-4.144, 3.745], loss: 2.726345, mean_absolute_error: 45.013641, mean_q: 90.187706
 22247/50000: episode: 135, duration: 1.596s, episode steps: 231, steps per second: 145, episode reward: 231.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.263 [-3.951, 2.552], loss: 2.492333, mean_absolute_error: 44.687222, mean_q: 89.625725
 22619/50000: episode: 136, duration: 2.549s, episode steps: 372, steps per second: 146, episode reward: 372.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.192 [-1.035, 2.433], loss: 2.315422, mean_absolute_error: 45.113110, mean_q: 90.632683
 22943/50000: episode: 137, duration: 2.231s, episode steps: 324, steps per second: 145, episode reward: 324.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.165 [-4.160, 3.648], loss: 2.300390, mean_absolute_error: 44.535957, mean_q: 89.504799
 23166/50000: episode: 138, duration: 1.525s, episode steps: 223, steps per second: 146, episode reward: 223.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: -0.263 [-4.314, 3.443], loss: 1.617930, mean_absolute_error: 44.954788, mean_q: 90.314079
 23453/50000: episode: 139, duration: 1.972s, episode steps: 287, steps per second: 146, episode reward: 287.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.249 [-0.742, 2.404], loss: 2.977593, mean_absolute_error: 44.788467, mean_q: 89.788803
 23634/50000: episode: 140, duration: 1.277s, episode steps: 181, steps per second: 142, episode reward: 181.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.436 [0.000, 1.000], mean observation: -0.275 [-4.347, 3.796], loss: 2.007977, mean_absolute_error: 45.019764, mean_q: 90.423027
 23864/50000: episode: 141, duration: 1.599s, episode steps: 230, steps per second: 144, episode reward: 230.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.261 [-4.148, 2.802], loss: 2.309556, mean_absolute_error: 44.681877, mean_q: 89.538750
 24071/50000: episode: 142, duration: 1.433s, episode steps: 207, steps per second: 144, episode reward: 207.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.291 [-4.339, 3.168], loss: 2.954352, mean_absolute_error: 44.777966, mean_q: 89.843697
 24254/50000: episode: 143, duration: 1.265s, episode steps: 183, steps per second: 145, episode reward: 183.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.530 [0.000, 1.000], mean observation: 0.384 [-0.860, 2.412], loss: 3.583566, mean_absolute_error: 44.794003, mean_q: 89.653580
 24465/50000: episode: 144, duration: 1.451s, episode steps: 211, steps per second: 145, episode reward: 211.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.445 [0.000, 1.000], mean observation: -0.270 [-4.367, 3.696], loss: 2.419414, mean_absolute_error: 44.684433, mean_q: 89.449211
 24684/50000: episode: 145, duration: 1.519s, episode steps: 219, steps per second: 144, episode reward: 219.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.324 [-0.826, 2.423], loss: 1.930351, mean_absolute_error: 44.382248, mean_q: 88.758743
 24853/50000: episode: 146, duration: 1.169s, episode steps: 169, steps per second: 145, episode reward: 169.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.432 [0.000, 1.000], mean observation: -0.316 [-4.370, 3.763], loss: 1.578348, mean_absolute_error: 43.945091, mean_q: 88.019608
 25034/50000: episode: 147, duration: 1.243s, episode steps: 181, steps per second: 146, episode reward: 181.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.442 [0.000, 1.000], mean observation: -0.336 [-3.966, 2.463], loss: 2.042490, mean_absolute_error: 43.899338, mean_q: 87.953209
 25228/50000: episode: 148, duration: 1.351s, episode steps: 194, steps per second: 144, episode reward: 194.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.370 [-0.762, 2.414], loss: 2.159837, mean_absolute_error: 44.009563, mean_q: 88.133621
 25465/50000: episode: 149, duration: 1.647s, episode steps: 237, steps per second: 144, episode reward: 237.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.451 [0.000, 1.000], mean observation: -0.242 [-4.334, 3.454], loss: 1.435434, mean_absolute_error: 43.756073, mean_q: 87.362190
 25618/50000: episode: 150, duration: 1.061s, episode steps: 153, steps per second: 144, episode reward: 153.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.281 [-1.641, 1.028], loss: 1.810513, mean_absolute_error: 43.852592, mean_q: 87.759125
 25828/50000: episode: 151, duration: 1.476s, episode steps: 210, steps per second: 142, episode reward: 210.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.345 [-0.775, 2.414], loss: 1.619101, mean_absolute_error: 43.514008, mean_q: 86.948120
 26021/50000: episode: 152, duration: 1.542s, episode steps: 193, steps per second: 125, episode reward: 193.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: -0.298 [-4.389, 3.590], loss: 0.925916, mean_absolute_error: 43.681824, mean_q: 87.347527
 26218/50000: episode: 153, duration: 1.565s, episode steps: 197, steps per second: 126, episode reward: 197.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.442 [0.000, 1.000], mean observation: -0.296 [-4.325, 3.345], loss: 1.475338, mean_absolute_error: 43.275600, mean_q: 86.549355
 26488/50000: episode: 154, duration: 2.157s, episode steps: 270, steps per second: 125, episode reward: 270.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.174 [-3.819, 3.525], loss: 1.997354, mean_absolute_error: 43.407421, mean_q: 86.716873
 26692/50000: episode: 155, duration: 1.634s, episode steps: 204, steps per second: 125, episode reward: 204.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.347 [-0.845, 2.425], loss: 1.957462, mean_absolute_error: 43.358269, mean_q: 86.656296
 26852/50000: episode: 156, duration: 1.282s, episode steps: 160, steps per second: 125, episode reward: 160.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.431 [0.000, 1.000], mean observation: -0.313 [-4.177, 3.538], loss: 1.833076, mean_absolute_error: 43.273949, mean_q: 86.385513
 27062/50000: episode: 157, duration: 1.666s, episode steps: 210, steps per second: 126, episode reward: 210.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: -0.275 [-4.159, 3.083], loss: 3.360636, mean_absolute_error: 43.055195, mean_q: 85.697144
 27282/50000: episode: 158, duration: 1.514s, episode steps: 220, steps per second: 145, episode reward: 220.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.185 [-1.658, 1.146], loss: 1.125285, mean_absolute_error: 42.778461, mean_q: 85.440613
 27512/50000: episode: 159, duration: 1.590s, episode steps: 230, steps per second: 145, episode reward: 230.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.313 [-1.070, 2.437], loss: 1.355696, mean_absolute_error: 42.799152, mean_q: 85.448166
 27761/50000: episode: 160, duration: 1.717s, episode steps: 249, steps per second: 145, episode reward: 249.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.273 [-1.151, 2.418], loss: 1.808795, mean_absolute_error: 42.669357, mean_q: 85.186989
 27944/50000: episode: 161, duration: 1.274s, episode steps: 183, steps per second: 144, episode reward: 183.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.530 [0.000, 1.000], mean observation: 0.385 [-0.946, 2.421], loss: 1.969200, mean_absolute_error: 42.943989, mean_q: 85.615120
 28128/50000: episode: 162, duration: 1.289s, episode steps: 184, steps per second: 143, episode reward: 184.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.383 [-0.899, 2.443], loss: 2.335536, mean_absolute_error: 42.801750, mean_q: 85.430008
 28348/50000: episode: 163, duration: 1.528s, episode steps: 220, steps per second: 144, episode reward: 220.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.285 [-4.174, 2.985], loss: 1.051016, mean_absolute_error: 41.847332, mean_q: 83.818054
 28544/50000: episode: 164, duration: 1.370s, episode steps: 196, steps per second: 143, episode reward: 196.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.361 [-1.055, 2.439], loss: 1.354805, mean_absolute_error: 42.172131, mean_q: 84.242096
 28749/50000: episode: 165, duration: 1.407s, episode steps: 205, steps per second: 146, episode reward: 205.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.224 [-1.852, 1.130], loss: 1.451527, mean_absolute_error: 42.083088, mean_q: 84.072487
 28948/50000: episode: 166, duration: 1.383s, episode steps: 199, steps per second: 144, episode reward: 199.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.360 [-1.027, 2.422], loss: 1.327872, mean_absolute_error: 42.337032, mean_q: 84.456589
 29252/50000: episode: 167, duration: 2.091s, episode steps: 304, steps per second: 145, episode reward: 304.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.240 [-0.850, 2.360], loss: 1.650150, mean_absolute_error: 41.717384, mean_q: 83.099312
 29469/50000: episode: 168, duration: 1.500s, episode steps: 217, steps per second: 145, episode reward: 217.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: -0.251 [-4.299, 3.559], loss: 0.834444, mean_absolute_error: 41.724079, mean_q: 83.269424
 29651/50000: episode: 169, duration: 1.282s, episode steps: 182, steps per second: 142, episode reward: 182.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.245 [-1.654, 1.100], loss: 1.277794, mean_absolute_error: 41.571854, mean_q: 82.791817
 29868/50000: episode: 170, duration: 1.498s, episode steps: 217, steps per second: 145, episode reward: 217.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.325 [-1.234, 2.340], loss: 1.183069, mean_absolute_error: 41.992943, mean_q: 83.605087
 30100/50000: episode: 171, duration: 1.611s, episode steps: 232, steps per second: 144, episode reward: 232.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: -0.249 [-4.569, 3.781], loss: 1.267506, mean_absolute_error: 41.242687, mean_q: 82.238197
 30310/50000: episode: 172, duration: 1.456s, episode steps: 210, steps per second: 144, episode reward: 210.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.347 [-0.797, 2.433], loss: 1.338637, mean_absolute_error: 42.057087, mean_q: 83.951675
 30629/50000: episode: 173, duration: 2.207s, episode steps: 319, steps per second: 145, episode reward: 319.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.229 [-1.053, 2.425], loss: 1.771871, mean_absolute_error: 41.301617, mean_q: 82.253334
 30866/50000: episode: 174, duration: 1.658s, episode steps: 237, steps per second: 143, episode reward: 237.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.306 [-0.861, 2.358], loss: 1.975935, mean_absolute_error: 41.173267, mean_q: 82.227631
 31121/50000: episode: 175, duration: 1.791s, episode steps: 255, steps per second: 142, episode reward: 255.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.237 [-4.311, 3.444], loss: 1.302969, mean_absolute_error: 41.294426, mean_q: 82.425613
 31621/50000: episode: 176, duration: 3.448s, episode steps: 500, steps per second: 145, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.496 [0.000, 1.000], mean observation: -0.016 [-1.078, 1.181], loss: 1.207951, mean_absolute_error: 41.188881, mean_q: 82.259827
 31827/50000: episode: 177, duration: 1.421s, episode steps: 206, steps per second: 145, episode reward: 206.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.348 [-0.846, 2.403], loss: 1.439620, mean_absolute_error: 41.686741, mean_q: 83.311172
 32327/50000: episode: 178, duration: 3.458s, episode steps: 500, steps per second: 145, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.496 [0.000, 1.000], mean observation: -0.055 [-1.351, 1.447], loss: 2.372179, mean_absolute_error: 41.372578, mean_q: 82.476624
 32546/50000: episode: 179, duration: 1.527s, episode steps: 219, steps per second: 143, episode reward: 219.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.306 [-0.848, 2.204], loss: 0.719318, mean_absolute_error: 41.158737, mean_q: 82.200523
 32772/50000: episode: 180, duration: 1.597s, episode steps: 226, steps per second: 141, episode reward: 226.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.301 [-0.779, 2.255], loss: 1.265093, mean_absolute_error: 41.306156, mean_q: 82.512306
 33095/50000: episode: 181, duration: 2.242s, episode steps: 323, steps per second: 144, episode reward: 323.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.220 [-1.117, 2.402], loss: 1.450254, mean_absolute_error: 41.691574, mean_q: 83.222336
 33316/50000: episode: 182, duration: 1.535s, episode steps: 221, steps per second: 144, episode reward: 221.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.303 [-1.944, 3.144], loss: 1.804502, mean_absolute_error: 41.377316, mean_q: 82.681633
 33532/50000: episode: 183, duration: 1.514s, episode steps: 216, steps per second: 143, episode reward: 216.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.332 [-0.963, 2.417], loss: 2.259769, mean_absolute_error: 41.198429, mean_q: 82.224205
 33809/50000: episode: 184, duration: 1.928s, episode steps: 277, steps per second: 144, episode reward: 277.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.248 [-1.012, 2.437], loss: 0.619187, mean_absolute_error: 41.420326, mean_q: 82.879257
 33976/50000: episode: 185, duration: 1.160s, episode steps: 167, steps per second: 144, episode reward: 167.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.551 [0.000, 1.000], mean observation: 0.403 [-1.967, 3.197], loss: 2.007524, mean_absolute_error: 41.518562, mean_q: 82.898895
 34166/50000: episode: 186, duration: 1.342s, episode steps: 190, steps per second: 142, episode reward: 190.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.368 [-1.206, 2.400], loss: 0.675229, mean_absolute_error: 41.208054, mean_q: 82.308006
 34495/50000: episode: 187, duration: 2.303s, episode steps: 329, steps per second: 143, episode reward: 329.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.221 [-1.043, 2.425], loss: 0.999844, mean_absolute_error: 41.403713, mean_q: 82.795120
 34686/50000: episode: 188, duration: 1.341s, episode steps: 191, steps per second: 142, episode reward: 191.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.365 [-0.924, 2.404], loss: 1.293401, mean_absolute_error: 41.330891, mean_q: 82.780762
 34889/50000: episode: 189, duration: 1.421s, episode steps: 203, steps per second: 143, episode reward: 203.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.344 [-1.066, 2.403], loss: 2.406265, mean_absolute_error: 41.951508, mean_q: 83.683411
 35098/50000: episode: 190, duration: 1.467s, episode steps: 209, steps per second: 142, episode reward: 209.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.333 [-1.215, 2.351], loss: 0.833693, mean_absolute_error: 41.833992, mean_q: 83.681442
 35598/50000: episode: 191, duration: 3.468s, episode steps: 500, steps per second: 144, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.083 [-1.126, 1.489], loss: 1.401402, mean_absolute_error: 41.736038, mean_q: 83.462440
 35893/50000: episode: 192, duration: 2.073s, episode steps: 295, steps per second: 142, episode reward: 295.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.230 [-1.068, 2.412], loss: 0.803768, mean_absolute_error: 42.078949, mean_q: 84.134087
 36310/50000: episode: 193, duration: 2.907s, episode steps: 417, steps per second: 143, episode reward: 417.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.178 [-1.226, 2.402], loss: 1.328743, mean_absolute_error: 41.872559, mean_q: 83.675903
 36520/50000: episode: 194, duration: 1.464s, episode steps: 210, steps per second: 143, episode reward: 210.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.333 [-1.076, 2.421], loss: 1.483828, mean_absolute_error: 42.101376, mean_q: 84.102638
 36761/50000: episode: 195, duration: 1.708s, episode steps: 241, steps per second: 141, episode reward: 241.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.295 [-1.067, 2.459], loss: 2.020699, mean_absolute_error: 41.886223, mean_q: 83.540260
 36988/50000: episode: 196, duration: 1.578s, episode steps: 227, steps per second: 144, episode reward: 227.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.315 [-1.113, 2.428], loss: 0.995552, mean_absolute_error: 41.577007, mean_q: 83.159241
 37211/50000: episode: 197, duration: 1.644s, episode steps: 223, steps per second: 136, episode reward: 223.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.319 [-1.199, 2.432], loss: 1.212343, mean_absolute_error: 41.561813, mean_q: 83.078049
 37410/50000: episode: 198, duration: 1.610s, episode steps: 199, steps per second: 124, episode reward: 199.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.358 [-1.090, 2.411], loss: 0.923452, mean_absolute_error: 41.631702, mean_q: 83.254791
 37611/50000: episode: 199, duration: 1.624s, episode steps: 201, steps per second: 124, episode reward: 201.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.355 [-1.253, 2.405], loss: 2.639351, mean_absolute_error: 41.846863, mean_q: 83.719376
 37837/50000: episode: 200, duration: 1.830s, episode steps: 226, steps per second: 123, episode reward: 226.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.339 [-0.902, 2.418], loss: 0.893401, mean_absolute_error: 41.520618, mean_q: 83.120682
 38050/50000: episode: 201, duration: 1.721s, episode steps: 213, steps per second: 124, episode reward: 213.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.339 [-1.195, 2.447], loss: 3.219558, mean_absolute_error: 41.472218, mean_q: 83.008995
 38327/50000: episode: 202, duration: 2.241s, episode steps: 277, steps per second: 124, episode reward: 277.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.233 [-4.390, 3.581], loss: 1.047288, mean_absolute_error: 41.644623, mean_q: 83.397591
 38651/50000: episode: 203, duration: 2.360s, episode steps: 324, steps per second: 137, episode reward: 324.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.234 [-1.014, 2.411], loss: 1.325822, mean_absolute_error: 41.559032, mean_q: 83.058098
 38883/50000: episode: 204, duration: 1.626s, episode steps: 232, steps per second: 143, episode reward: 232.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.319 [-0.917, 2.413], loss: 1.728758, mean_absolute_error: 41.635563, mean_q: 83.367455
 39086/50000: episode: 205, duration: 1.415s, episode steps: 203, steps per second: 143, episode reward: 203.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.351 [-1.169, 2.418], loss: 0.564262, mean_absolute_error: 41.955093, mean_q: 84.001137
 39271/50000: episode: 206, duration: 1.298s, episode steps: 185, steps per second: 142, episode reward: 185.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.384 [-1.136, 2.437], loss: 1.117912, mean_absolute_error: 41.683945, mean_q: 83.423241
 39606/50000: episode: 207, duration: 2.351s, episode steps: 335, steps per second: 143, episode reward: 335.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.222 [-1.172, 2.407], loss: 1.694881, mean_absolute_error: 41.525238, mean_q: 83.061005
 39880/50000: episode: 208, duration: 1.941s, episode steps: 274, steps per second: 141, episode reward: 274.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.265 [-1.329, 2.409], loss: 0.940058, mean_absolute_error: 41.382404, mean_q: 82.898407
 40186/50000: episode: 209, duration: 2.128s, episode steps: 306, steps per second: 144, episode reward: 306.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.238 [-1.114, 2.425], loss: 0.790927, mean_absolute_error: 41.636059, mean_q: 83.387939
 40467/50000: episode: 210, duration: 1.976s, episode steps: 281, steps per second: 142, episode reward: 281.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.278 [-1.215, 2.434], loss: 0.800119, mean_absolute_error: 42.041946, mean_q: 84.210190
 40799/50000: episode: 211, duration: 2.325s, episode steps: 332, steps per second: 143, episode reward: 332.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.253 [-1.033, 2.431], loss: 0.716618, mean_absolute_error: 42.223583, mean_q: 84.414795
 41089/50000: episode: 212, duration: 2.030s, episode steps: 290, steps per second: 143, episode reward: 290.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.267 [-1.208, 2.401], loss: 1.265694, mean_absolute_error: 42.075119, mean_q: 84.017113
 41589/50000: episode: 213, duration: 3.536s, episode steps: 500, steps per second: 141, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.504 [0.000, 1.000], mean observation: 0.095 [-1.399, 1.551], loss: 1.381059, mean_absolute_error: 41.697819, mean_q: 83.423981
 42089/50000: episode: 214, duration: 3.524s, episode steps: 500, steps per second: 142, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.011 [-1.154, 1.130], loss: 0.730811, mean_absolute_error: 42.050991, mean_q: 84.137436
 42589/50000: episode: 215, duration: 3.495s, episode steps: 500, steps per second: 143, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.005 [-1.401, 1.184], loss: 1.645184, mean_absolute_error: 42.223244, mean_q: 84.404884
 43089/50000: episode: 216, duration: 3.521s, episode steps: 500, steps per second: 142, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.498 [0.000, 1.000], mean observation: -0.023 [-1.254, 1.151], loss: 1.897087, mean_absolute_error: 42.233017, mean_q: 84.335670
 43589/50000: episode: 217, duration: 3.531s, episode steps: 500, steps per second: 142, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.014 [-1.104, 1.248], loss: 2.680966, mean_absolute_error: 42.391884, mean_q: 84.695503
 43809/50000: episode: 218, duration: 1.556s, episode steps: 220, steps per second: 141, episode reward: 220.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.170 [-1.368, 1.089], loss: 1.865975, mean_absolute_error: 42.201302, mean_q: 84.372520
 44309/50000: episode: 219, duration: 3.508s, episode steps: 500, steps per second: 143, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.015 [-1.206, 1.210], loss: 0.964294, mean_absolute_error: 42.293362, mean_q: 84.641479
 44518/50000: episode: 220, duration: 1.471s, episode steps: 209, steps per second: 142, episode reward: 209.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.371 [-1.249, 2.429], loss: 2.004365, mean_absolute_error: 42.038063, mean_q: 83.928398
 45018/50000: episode: 221, duration: 3.532s, episode steps: 500, steps per second: 142, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.502 [0.000, 1.000], mean observation: -0.002 [-1.262, 1.309], loss: 1.256025, mean_absolute_error: 42.290611, mean_q: 84.619377
 45518/50000: episode: 222, duration: 3.520s, episode steps: 500, steps per second: 142, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.504 [0.000, 1.000], mean observation: -0.019 [-1.587, 1.382], loss: 2.181186, mean_absolute_error: 42.507191, mean_q: 85.060539
 46018/50000: episode: 223, duration: 3.552s, episode steps: 500, steps per second: 141, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.498 [0.000, 1.000], mean observation: -0.000 [-1.359, 1.225], loss: 2.445640, mean_absolute_error: 42.530754, mean_q: 85.111122
 46518/50000: episode: 224, duration: 3.478s, episode steps: 500, steps per second: 144, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.016 [-1.335, 1.143], loss: 1.340773, mean_absolute_error: 42.761265, mean_q: 85.699028
 47018/50000: episode: 225, duration: 3.542s, episode steps: 500, steps per second: 141, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.037 [-1.100, 1.255], loss: 1.289494, mean_absolute_error: 42.943638, mean_q: 86.013817
 47518/50000: episode: 226, duration: 3.558s, episode steps: 500, steps per second: 141, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.498 [0.000, 1.000], mean observation: -0.032 [-1.150, 1.356], loss: 2.215645, mean_absolute_error: 43.156174, mean_q: 86.526756
 48018/50000: episode: 227, duration: 3.544s, episode steps: 500, steps per second: 141, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.498 [0.000, 1.000], mean observation: -0.025 [-1.591, 1.225], loss: 2.842190, mean_absolute_error: 43.625877, mean_q: 87.367218
 48518/50000: episode: 228, duration: 3.669s, episode steps: 500, steps per second: 136, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.101 [-1.149, 1.031], loss: 3.109114, mean_absolute_error: 43.450954, mean_q: 86.983040
 48923/50000: episode: 229, duration: 3.292s, episode steps: 405, steps per second: 123, episode reward: 405.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: -0.136 [-1.660, 1.129], loss: 4.396276, mean_absolute_error: 44.035351, mean_q: 88.101929
 49304/50000: episode: 230, duration: 3.115s, episode steps: 381, steps per second: 122, episode reward: 381.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.496 [0.000, 1.000], mean observation: -0.087 [-1.427, 1.191], loss: 3.260462, mean_absolute_error: 44.197563, mean_q: 88.468483
 49528/50000: episode: 231, duration: 1.833s, episode steps: 224, steps per second: 122, episode reward: 224.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.241 [-1.663, 0.978], loss: 1.549309, mean_absolute_error: 44.178738, mean_q: 88.550743
 49721/50000: episode: 232, duration: 1.469s, episode steps: 193, steps per second: 131, episode reward: 193.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.451 [0.000, 1.000], mean observation: -0.233 [-3.562, 3.101], loss: 3.552085, mean_absolute_error: 43.955868, mean_q: 88.171387